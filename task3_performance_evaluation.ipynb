{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Performance Evaluation with Multiple Metrics\n",
        "\n",
        "**Title:** \"Multi-Metric NER Performance Evaluation Against Ground Truth\"\n",
        "\n",
        "## Objective\n",
        "Create a comprehensive evaluation module for NER predictions from Task 2 that:\n",
        "- Loads ground truth from 'original' subdirectory\n",
        "- Parses both predicted and ground truth annotations into comparable formats\n",
        "- Implements entity-level evaluation (exact boundary + label matching)\n",
        "- Uses seqeval library for proper NER evaluation\n",
        "- Provides per-entity-type metrics, overall metrics, and confusion matrix\n",
        "- Includes detailed justifications for evaluation approach choices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Set, Optional\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Install seqeval if not already installed\n",
        "try:\n",
        "    from seqeval.metrics import (\n",
        "        classification_report,\n",
        "        accuracy_score,\n",
        "        precision_score,\n",
        "        recall_score,\n",
        "        f1_score\n",
        "    )\n",
        "except ImportError:\n",
        "    print(\"⚠ seqeval not found. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seqeval\"])\n",
        "    from seqeval.metrics import (\n",
        "        classification_report,\n",
        "        accuracy_score,\n",
        "        precision_score,\n",
        "        recall_score,\n",
        "        f1_score\n",
        "    )\n",
        "    print(\"✓ seqeval installed successfully\")\n",
        "\n",
        "# Note: confusion_matrix is not available in seqeval.metrics\n",
        "# We use our own generate_confusion_matrix() function for entity-level confusion matrices\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = Path(\"cadec\")\n",
        "TEXT_DIR = BASE_DIR / \"text\"\n",
        "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
        "\n",
        "# Verify directories exist\n",
        "if not TEXT_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {TEXT_DIR}\")\n",
        "if not ORIGINAL_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {ORIGINAL_DIR}\")\n",
        "\n",
        "print(\"✓ Directories verified\")\n",
        "print(f\"  - Text directory: {TEXT_DIR}\")\n",
        "print(f\"  - Original directory: {ORIGINAL_DIR}\")\n",
        "\n",
        "# Label types we're evaluating\n",
        "LABEL_TYPES = ['ADR', 'Drug', 'Disease', 'Symptom']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading and Parsing Ground Truth Annotations\n",
        "\n",
        "Load ground truth annotations from the 'original' subdirectory and parse them into a structured format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load and parse ground truth annotation file from 'original' subdirectory.\n",
        "    \n",
        "    Format: TAG\\tLABEL START END\\tTEXT\n",
        "    Example: T1\tADR 9 19\tbit drowsy\n",
        "    Example with multiple ranges: T6\tSymptom 66 74;76 94;98 107\tthe heel I couldn't walk on very well\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ann_file_path : Path\n",
        "        Path to the .ann annotation file\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of entity dictionaries with:\n",
        "        - 'label': Entity type (ADR, Drug, Disease, Symptom)\n",
        "        - 'text': Entity text\n",
        "        - 'start': Start character position\n",
        "        - 'end': End character position\n",
        "        - 'tag': Original tag identifier (T1, T2, etc.)\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    \n",
        "    try:\n",
        "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                \n",
        "                # Skip empty lines and comment lines (starting with '#')\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                \n",
        "                # Parse entity annotation lines (starting with 'T' followed by a number)\n",
        "                # Format: TAG\\tLABEL RANGES\\tTEXT\n",
        "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
        "                if match:\n",
        "                    tag = match.group(1)\n",
        "                    label_and_ranges = match.group(2)\n",
        "                    text = match.group(3)\n",
        "                    \n",
        "                    # Extract label type (first word) and ranges (remaining part)\n",
        "                    parts = label_and_ranges.split(None, 1)\n",
        "                    if len(parts) < 2:\n",
        "                        continue\n",
        "                    \n",
        "                    label_type = parts[0]\n",
        "                    ranges_str = parts[1]\n",
        "                    \n",
        "                    # Only process ADR, Drug, Disease, Symptom labels\n",
        "                    if label_type not in LABEL_TYPES:\n",
        "                        continue\n",
        "                    \n",
        "                    # Extract ranges (can be multiple pairs separated by semicolons)\n",
        "                    ranges = []\n",
        "                    if ';' in ranges_str:\n",
        "                        # Multiple ranges format: \"START1 END1;START2 END2;...\"\n",
        "                        range_pairs = ranges_str.split(';')\n",
        "                        for rp in range_pairs:\n",
        "                            rp = rp.strip()\n",
        "                            if rp:\n",
        "                                range_nums = rp.split()\n",
        "                                if len(range_nums) >= 2:\n",
        "                                    try:\n",
        "                                        start = int(range_nums[0])\n",
        "                                        end = int(range_nums[1])\n",
        "                                        ranges.append((start, end))\n",
        "                                    except ValueError:\n",
        "                                        continue\n",
        "                    else:\n",
        "                        # Single range format: \"START END\"\n",
        "                        range_nums = ranges_str.split()\n",
        "                        if len(range_nums) >= 2:\n",
        "                            try:\n",
        "                                start = int(range_nums[0])\n",
        "                                end = int(range_nums[1])\n",
        "                                ranges = [(start, end)]\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "                    \n",
        "                    # Create entity entries for each range\n",
        "                    # For multiple ranges, we create separate entities (standard practice in NER)\n",
        "                    for start, end in ranges:\n",
        "                        entities.append({\n",
        "                            'label': label_type,\n",
        "                            'text': text.strip(),\n",
        "                            'start': start,\n",
        "                            'end': end,\n",
        "                            'tag': tag\n",
        "                        })\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ground truth from {ann_file_path}: {e}\")\n",
        "        return []\n",
        "    \n",
        "    return entities\n",
        "\n",
        "print(\"✓ Ground truth loading function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading Predicted Annotations\n",
        "\n",
        "Load predicted annotations. These can be:\n",
        "- Generated on-the-fly using Task 2 pipeline\n",
        "- Loaded from saved prediction files (if available)\n",
        "\n",
        "For this implementation, we'll create a function that can parse predictions in the same format as ground truth (TAG\\tLABEL START END\\tTEXT).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_predictions(ann_file_path: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load and parse predicted annotation file.\n",
        "    \n",
        "    Uses the same parsing logic as load_ground_truth() since predictions\n",
        "    should be in the same format (generated by Task 2's conversion function).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ann_file_path : Path\n",
        "        Path to the predicted .ann file (or can be a list of annotation lines)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of entity dictionaries with same structure as ground truth\n",
        "    \"\"\"\n",
        "    # If it's a Path object pointing to a file\n",
        "    if isinstance(ann_file_path, Path) and ann_file_path.exists():\n",
        "        return load_ground_truth(ann_file_path)\n",
        "    \n",
        "    # If it's a list of annotation lines (from Task 2 output)\n",
        "    if isinstance(ann_file_path, list):\n",
        "        entities = []\n",
        "        for line in ann_file_path:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('#'):\n",
        "                continue\n",
        "            \n",
        "            # Parse: TAG\\tLABEL START END\\tTEXT\n",
        "            match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
        "            if match:\n",
        "                tag = match.group(1)\n",
        "                label_and_ranges = match.group(2)\n",
        "                text = match.group(3)\n",
        "                \n",
        "                parts = label_and_ranges.split(None, 1)\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "                \n",
        "                label_type = parts[0]\n",
        "                ranges_str = parts[1]\n",
        "                \n",
        "                if label_type not in LABEL_TYPES:\n",
        "                    continue\n",
        "                \n",
        "                # Parse ranges\n",
        "                ranges = []\n",
        "                if ';' in ranges_str:\n",
        "                    range_pairs = ranges_str.split(';')\n",
        "                    for rp in range_pairs:\n",
        "                        rp = rp.strip()\n",
        "                        if rp:\n",
        "                            range_nums = rp.split()\n",
        "                            if len(range_nums) >= 2:\n",
        "                                try:\n",
        "                                    start = int(range_nums[0])\n",
        "                                    end = int(range_nums[1])\n",
        "                                    ranges.append((start, end))\n",
        "                                except ValueError:\n",
        "                                    continue\n",
        "                else:\n",
        "                    range_nums = ranges_str.split()\n",
        "                    if len(range_nums) >= 2:\n",
        "                        try:\n",
        "                            start = int(range_nums[0])\n",
        "                            end = int(range_nums[1])\n",
        "                            ranges = [(start, end)]\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "                \n",
        "                for start, end in ranges:\n",
        "                    entities.append({\n",
        "                        'label': label_type,\n",
        "                        'text': text.strip(),\n",
        "                        'start': start,\n",
        "                        'end': end,\n",
        "                        'tag': tag\n",
        "                    })\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    return []\n",
        "\n",
        "print(\"✓ Prediction loading function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entity-Level Evaluation Implementation and Justification\n",
        "\n",
        "### Why Entity-Level Evaluation is More Appropriate Than Token-Level\n",
        "\n",
        "Entity-level evaluation is the **recommended approach** for medical NER evaluation, especially for downstream tasks. Here's why:\n",
        "\n",
        "#### 1. **Downstream Task Alignment**\n",
        "Medical information extraction systems operate on **complete entities**, not individual tokens:\n",
        "- **Drug-Disease Relationships**: Need complete entity spans to extract meaningful relationships (e.g., \"ibuprofen treats arthritis\" requires both full entities)\n",
        "- **Adverse Event Detection**: ADR extraction systems need exact entity boundaries to properly link drugs to adverse reactions\n",
        "- **Knowledge Graph Construction**: Medical knowledge graphs require precise entity nodes with correct boundaries\n",
        "- **Clinical Decision Support**: Partial matches provide incomplete or ambiguous information that can mislead clinical systems\n",
        "\n",
        "#### 2. **Importance of Exact Boundary Matching in Medical NER**\n",
        "\n",
        "In medical contexts, boundary precision is **critical for clinical safety**:\n",
        "- **Semantic Meaning**: \"stomach pain\" vs \"stomach\" + \"pain\" convey different clinical meanings\n",
        "  - \"stomach pain\" = specific symptom entity\n",
        "  - \"stomach\" + \"pain\" = potentially two separate concepts\n",
        "- **Drug Names**: \"aspirin\" (6 chars) vs \"aspirin\" with trailing space can lead to database lookup failures\n",
        "- **Disease Names**: \"type 2 diabetes\" must be captured as complete entity for proper ICD-10 coding\n",
        "- **Dosage Information**: Boundary errors can misidentify dosage amounts (e.g., \"50mg\" vs \"50 mg\")\n",
        "\n",
        "#### 3. **Partial Matches Should Be Considered False Positives**\n",
        "\n",
        "Our evaluation treats **any partial match as a False Positive**:\n",
        "- **Rationale**: A partial match doesn't provide complete, usable information for downstream tasks\n",
        "- **Example Scenarios**:\n",
        "  - GT: \"ibuprofen\" [10, 19], Pred: \"ibuprofen\" [10, 20] → **FP** (boundary mismatch)\n",
        "  - GT: \"bit drowsy\" [10, 19], Pred: \"drowsy\" [14, 19] → **FP** (partial boundary)\n",
        "  - GT: ADR [10, 19], Pred: Drug [10, 19] → **FP** (label confusion)\n",
        "- **Strict Matching Encourages**:\n",
        "  - Models learn precise entity boundaries\n",
        "  - Production systems deliver accurate extractions\n",
        "  - Reduces errors in clinical applications\n",
        "\n",
        "#### 4. **Clinical Safety Implications**\n",
        "\n",
        "Incorrect entity boundaries can have serious consequences:\n",
        "- **Drug Interaction Detection**: Wrong boundaries can miss critical drug-drug interactions\n",
        "- **Patient Safety**: Misidentified adverse events can lead to incorrect medical decisions\n",
        "- **Regulatory Compliance**: Medical device software requires exact entity matching for FDA submissions\n",
        "\n",
        "### How seqeval Handles Entity Boundary Evaluation\n",
        "\n",
        "The `seqeval` library implements **CoNLL-2003 evaluation standards**:\n",
        "\n",
        "- **Exact Match Requirement**: Entities must match EXACTLY on:\n",
        "  - Boundary start position\n",
        "  - Boundary end position  \n",
        "  - Label type\n",
        "- **No Partial Credit**: A prediction (ADR, 10, 20) only matches ground truth (ADR, 10, 20)\n",
        "- **Overlapping Entities**: \n",
        "  - Prediction [10, 25] vs Ground Truth [10, 20] → **False Positive**\n",
        "  - Prediction [15, 20] vs Ground Truth [10, 20] → **False Positive**\n",
        "- **BIO Format Evaluation**: seqeval works on BIO-tagged sequences but evaluates at entity level\n",
        "- **Token-Level vs Entity-Level**: seqeval calculates metrics on complete entities, not individual tokens\n",
        "\n",
        "**Alignment with Standards**: This approach follows established NER evaluation methodology used in:\n",
        "- CoNLL-2003 shared task\n",
        "- SemEval medical NER challenges\n",
        "- Clinical NLP evaluation benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entity_level_evaluation(ground_truth: List[Dict], predictions: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Perform entity-level evaluation (exact boundary + label matching).\n",
        "    \n",
        "    Entity-level evaluation requires:\n",
        "    - Exact match of entity boundaries (start AND end positions)\n",
        "    - Exact match of label type\n",
        "    \n",
        "    Returns TP, FP, FN counts for each entity type and overall.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ground_truth : List[Dict]\n",
        "        List of ground truth entities\n",
        "    predictions : List[Dict]\n",
        "        List of predicted entities\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict\n",
        "        Dictionary containing:\n",
        "        - 'tp', 'fp', 'fn' per entity type\n",
        "        - Overall 'tp', 'fp', 'fn'\n",
        "        - 'precision', 'recall', 'f1' per entity type\n",
        "        - Overall 'precision', 'recall', 'f1'\n",
        "    \"\"\"\n",
        "    # Convert entities to sets of tuples for exact matching\n",
        "    # Format: (label, start, end, text) - text included for verification\n",
        "    gt_set = set()\n",
        "    for entity in ground_truth:\n",
        "        gt_set.add((entity['label'], entity['start'], entity['end'], entity['text']))\n",
        "    \n",
        "    pred_set = set()\n",
        "    for entity in predictions:\n",
        "        pred_set.add((entity['label'], entity['start'], entity['end'], entity['text']))\n",
        "    \n",
        "    # Calculate True Positives: entities that appear in both sets\n",
        "    tp_all = gt_set.intersection(pred_set)\n",
        "    \n",
        "    # Calculate False Positives: predicted entities not in ground truth\n",
        "    fp_all = pred_set - gt_set\n",
        "    \n",
        "    # Calculate False Negatives: ground truth entities not predicted\n",
        "    fn_all = gt_set - pred_set\n",
        "    \n",
        "    # Per-entity-type metrics\n",
        "    results = {\n",
        "        'tp': {},\n",
        "        'fp': {},\n",
        "        'fn': {},\n",
        "        'precision': {},\n",
        "        'recall': {},\n",
        "        'f1': {}\n",
        "    }\n",
        "    \n",
        "    # Calculate metrics per entity type\n",
        "    for label_type in LABEL_TYPES:\n",
        "        # Filter by label type\n",
        "        tp_type = {e for e in tp_all if e[0] == label_type}\n",
        "        fp_type = {e for e in fp_all if e[0] == label_type}\n",
        "        fn_type = {e for e in fn_all if e[0] == label_type}\n",
        "        \n",
        "        tp_count = len(tp_type)\n",
        "        fp_count = len(fp_type)\n",
        "        fn_count = len(fn_type)\n",
        "        \n",
        "        # Calculate Precision, Recall, F1\n",
        "        precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0.0\n",
        "        recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        results['tp'][label_type] = tp_count\n",
        "        results['fp'][label_type] = fp_count\n",
        "        results['fn'][label_type] = fn_count\n",
        "        results['precision'][label_type] = precision\n",
        "        results['recall'][label_type] = recall\n",
        "        results['f1'][label_type] = f1\n",
        "    \n",
        "    # Overall micro-averaged metrics\n",
        "    total_tp = len(tp_all)\n",
        "    total_fp = len(fp_all)\n",
        "    total_fn = len(fn_all)\n",
        "    \n",
        "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "    \n",
        "    results['tp']['OVERALL'] = total_tp\n",
        "    results['fp']['OVERALL'] = total_fp\n",
        "    results['fn']['OVERALL'] = total_fn\n",
        "    results['precision']['OVERALL'] = overall_precision\n",
        "    results['recall']['OVERALL'] = overall_recall\n",
        "    results['f1']['OVERALL'] = overall_f1\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Entity-level evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Converting to BIO Format for seqeval\n",
        "\n",
        "seqeval requires BIO-tagged sequences. We'll convert both ground truth and predictions to BIO format for comprehensive evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entities_to_bio_tags(entities: List[Dict], text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convert entity annotations to BIO-tagged sequence.\n",
        "    \n",
        "    This function:\n",
        "    1. Tokenizes text word-by-word\n",
        "    2. Assigns BIO tags based on entity spans\n",
        "    3. Handles overlapping entities (takes first entity in case of overlap)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    entities : List[Dict]\n",
        "        List of entity dictionaries with 'label', 'start', 'end'\n",
        "    text : str\n",
        "        Original text to tag\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[str]\n",
        "        List of BIO tags (one per word/token)\n",
        "    \"\"\"\n",
        "    # Tokenize text word-by-word and get character positions\n",
        "    words = text.split()\n",
        "    word_tokens = []\n",
        "    current_pos = 0\n",
        "    \n",
        "    for word in words:\n",
        "        word_start = text.find(word, current_pos)\n",
        "        if word_start == -1:\n",
        "            word_start = current_pos\n",
        "        word_end = word_start + len(word)\n",
        "        word_tokens.append((word, word_start, word_end))\n",
        "        \n",
        "        next_pos = word_end\n",
        "        while next_pos < len(text) and text[next_pos].isspace():\n",
        "            next_pos += 1\n",
        "        current_pos = next_pos\n",
        "    \n",
        "    # Initialize all tags as 'O'\n",
        "    bio_tags = ['O'] * len(word_tokens)\n",
        "    \n",
        "    # Sort entities by start position to handle overlaps consistently\n",
        "    sorted_entities = sorted(entities, key=lambda x: (x['start'], x['end']))\n",
        "    \n",
        "    # Assign BIO tags\n",
        "    for entity in sorted_entities:\n",
        "        label = entity['label']\n",
        "        start_char = entity['start']\n",
        "        end_char = entity['end']\n",
        "        \n",
        "        # Find words that overlap with this entity span\n",
        "        overlapping_indices = []\n",
        "        for i, (word, word_start, word_end) in enumerate(word_tokens):\n",
        "            # Word overlaps if: word_start < end_char AND word_end > start_char\n",
        "            if word_start < end_char and word_end > start_char:\n",
        "                overlapping_indices.append(i)\n",
        "        \n",
        "        # Assign BIO labels: B- for first word, I- for subsequent words\n",
        "        if overlapping_indices:\n",
        "            for idx, word_idx in enumerate(overlapping_indices):\n",
        "                # Only assign if not already assigned (handle overlaps)\n",
        "                if bio_tags[word_idx] == 'O':\n",
        "                    if idx == 0:\n",
        "                        bio_tags[word_idx] = f\"B-{label}\"\n",
        "                    else:\n",
        "                        # Check if previous word had the same entity type\n",
        "                        prev_label = bio_tags[word_idx - 1]\n",
        "                        if prev_label.endswith(label):\n",
        "                            bio_tags[word_idx] = f\"I-{label}\"\n",
        "                        else:\n",
        "                            bio_tags[word_idx] = f\"B-{label}\"  # New entity of same type\n",
        "    \n",
        "    return bio_tags\n",
        "\n",
        "print(\"✓ BIO conversion function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Confusion Matrix Generation\n",
        "\n",
        "Generate a confusion matrix to visualize common misclassifications between entity types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_confusion_matrix(ground_truth: List[Dict], predictions: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate a confusion matrix showing misclassifications.\n",
        "    \n",
        "    For each predicted entity that doesn't exactly match ground truth,\n",
        "    track what label was predicted vs what should have been predicted.\n",
        "    \n",
        "    Note: Only includes entities where boundaries match but labels differ,\n",
        "    or where boundaries partially overlap (showing label confusion).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ground_truth : List[Dict]\n",
        "        List of ground truth entities\n",
        "    predictions : List[Dict]\n",
        "        List of predicted entities\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Confusion matrix with ground truth labels as rows, predicted labels as columns\n",
        "    \"\"\"\n",
        "    # Create mapping of (start, end) -> label for ground truth\n",
        "    gt_by_position = {}\n",
        "    for entity in ground_truth:\n",
        "        pos_key = (entity['start'], entity['end'])\n",
        "        # If multiple entities at same position, keep the first\n",
        "        if pos_key not in gt_by_position:\n",
        "            gt_by_position[pos_key] = entity['label']\n",
        "    \n",
        "    # Create mapping of (start, end) -> label for predictions\n",
        "    pred_by_position = {}\n",
        "    for entity in predictions:\n",
        "        pos_key = (entity['start'], entity['end'])\n",
        "        if pos_key not in pred_by_position:\n",
        "            pred_by_position[pos_key] = entity['label']\n",
        "    \n",
        "    # Build confusion matrix\n",
        "    confusion_dict = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    # Check all predicted positions\n",
        "    for pos_key, pred_label in pred_by_position.items():\n",
        "        gt_label = gt_by_position.get(pos_key, 'NONE')\n",
        "        confusion_dict[gt_label][pred_label] += 1\n",
        "    \n",
        "    # Check all ground truth positions not in predictions\n",
        "    for pos_key, gt_label in gt_by_position.items():\n",
        "        if pos_key not in pred_by_position:\n",
        "            confusion_dict[gt_label]['NONE'] += 1\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    all_labels = LABEL_TYPES + ['NONE']\n",
        "    confusion_data = []\n",
        "    \n",
        "    for gt_label in all_labels:\n",
        "        row = {'Ground Truth': gt_label}\n",
        "        for pred_label in all_labels:\n",
        "            row[pred_label] = confusion_dict[gt_label][pred_label]\n",
        "        confusion_data.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(confusion_data)\n",
        "    df = df.set_index('Ground Truth')\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"✓ Confusion matrix generation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Complete Evaluation Pipeline\n",
        "\n",
        "This function orchestrates the entire evaluation process for a single file or all files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_file(text_file_path: Path, \n",
        "                  gt_entities: List[Dict],\n",
        "                  pred_entities: List[Dict],\n",
        "                  text: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate predictions against ground truth for a single file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text_file_path : Path\n",
        "        Path to the text file (for reference)\n",
        "    gt_entities : List[Dict]\n",
        "        Ground truth entities\n",
        "    pred_entities : List[Dict]\n",
        "        Predicted entities\n",
        "    text : str\n",
        "        Original text\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict\n",
        "        Dictionary containing all evaluation results\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # 1. Entity-level evaluation\n",
        "    entity_results = entity_level_evaluation(gt_entities, pred_entities)\n",
        "    results['entity_level'] = entity_results\n",
        "    \n",
        "    # 2. BIO format conversion for seqeval\n",
        "    gt_bio = entities_to_bio_tags(gt_entities, text)\n",
        "    pred_bio = entities_to_bio_tags(pred_entities, text)\n",
        "    \n",
        "    # Ensure same length (pad or truncate if needed)\n",
        "    min_len = min(len(gt_bio), len(pred_bio))\n",
        "    gt_bio = gt_bio[:min_len]\n",
        "    pred_bio = pred_bio[:min_len]\n",
        "    \n",
        "    # 3. seqeval metrics\n",
        "    try:\n",
        "        # Calculate seqeval metrics\n",
        "        # Note: seqeval automatically infers labels from BIO sequences\n",
        "        seqeval_results = {\n",
        "            'accuracy': accuracy_score([gt_bio], [pred_bio]),\n",
        "            'precision': precision_score([gt_bio], [pred_bio]),\n",
        "            'recall': recall_score([gt_bio], [pred_bio]),\n",
        "            'f1': f1_score([gt_bio], [pred_bio])\n",
        "        }\n",
        "        \n",
        "        # Classification report (detailed per-entity metrics)\n",
        "        # seqeval's classification_report doesn't accept 'labels' parameter\n",
        "        # It automatically extracts labels from the BIO sequences\n",
        "        try:\n",
        "            report = classification_report([gt_bio], [pred_bio], output_dict=True, zero_division=0)\n",
        "            seqeval_results['classification_report'] = report\n",
        "        except Exception as report_error:\n",
        "            # If classification_report fails, continue without it\n",
        "            print(f\"  Note: Could not generate classification report: {report_error}\")\n",
        "        \n",
        "        results['seqeval'] = seqeval_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error in seqeval evaluation for {text_file_path.name}: {e}\")\n",
        "        results['seqeval'] = None\n",
        "    \n",
        "    # 4. Confusion matrix\n",
        "    confusion_matrix_df = generate_confusion_matrix(gt_entities, pred_entities)\n",
        "    results['confusion_matrix'] = confusion_matrix_df\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Complete evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Aggregating Results Across All Files\n",
        "\n",
        "Function to evaluate all files and aggregate metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_results(all_results: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Aggregate evaluation results across all files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    all_results : List[Dict]\n",
        "        List of evaluation results for each file\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict\n",
        "        Aggregated metrics\n",
        "    \"\"\"\n",
        "    # Aggregate entity-level metrics\n",
        "    aggregated = {\n",
        "        'tp': Counter(),\n",
        "        'fp': Counter(),\n",
        "        'fn': Counter()\n",
        "    }\n",
        "    \n",
        "    # Sum up TP, FP, FN across all files\n",
        "    for result in all_results:\n",
        "        entity_level = result.get('entity_level', {})\n",
        "        for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "            tp = entity_level.get('tp', {}).get(label_type, 0)\n",
        "            fp = entity_level.get('fp', {}).get(label_type, 0)\n",
        "            fn = entity_level.get('fn', {}).get(label_type, 0)\n",
        "            \n",
        "            aggregated['tp'][label_type] += tp\n",
        "            aggregated['fp'][label_type] += fp\n",
        "            aggregated['fn'][label_type] += fn\n",
        "    \n",
        "    # Calculate aggregated precision, recall, F1\n",
        "    aggregated['precision'] = {}\n",
        "    aggregated['recall'] = {}\n",
        "    aggregated['f1'] = {}\n",
        "    \n",
        "    for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "        tp = aggregated['tp'][label_type]\n",
        "        fp = aggregated['fp'][label_type]\n",
        "        fn = aggregated['fn'][label_type]\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        aggregated['precision'][label_type] = precision\n",
        "        aggregated['recall'][label_type] = recall\n",
        "        aggregated['f1'][label_type] = f1\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "print(\"✓ Aggregation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Integration with Task 2 Pipeline\n",
        "\n",
        "To evaluate predictions, we need to either:\n",
        "1. Load saved predictions from Task 2\n",
        "2. Re-generate predictions using Task 2 pipeline\n",
        "\n",
        "Here, we'll create a helper function that can generate predictions using Task 2 functions (if available) or load them from files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to import Task 2 functions for generating predictions\n",
        "# If Task 2 notebook functions are not directly importable, we'll need to\n",
        "# either regenerate predictions or load them from saved files\n",
        "\n",
        "try:\n",
        "    # Attempt to import from task2 (may not work if not in same session)\n",
        "    # This is a placeholder - in practice, you'd either:\n",
        "    # 1. Save predictions from Task 2 to files\n",
        "    # 2. Run Task 2 in the same notebook session\n",
        "    # 3. Load Task 2 functions dynamically\n",
        "    \n",
        "    # For now, we'll assume predictions are generated elsewhere or saved\n",
        "    print(\"Note: For full evaluation, predictions should be generated using Task 2 pipeline\")\n",
        "    print(\"      or loaded from saved prediction files.\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Task 2 functions not directly importable.\")\n",
        "    print(\"Please ensure predictions are available for evaluation.\")\n",
        "\n",
        "print(\"✓ Integration placeholder defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Testing with Sample File\n",
        "\n",
        "Let's test the evaluation pipeline with a sample file to verify everything works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample file\n",
        "sample_text_files = list(TEXT_DIR.glob(\"*.txt\"))[:1]  # Get first file for testing\n",
        "\n",
        "if sample_text_files:\n",
        "    test_file = sample_text_files[0]\n",
        "    print(f\"Testing evaluation with file: {test_file.name}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load text\n",
        "    with open(test_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().strip()\n",
        "    \n",
        "    # Load ground truth\n",
        "    gt_file = ORIGINAL_DIR / test_file.name.replace('.txt', '.ann')\n",
        "    gt_entities = load_ground_truth(gt_file)\n",
        "    \n",
        "    print(f\"\\nGround Truth: {len(gt_entities)} entities found\")\n",
        "    for entity in gt_entities[:5]:  # Show first 5\n",
        "        print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
        "    \n",
        "    # For testing, create dummy predictions (in real scenario, these come from Task 2)\n",
        "    # Here we'll use a subset of ground truth to simulate predictions\n",
        "    print(\"\\n⚠ Note: Using dummy predictions for demonstration.\")\n",
        "    print(\"   In actual evaluation, use predictions from Task 2 pipeline.\")\n",
        "    \n",
        "    # Create dummy predictions (remove some entities, add some wrong ones)\n",
        "    pred_entities = gt_entities[:len(gt_entities)//2] if len(gt_entities) > 1 else []\n",
        "    \n",
        "    if pred_entities:\n",
        "        print(f\"\\nDummy Predictions: {len(pred_entities)} entities\")\n",
        "        for entity in pred_entities[:5]:\n",
        "            print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
        "    \n",
        "    # Evaluate\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    results = evaluate_file(test_file, gt_entities, pred_entities, text)\n",
        "    \n",
        "    # Display entity-level results\n",
        "    print(\"\\nEntity-Level Metrics:\")\n",
        "    print(\"-\" * 80)\n",
        "    entity_level = results['entity_level']\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'Label': LABEL_TYPES + ['OVERALL'],\n",
        "        'Precision': [entity_level['precision'][l] for l in LABEL_TYPES + ['OVERALL']],\n",
        "        'Recall': [entity_level['recall'][l] for l in LABEL_TYPES + ['OVERALL']],\n",
        "        'F1': [entity_level['f1'][l] for l in LABEL_TYPES + ['OVERALL']],\n",
        "        'TP': [entity_level['tp'][l] for l in LABEL_TYPES + ['OVERALL']],\n",
        "        'FP': [entity_level['fp'][l] for l in LABEL_TYPES + ['OVERALL']],\n",
        "        'FN': [entity_level['fn'][l] for l in LABEL_TYPES + ['OVERALL']]\n",
        "    })\n",
        "    \n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    # Display seqeval results if available\n",
        "    if results.get('seqeval'):\n",
        "        print(\"\\nseqeval Metrics:\")\n",
        "        print(\"-\" * 80)\n",
        "        seqeval = results['seqeval']\n",
        "        print(f\"  Accuracy:  {seqeval.get('accuracy', 0):.4f}\")\n",
        "        print(f\"  Precision: {seqeval.get('precision', 0):.4f}\")\n",
        "        print(f\"  Recall:    {seqeval.get('recall', 0):.4f}\")\n",
        "        print(f\"  F1-Score:  {seqeval.get('f1', 0):.4f}\")\n",
        "    \n",
        "    # Display confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(results['confusion_matrix'].to_string())\n",
        "    \n",
        "else:\n",
        "    print(\"⚠ No text files found for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Full Dataset Evaluation\n",
        "\n",
        "This section evaluates all files in the dataset. Note that generating predictions for all files using Task 2 may take considerable time.\n",
        "\n",
        "**Important**: You'll need to either:\n",
        "1. Have predictions saved from Task 2 execution\n",
        "2. Integrate Task 2 pipeline here to generate predictions on-the-fly\n",
        "3. Use a subset of files for faster evaluation\n",
        "\n",
        "For demonstration, we'll create a framework that can handle all scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_all_files(text_files: List[Path], \n",
        "                       get_predictions_func,\n",
        "                       max_files: Optional[int] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate all text files in the dataset.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text_files : List[Path]\n",
        "        List of text file paths to evaluate\n",
        "    get_predictions_func : callable\n",
        "        Function that takes (text_file_path, text) and returns predicted entities\n",
        "        Format: List[Dict] with 'label', 'start', 'end', 'text'\n",
        "    max_files : int, optional\n",
        "        Maximum number of files to evaluate (for testing on subset)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict\n",
        "        Aggregated evaluation results\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    files_evaluated = 0\n",
        "    \n",
        "    if max_files:\n",
        "        text_files = text_files[:max_files]\n",
        "    \n",
        "    total_files = len(text_files)\n",
        "    print(f\"Evaluating {total_files} files...\")\n",
        "    \n",
        "    for idx, text_file in enumerate(text_files, 1):\n",
        "        try:\n",
        "            # Load text\n",
        "            with open(text_file, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "            \n",
        "            # Load ground truth\n",
        "            gt_file = ORIGINAL_DIR / text_file.name.replace('.txt', '.ann')\n",
        "            if not gt_file.exists():\n",
        "                print(f\"⚠ Skipping {text_file.name}: ground truth not found\")\n",
        "                continue\n",
        "            \n",
        "            gt_entities = load_ground_truth(gt_file)\n",
        "            \n",
        "            # Get predictions\n",
        "            pred_entities = get_predictions_func(text_file, text)\n",
        "            \n",
        "            # Evaluate\n",
        "            file_results = evaluate_file(text_file, gt_entities, pred_entities, text)\n",
        "            all_results.append(file_results)\n",
        "            \n",
        "            files_evaluated += 1\n",
        "            \n",
        "            # Progress update\n",
        "            if idx % 50 == 0:\n",
        "                print(f\"  Processed {idx}/{total_files} files...\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error evaluating {text_file.name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n✓ Evaluation complete: {files_evaluated} files evaluated\")\n",
        "    \n",
        "    # Aggregate results\n",
        "    aggregated = aggregate_results(all_results)\n",
        "    \n",
        "    return {\n",
        "        'per_file_results': all_results,\n",
        "        'aggregated': aggregated,\n",
        "        'files_evaluated': files_evaluated\n",
        "    }\n",
        "\n",
        "print(\"✓ Full dataset evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_evaluation_results(aggregated_results: Dict):\n",
        "    \"\"\"\n",
        "    Display comprehensive evaluation results in a formatted way.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    aggregated_results : Dict\n",
        "        Results from aggregate_results() or evaluate_all_files()\n",
        "    \"\"\"\n",
        "    aggregated = aggregated_results.get('aggregated', aggregated_results)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Per-entity-type metrics\n",
        "    print(\"\\nPer-Entity-Type Metrics:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    metrics_data = []\n",
        "    for label_type in LABEL_TYPES:\n",
        "        metrics_data.append({\n",
        "            'Entity Type': label_type,\n",
        "            'Precision': f\"{aggregated['precision'][label_type]:.4f}\",\n",
        "            'Recall': f\"{aggregated['recall'][label_type]:.4f}\",\n",
        "            'F1-Score': f\"{aggregated['f1'][label_type]:.4f}\",\n",
        "            'TP': aggregated['tp'][label_type],\n",
        "            'FP': aggregated['fp'][label_type],\n",
        "            'FN': aggregated['fn'][label_type]\n",
        "        })\n",
        "    \n",
        "    # Add overall metrics\n",
        "    metrics_data.append({\n",
        "        'Entity Type': 'OVERALL (Micro-Avg)',\n",
        "        'Precision': f\"{aggregated['precision']['OVERALL']:.4f}\",\n",
        "        'Recall': f\"{aggregated['recall']['OVERALL']:.4f}\",\n",
        "        'F1-Score': f\"{aggregated['f1']['OVERALL']:.4f}\",\n",
        "        'TP': aggregated['tp']['OVERALL'],\n",
        "        'FP': aggregated['fp']['OVERALL'],\n",
        "        'FN': aggregated['fn']['OVERALL']\n",
        "    })\n",
        "    \n",
        "    df_metrics = pd.DataFrame(metrics_data)\n",
        "    print(df_metrics.to_string(index=False))\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Summary Statistics:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Total True Positives:  {aggregated['tp']['OVERALL']}\")\n",
        "    print(f\"Total False Positives: {aggregated['fp']['OVERALL']}\")\n",
        "    print(f\"Total False Negatives: {aggregated['fn']['OVERALL']}\")\n",
        "    print(f\"\\nOverall Precision: {aggregated['precision']['OVERALL']:.4f}\")\n",
        "    print(f\"Overall Recall:    {aggregated['recall']['OVERALL']:.4f}\")\n",
        "    print(f\"Overall F1-Score:  {aggregated['f1']['OVERALL']:.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"✓ Results display function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example: Evaluation with Task 2 Integration\n",
        "\n",
        "This cell demonstrates how to integrate with Task 2 to generate predictions and evaluate them.\n",
        "\n",
        "**Note**: This requires Task 2 functions to be available. You may need to:\n",
        "1. Copy relevant functions from Task 2 notebook\n",
        "2. Import them if they're in a module\n",
        "3. Or re-run Task 2 in the same session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example integration with Task 2\n",
        "# This is a template - adjust based on how Task 2 predictions are generated/saved\n",
        "\n",
        "def example_get_predictions(text_file_path: Path, text: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Example function to get predictions for a text file.\n",
        "    \n",
        "    In practice, this would:\n",
        "    1. Call Task 2's process_text_file() function\n",
        "    2. Convert annotation lines to entity dictionaries\n",
        "    3. Return list of entities\n",
        "    \n",
        "    For now, this is a placeholder that returns empty list.\n",
        "    Replace this with actual Task 2 integration.\n",
        "    \"\"\"\n",
        "    # Placeholder: return empty predictions\n",
        "    # In real implementation, integrate with Task 2:\n",
        "    #\n",
        "    # from task2_functions import process_text_file  # or however Task 2 is structured\n",
        "    # bio_tagged, annotation_lines = process_text_file(text_file_path, ner_pipeline, tokenizer)\n",
        "    # pred_entities = load_predictions(annotation_lines)\n",
        "    # return pred_entities\n",
        "    \n",
        "    return []\n",
        "\n",
        "# Example usage (commented out - uncomment and modify when Task 2 is integrated):\n",
        "\"\"\"\n",
        "# Get sample files for testing\n",
        "text_files = list(TEXT_DIR.glob(\"*.txt\"))[:10]  # Evaluate first 10 files\n",
        "\n",
        "# Evaluate with Task 2 predictions\n",
        "results = evaluate_all_files(\n",
        "    text_files=text_files,\n",
        "    get_predictions_func=example_get_predictions,\n",
        "    max_files=10\n",
        ")\n",
        "\n",
        "# Display results\n",
        "display_evaluation_results(results)\n",
        "\"\"\"\n",
        "\n",
        "print(\"✓ Example integration template defined\")\n",
        "print(\"  → Uncomment and modify the example code above when Task 2 is integrated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Edge Cases in Entity Matching\n",
        "\n",
        "This section documents important edge cases handled in the evaluation:\n",
        "\n",
        "### Edge Cases:\n",
        "\n",
        "1. **Overlapping Entities**\n",
        "   - If ground truth has entity at [10, 20] and prediction has [10, 25]:\n",
        "     - This is treated as a False Positive (boundaries don't match exactly)\n",
        "   - seqeval handles this by requiring exact boundary matches\n",
        "\n",
        "2. **Partial Matches**\n",
        "   - Entity with correct label but wrong boundaries is a False Positive\n",
        "   - Example: GT=\"ibuprofen\" [10, 19], Pred=\"ibuprofen\" [10, 20] → FP\n",
        "\n",
        "3. **Label Confusion**\n",
        "   - Correct boundaries but wrong label is a False Positive\n",
        "   - Example: GT=ADR [10, 19], Pred=Drug [10, 19] → FP\n",
        "\n",
        "4. **Multiple Ranges**\n",
        "   - Ground truth entities with multiple character ranges (semicolon-separated)\n",
        "   - Each range is treated as a separate entity for evaluation\n",
        "\n",
        "5. **Empty Predictions/Ground Truth**\n",
        "   - If no predictions: all ground truth entities are False Negatives\n",
        "   - If no ground truth: all predictions are False Positives\n",
        "\n",
        "### Why This Strict Matching?\n",
        "\n",
        "- **Medical Safety**: Exact boundaries ensure correct entity extraction\n",
        "- **Downstream Tasks**: Knowledge graphs, relation extraction need precise entities\n",
        "- **Reproducibility**: Standard CoNLL evaluation methodology\n",
        "- **Clinical Accuracy**: Partial matches can misrepresent medical conditions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test edge cases\n",
        "print(\"Edge Case Testing:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Example ground truth\n",
        "gt_example = [\n",
        "    {'label': 'ADR', 'text': 'drowsy', 'start': 10, 'end': 16},\n",
        "    {'label': 'Drug', 'text': 'ibuprofen', 'start': 30, 'end': 39},\n",
        "]\n",
        "\n",
        "# Test Case 1: Exact match\n",
        "pred_exact = [\n",
        "    {'label': 'ADR', 'text': 'drowsy', 'start': 10, 'end': 16},\n",
        "    {'label': 'Drug', 'text': 'ibuprofen', 'start': 30, 'end': 39},\n",
        "]\n",
        "result1 = entity_level_evaluation(gt_example, pred_exact)\n",
        "print(\"\\n1. Exact Match:\")\n",
        "print(f\"   TP: {result1['tp']['OVERALL']}, FP: {result1['fp']['OVERALL']}, FN: {result1['fn']['OVERALL']}\")\n",
        "print(f\"   Precision: {result1['precision']['OVERALL']:.4f}, Recall: {result1['recall']['OVERALL']:.4f}\")\n",
        "\n",
        "# Test Case 2: Boundary mismatch (partial match)\n",
        "pred_boundary = [\n",
        "    {'label': 'ADR', 'text': 'drowsy', 'start': 10, 'end': 17},  # Wrong boundary\n",
        "    {'label': 'Drug', 'text': 'ibuprofen', 'start': 30, 'end': 39},\n",
        "]\n",
        "result2 = entity_level_evaluation(gt_example, pred_boundary)\n",
        "print(\"\\n2. Boundary Mismatch (Partial Match):\")\n",
        "print(f\"   TP: {result2['tp']['OVERALL']}, FP: {result2['fp']['OVERALL']}, FN: {result2['fn']['OVERALL']}\")\n",
        "print(f\"   Note: Partial match treated as FP (no TP)\")\n",
        "\n",
        "# Test Case 3: Label confusion\n",
        "pred_label = [\n",
        "    {'label': 'ADR', 'text': 'drowsy', 'start': 10, 'end': 16},\n",
        "    {'label': 'ADR', 'text': 'ibuprofen', 'start': 30, 'end': 39},  # Wrong label\n",
        "]\n",
        "result3 = entity_level_evaluation(gt_example, pred_label)\n",
        "print(\"\\n3. Label Confusion:\")\n",
        "print(f\"   TP: {result3['tp']['OVERALL']}, FP: {result3['fp']['OVERALL']}, FN: {result3['fn']['OVERALL']}\")\n",
        "print(f\"   Note: Correct boundary but wrong label → FP\")\n",
        "\n",
        "# Test Case 4: Missing prediction\n",
        "pred_missing = [\n",
        "    {'label': 'ADR', 'text': 'drowsy', 'start': 10, 'end': 16},\n",
        "    # Missing Drug entity\n",
        "]\n",
        "result4 = entity_level_evaluation(gt_example, pred_missing)\n",
        "print(\"\\n4. Missing Prediction (False Negative):\")\n",
        "print(f\"   TP: {result4['tp']['OVERALL']}, FP: {result4['fp']['OVERALL']}, FN: {result4['fn']['OVERALL']}\")\n",
        "print(f\"   Note: Missing entity → FN\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ Edge case handling verified\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Summary\n",
        "\n",
        "This notebook provides a comprehensive evaluation framework for Medical NER with:\n",
        "\n",
        "### ✅ Features Implemented:\n",
        "\n",
        "1. **Ground Truth Loading**: Parses annotation files from 'original' subdirectory\n",
        "2. **Prediction Loading**: Handles predictions in same format (from Task 2)\n",
        "3. **Entity-Level Evaluation**: Exact boundary + label matching\n",
        "4. **seqeval Integration**: Standard NER evaluation metrics\n",
        "5. **Per-Entity-Type Metrics**: ADR, Drug, Disease, Symptom\n",
        "6. **Overall Micro-Averaged Metrics**: Aggregate performance\n",
        "7. **Confusion Matrix**: Visualize misclassifications\n",
        "8. **Edge Case Handling**: Overlapping entities, partial matches, label confusion\n",
        "\n",
        "### 📊 Evaluation Approach:\n",
        "\n",
        "- **Entity-Level**: Requires exact boundary AND label match for True Positive\n",
        "- **Strict Matching**: Partial matches → False Positive (encourages precise boundaries)\n",
        "- **Medical Focus**: Optimized for clinical downstream tasks\n",
        "\n",
        "### 🔧 Usage:\n",
        "\n",
        "1. Integrate with Task 2 to generate predictions\n",
        "2. Run evaluation on single file or full dataset\n",
        "3. Analyze per-entity-type and overall metrics\n",
        "4. Review confusion matrix for common errors\n",
        "\n",
        "### 📝 Notes:\n",
        "\n",
        "- Predictions must be generated using Task 2 pipeline or loaded from files\n",
        "- Evaluation uses exact boundary matching (standard CoNLL methodology)\n",
        "- seqeval provides additional token-level insights alongside entity-level metrics\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
