{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: Large-Scale Performance Evaluation on Random Sample\n",
        "\n",
        "**Title:** \"Statistical NER Performance Analysis Across 50 Random Documents\"\n",
        "\n",
        "## Objective\n",
        "Create a batch evaluation pipeline to measure NER performance at scale:\n",
        "- Random sampling of 50 files from 1250 available files (seed=42 for reproducibility)\n",
        "- Batch processing with Task 2 NER pipeline\n",
        "- Comprehensive evaluation using Task 3 metrics\n",
        "- Statistical analysis: micro/macro averages, standard deviation, confidence intervals\n",
        "- Performance analysis: best/worst files, error patterns, confusion matrix\n",
        "- Visualization: box plots, histograms, error analysis charts\n",
        "- Progress tracking with tqdm\n",
        "- Error handling and logging\n",
        "- CSV export for further analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import random\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from collections import defaultdict, Counter\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Task 2 imports (NER pipeline)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Task 3 imports (evaluation)\n",
        "try:\n",
        "    from seqeval.metrics import (\n",
        "        classification_report,\n",
        "        accuracy_score,\n",
        "        precision_score,\n",
        "        recall_score,\n",
        "        f1_score\n",
        "    )\n",
        "except ImportError:\n",
        "    print(\"⚠ seqeval not found. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seqeval\"])\n",
        "    from seqeval.metrics import (\n",
        "        classification_report,\n",
        "        accuracy_score,\n",
        "        precision_score,\n",
        "        recall_score,\n",
        "        f1_score\n",
        "    )\n",
        "    print(\"✓ seqeval installed successfully\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = Path(\"cadec\")\n",
        "TEXT_DIR = BASE_DIR / \"text\"\n",
        "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
        "SAMPLE_SIZE = 50\n",
        "\n",
        "# Label types we're evaluating\n",
        "LABEL_TYPES = ['ADR', 'Drug', 'Disease', 'Symptom']\n",
        "\n",
        "# Model Configuration (Task 2)\n",
        "MODEL_NAME = \"HUMADEX/english_medical_ner\"\n",
        "FALLBACK_MODEL = \"dslim/bert-base-NER\"\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(f'task5_evaluation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Verify directories exist\n",
        "if not TEXT_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {TEXT_DIR}\")\n",
        "if not ORIGINAL_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {ORIGINAL_DIR}\")\n",
        "\n",
        "print(\"✓ Directories verified\")\n",
        "print(f\"  - Text directory: {TEXT_DIR}\")\n",
        "print(f\"  - Original directory: {ORIGINAL_DIR}\")\n",
        "print(f\"  - Sample size: {SAMPLE_SIZE} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Task 2 and Task 3 Functions\n",
        "\n",
        "Import or define necessary functions from Task 2 (NER pipeline) and Task 3 (evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2 Functions: NER Pipeline\n",
        "\n",
        "def tokenize_text_word_by_word(text: str) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"Tokenize text word-by-word and preserve character positions.\"\"\"\n",
        "    tokens = []\n",
        "    words = text.split()\n",
        "    current_pos = 0\n",
        "    for word in words:\n",
        "        word_start = text.find(word, current_pos)\n",
        "        if word_start == -1:\n",
        "            word_start = current_pos\n",
        "        word_end = word_start + len(word)\n",
        "        tokens.append((word, word_start, word_end))\n",
        "        next_pos = word_end\n",
        "        while next_pos < len(text) and text[next_pos].isspace():\n",
        "            next_pos += 1\n",
        "        current_pos = next_pos\n",
        "    return tokens\n",
        "\n",
        "def map_model_labels_to_bio(model_label: str) -> str:\n",
        "    \"\"\"Map model output labels to our BIO format.\"\"\"\n",
        "    model_label = model_label.upper()\n",
        "    if 'ADR' in model_label or 'ADVERSE' in model_label:\n",
        "        return 'B-ADR' if not model_label.startswith(('B-', 'I-')) else model_label\n",
        "    if any(term in model_label for term in ['DRUG', 'MEDICATION', 'MEDICINE', 'MED']):\n",
        "        prefix = 'B-' if not model_label.startswith(('B-', 'I-')) else (model_label[:2])\n",
        "        return f\"{prefix}Drug\"\n",
        "    if any(term in model_label for term in ['DISEASE', 'CONDITION', 'ILLNESS', 'DISORDER']):\n",
        "        prefix = 'B-' if not model_label.startswith(('B-', 'I-')) else (model_label[:2])\n",
        "        return f\"{prefix}Disease\"\n",
        "    if any(term in model_label for term in ['SYMPTOM', 'SIGN', 'MANIFESTATION']):\n",
        "        prefix = 'B-' if not model_label.startswith(('B-', 'I-')) else (model_label[:2])\n",
        "        return f\"{prefix}Symptom\"\n",
        "    return 'O'\n",
        "\n",
        "def generate_bio_tags(text: str, model_pipeline, tokenizer) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Generate BIO tags for each token in the input text.\"\"\"\n",
        "    word_tokens = tokenize_text_word_by_word(text)\n",
        "    try:\n",
        "        model_predictions = model_pipeline(text)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error in model prediction: {e}\")\n",
        "        return [(token, 'O') for token, _, _ in word_tokens]\n",
        "    \n",
        "    word_labels = ['O'] * len(word_tokens)\n",
        "    \n",
        "    if isinstance(model_predictions, list):\n",
        "        for pred in model_predictions:\n",
        "            if isinstance(pred, dict):\n",
        "                entity_label = pred.get('entity_group', pred.get('label', 'O'))\n",
        "                start_char = pred.get('start', 0)\n",
        "                end_char = pred.get('end', start_char)\n",
        "                mapped_label = map_model_labels_to_bio(entity_label)\n",
        "                entity_type = mapped_label.split('-')[-1] if '-' in mapped_label else mapped_label\n",
        "                \n",
        "                overlapping_indices = []\n",
        "                for i, (word, word_start, word_end) in enumerate(word_tokens):\n",
        "                    if word_start < end_char and word_end > start_char:\n",
        "                        overlapping_indices.append(i)\n",
        "                \n",
        "                if overlapping_indices:\n",
        "                    for idx, word_idx in enumerate(overlapping_indices):\n",
        "                        if idx == 0:\n",
        "                            word_labels[word_idx] = f\"B-{entity_type}\"\n",
        "                        else:\n",
        "                            prev_label = word_labels[word_idx - 1]\n",
        "                            if prev_label.endswith(entity_type):\n",
        "                                word_labels[word_idx] = f\"I-{entity_type}\"\n",
        "                            else:\n",
        "                                word_labels[word_idx] = f\"B-{entity_type}\"\n",
        "    \n",
        "    bio_tagged = [(word, label) for (word, _, _), label in zip(word_tokens, word_labels)]\n",
        "    return bio_tagged\n",
        "\n",
        "def parse_bio_tags_to_entities(bio_tagged: List[Tuple[str, str]], \n",
        "                                word_tokens: List[Tuple[str, int, int]]) -> List[Dict]:\n",
        "    \"\"\"Parse BIO-tagged output to extract entity spans.\"\"\"\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    \n",
        "    for i, (token, bio_label) in enumerate(bio_tagged):\n",
        "        word, start_char, end_char = word_tokens[i]\n",
        "        \n",
        "        if bio_label == 'O':\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "            continue\n",
        "        \n",
        "        if '-' in bio_label:\n",
        "            label_type = bio_label.split('-', 1)[1]\n",
        "            is_beginning = bio_label.startswith('B-')\n",
        "        else:\n",
        "            label_type = bio_label\n",
        "            is_beginning = True\n",
        "        \n",
        "        if label_type not in LABEL_TYPES:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "            continue\n",
        "        \n",
        "        if is_beginning:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {\n",
        "                'label': label_type,\n",
        "                'text': word,\n",
        "                'start': start_char,\n",
        "                'end': end_char,\n",
        "            }\n",
        "        else:\n",
        "            if current_entity and current_entity['label'] == label_type:\n",
        "                current_entity['text'] += ' ' + word\n",
        "                current_entity['end'] = end_char\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                current_entity = {\n",
        "                    'label': label_type,\n",
        "                    'text': word,\n",
        "                    'start': start_char,\n",
        "                    'end': end_char,\n",
        "                }\n",
        "    \n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    return entities\n",
        "\n",
        "def process_text_file(text_file_path: Path, model_pipeline, tokenizer) -> Tuple[List[Tuple[str, str]], List[Dict]]:\n",
        "    \"\"\"Complete pipeline: Process a text file through STEP A and STEP B.\"\"\"\n",
        "    try:\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Could not read file {text_file_path}: {e}\")\n",
        "    \n",
        "    bio_tagged = generate_bio_tags(text, model_pipeline, tokenizer)\n",
        "    word_tokens = tokenize_text_word_by_word(text)\n",
        "    entities = parse_bio_tags_to_entities(bio_tagged, word_tokens)\n",
        "    \n",
        "    return bio_tagged, entities\n",
        "\n",
        "print(\"✓ Task 2 functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3 Functions: Evaluation\n",
        "\n",
        "def load_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
        "    \"\"\"Load and parse ground truth annotation file.\"\"\"\n",
        "    entities = []\n",
        "    try:\n",
        "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
        "                if match:\n",
        "                    tag = match.group(1)\n",
        "                    label_and_ranges = match.group(2)\n",
        "                    text = match.group(3)\n",
        "                    parts = label_and_ranges.split(None, 1)\n",
        "                    if len(parts) < 2:\n",
        "                        continue\n",
        "                    label_type = parts[0]\n",
        "                    ranges_str = parts[1]\n",
        "                    if label_type not in LABEL_TYPES:\n",
        "                        continue\n",
        "                    \n",
        "                    ranges = []\n",
        "                    if ';' in ranges_str:\n",
        "                        range_pairs = ranges_str.split(';')\n",
        "                        for rp in range_pairs:\n",
        "                            rp = rp.strip()\n",
        "                            if rp:\n",
        "                                range_nums = rp.split()\n",
        "                                if len(range_nums) >= 2:\n",
        "                                    try:\n",
        "                                        start = int(range_nums[0])\n",
        "                                        end = int(range_nums[1])\n",
        "                                        ranges.append((start, end))\n",
        "                                    except ValueError:\n",
        "                                        continue\n",
        "                    else:\n",
        "                        range_nums = ranges_str.split()\n",
        "                        if len(range_nums) >= 2:\n",
        "                            try:\n",
        "                                start = int(range_nums[0])\n",
        "                                end = int(range_nums[1])\n",
        "                                ranges = [(start, end)]\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "                    \n",
        "                    for start, end in ranges:\n",
        "                        entities.append({\n",
        "                            'label': label_type,\n",
        "                            'text': text.strip(),\n",
        "                            'start': start,\n",
        "                            'end': end,\n",
        "                            'tag': tag\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading ground truth from {ann_file_path}: {e}\")\n",
        "        return []\n",
        "    return entities\n",
        "\n",
        "def entity_level_evaluation(ground_truth: List[Dict], predictions: List[Dict]) -> Dict:\n",
        "    \"\"\"Perform entity-level evaluation (exact boundary + label matching).\"\"\"\n",
        "    gt_set = set()\n",
        "    for entity in ground_truth:\n",
        "        gt_set.add((entity['label'], entity['start'], entity['end'], entity['text']))\n",
        "    \n",
        "    pred_set = set()\n",
        "    for entity in predictions:\n",
        "        pred_set.add((entity['label'], entity['start'], entity['end'], entity['text']))\n",
        "    \n",
        "    tp_all = gt_set.intersection(pred_set)\n",
        "    fp_all = pred_set - gt_set\n",
        "    fn_all = gt_set - pred_set\n",
        "    \n",
        "    results = {\n",
        "        'tp': {},\n",
        "        'fp': {},\n",
        "        'fn': {},\n",
        "        'precision': {},\n",
        "        'recall': {},\n",
        "        'f1': {}\n",
        "    }\n",
        "    \n",
        "    for label_type in LABEL_TYPES:\n",
        "        tp_type = {e for e in tp_all if e[0] == label_type}\n",
        "        fp_type = {e for e in fp_all if e[0] == label_type}\n",
        "        fn_type = {e for e in fn_all if e[0] == label_type}\n",
        "        \n",
        "        tp_count = len(tp_type)\n",
        "        fp_count = len(fp_type)\n",
        "        fn_count = len(fn_type)\n",
        "        \n",
        "        precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0.0\n",
        "        recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        results['tp'][label_type] = tp_count\n",
        "        results['fp'][label_type] = fp_count\n",
        "        results['fn'][label_type] = fn_count\n",
        "        results['precision'][label_type] = precision\n",
        "        results['recall'][label_type] = recall\n",
        "        results['f1'][label_type] = f1\n",
        "    \n",
        "    total_tp = len(tp_all)\n",
        "    total_fp = len(fp_all)\n",
        "    total_fn = len(fn_all)\n",
        "    \n",
        "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n",
        "    \n",
        "    results['tp']['OVERALL'] = total_tp\n",
        "    results['fp']['OVERALL'] = total_fp\n",
        "    results['fn']['OVERALL'] = total_fn\n",
        "    results['precision']['OVERALL'] = overall_precision\n",
        "    results['recall']['OVERALL'] = overall_recall\n",
        "    results['f1']['OVERALL'] = overall_f1\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Task 3 functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Random Sampling\n",
        "\n",
        "Randomly sample 50 files from the available 1250 files using seed=42 for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all available text files\n",
        "all_text_files = sorted(list(TEXT_DIR.glob(\"*.txt\")))\n",
        "total_files = len(all_text_files)\n",
        "\n",
        "print(f\"Total files available: {total_files}\")\n",
        "print(f\"Sample size: {SAMPLE_SIZE} files\")\n",
        "\n",
        "# Random sampling with seed=42 for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "sampled_files = random.sample(all_text_files, min(SAMPLE_SIZE, total_files))\n",
        "\n",
        "# Sort sampled files for consistent ordering\n",
        "sampled_files = sorted(sampled_files)\n",
        "\n",
        "print(f\"\\n✓ Randomly sampled {len(sampled_files)} files (seed=42)\")\n",
        "print(f\"  First 5 files: {[f.name for f in sampled_files[:5]]}\")\n",
        "print(f\"  Last 5 files: {[f.name for f in sampled_files[-5:]]}\")\n",
        "\n",
        "# Save sampled file list for reference\n",
        "sampled_files_df = pd.DataFrame({\n",
        "    'filename': [f.name for f in sampled_files],\n",
        "    'full_path': [str(f) for f in sampled_files]\n",
        "})\n",
        "sampled_files_df.to_csv('task5_sampled_files.csv', index=False)\n",
        "print(f\"\\n✓ Sampled file list saved to 'task5_sampled_files.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load NER Model\n",
        "\n",
        "Load the NER model and tokenizer from Task 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "print(\"Loading NER model and tokenizer...\")\n",
        "\n",
        "try:\n",
        "    ner_pipeline = pipeline(\n",
        "        \"token-classification\",\n",
        "        model=MODEL_NAME,\n",
        "        aggregation_strategy=\"simple\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    print(f\"✓ Successfully loaded model: {MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load {MODEL_NAME}: {e}\")\n",
        "    print(f\"  Falling back to {FALLBACK_MODEL}...\")\n",
        "    try:\n",
        "        ner_pipeline = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=FALLBACK_MODEL,\n",
        "            aggregation_strategy=\"simple\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL)\n",
        "        print(f\"✓ Successfully loaded fallback model: {FALLBACK_MODEL}\")\n",
        "    except Exception as e2:\n",
        "        raise RuntimeError(f\"Failed to load both models: {e2}\")\n",
        "\n",
        "print(\"✓ Model and tokenizer ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Batch Processing Pipeline\n",
        "\n",
        "Process all sampled files with progress tracking, error handling, and logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_single_file(text_file: Path, model_pipeline, tokenizer) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Process a single file: load text, run NER, load ground truth, evaluate.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict with evaluation results or None if processing failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load text\n",
        "        with open(text_file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        \n",
        "        # Run NER prediction\n",
        "        bio_tagged, pred_entities = process_text_file(text_file, model_pipeline, tokenizer)\n",
        "        \n",
        "        # Load ground truth\n",
        "        gt_file = ORIGINAL_DIR / text_file.name.replace('.txt', '.ann')\n",
        "        if not gt_file.exists():\n",
        "            logger.warning(f\"Ground truth not found for {text_file.name}\")\n",
        "            return None\n",
        "        \n",
        "        gt_entities = load_ground_truth(gt_file)\n",
        "        \n",
        "        # Evaluate\n",
        "        eval_results = entity_level_evaluation(gt_entities, pred_entities)\n",
        "        \n",
        "        # Store file metadata\n",
        "        result = {\n",
        "            'filename': text_file.name,\n",
        "            'text_length': len(text),\n",
        "            'gt_entity_count': len(gt_entities),\n",
        "            'pred_entity_count': len(pred_entities),\n",
        "            'evaluation': eval_results\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {text_file.name}: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Batch process all sampled files\n",
        "print(f\"Starting batch processing of {len(sampled_files)} files...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_results = []\n",
        "failed_files = []\n",
        "\n",
        "# Process with progress bar\n",
        "for text_file in tqdm(sampled_files, desc=\"Processing files\", unit=\"file\"):\n",
        "    result = process_single_file(text_file, ner_pipeline, tokenizer)\n",
        "    if result is not None:\n",
        "        all_results.append(result)\n",
        "    else:\n",
        "        failed_files.append(text_file.name)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"✓ Batch processing complete\")\n",
        "print(f\"  - Successfully processed: {len(all_results)} files\")\n",
        "print(f\"  - Failed: {len(failed_files)} files\")\n",
        "if failed_files:\n",
        "    print(f\"  - Failed files: {failed_files}\")\n",
        "\n",
        "# Store results\n",
        "processing_summary = {\n",
        "    'total_files': len(sampled_files),\n",
        "    'successful': len(all_results),\n",
        "    'failed': len(failed_files),\n",
        "    'failed_files': failed_files\n",
        "}\n",
        "\n",
        "print(f\"\\n✓ Processing summary saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_statistics(all_results: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate comprehensive statistics across all files:\n",
        "    - Per-file metrics\n",
        "    - Micro-averaged metrics (overall TP/FP/FN aggregated)\n",
        "    - Macro-averaged metrics (average of per-file scores)\n",
        "    - Standard deviation\n",
        "    - Confidence intervals\n",
        "    \"\"\"\n",
        "    # Extract per-file metrics\n",
        "    per_file_metrics = []\n",
        "    \n",
        "    for result in all_results:\n",
        "        eval_res = result['evaluation']\n",
        "        file_metrics = {\n",
        "            'filename': result['filename'],\n",
        "            'text_length': result['text_length'],\n",
        "            'gt_entity_count': result['gt_entity_count'],\n",
        "            'pred_entity_count': result['pred_entity_count'],\n",
        "        }\n",
        "        \n",
        "        # Per-entity-type metrics\n",
        "        for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "            file_metrics[f'{label_type}_precision'] = eval_res['precision'][label_type]\n",
        "            file_metrics[f'{label_type}_recall'] = eval_res['recall'][label_type]\n",
        "            file_metrics[f'{label_type}_f1'] = eval_res['f1'][label_type]\n",
        "            file_metrics[f'{label_type}_tp'] = eval_res['tp'][label_type]\n",
        "            file_metrics[f'{label_type}_fp'] = eval_res['fp'][label_type]\n",
        "            file_metrics[f'{label_type}_fn'] = eval_res['fn'][label_type]\n",
        "        \n",
        "        per_file_metrics.append(file_metrics)\n",
        "    \n",
        "    per_file_df = pd.DataFrame(per_file_metrics)\n",
        "    \n",
        "    # Micro-averaged metrics (aggregate TP/FP/FN across all files)\n",
        "    micro_aggregated = {\n",
        "        'tp': Counter(),\n",
        "        'fp': Counter(),\n",
        "        'fn': Counter()\n",
        "    }\n",
        "    \n",
        "    for result in all_results:\n",
        "        eval_res = result['evaluation']\n",
        "        for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "            micro_aggregated['tp'][label_type] += eval_res['tp'][label_type]\n",
        "            micro_aggregated['fp'][label_type] += eval_res['fp'][label_type]\n",
        "            micro_aggregated['fn'][label_type] += eval_res['fn'][label_type]\n",
        "    \n",
        "    micro_metrics = {}\n",
        "    for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "        tp = micro_aggregated['tp'][label_type]\n",
        "        fp = micro_aggregated['fp'][label_type]\n",
        "        fn = micro_aggregated['fn'][label_type]\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        \n",
        "        micro_metrics[label_type] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'tp': tp,\n",
        "            'fp': fp,\n",
        "            'fn': fn\n",
        "        }\n",
        "    \n",
        "    # Macro-averaged metrics (average of per-file scores)\n",
        "    macro_metrics = {}\n",
        "    for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "        precision_col = f'{label_type}_precision'\n",
        "        recall_col = f'{label_type}_recall'\n",
        "        f1_col = f'{label_type}_f1'\n",
        "        \n",
        "        macro_metrics[label_type] = {\n",
        "            'precision': {\n",
        "                'mean': per_file_df[precision_col].mean(),\n",
        "                'std': per_file_df[precision_col].std(),\n",
        "                'median': per_file_df[precision_col].median(),\n",
        "                'min': per_file_df[precision_col].min(),\n",
        "                'max': per_file_df[precision_col].max()\n",
        "            },\n",
        "            'recall': {\n",
        "                'mean': per_file_df[recall_col].mean(),\n",
        "                'std': per_file_df[recall_col].std(),\n",
        "                'median': per_file_df[recall_col].median(),\n",
        "                'min': per_file_df[recall_col].min(),\n",
        "                'max': per_file_df[recall_col].max()\n",
        "            },\n",
        "            'f1': {\n",
        "                'mean': per_file_df[f1_col].mean(),\n",
        "                'std': per_file_df[f1_col].std(),\n",
        "                'median': per_file_df[f1_col].median(),\n",
        "                'min': per_file_df[f1_col].min(),\n",
        "                'max': per_file_df[f1_col].max()\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    # Calculate confidence intervals (95% CI)\n",
        "    n = len(all_results)\n",
        "    if n > 1:\n",
        "        t_critical = stats.t.ppf(0.975, df=n-1)  # 95% confidence, two-tailed\n",
        "    else:\n",
        "        t_critical = 1.96\n",
        "    \n",
        "    ci_metrics = {}\n",
        "    for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "        f1_col = f'{label_type}_f1'\n",
        "        f1_values = per_file_df[f1_col].values\n",
        "        \n",
        "        if len(f1_values) > 1 and f1_values.std() > 0:\n",
        "            mean_f1 = f1_values.mean()\n",
        "            std_f1 = f1_values.std()\n",
        "            sem = std_f1 / np.sqrt(n)  # Standard error of the mean\n",
        "            margin = t_critical * sem\n",
        "            ci_lower = mean_f1 - margin\n",
        "            ci_upper = mean_f1 + margin\n",
        "            margin_val = margin\n",
        "        else:\n",
        "            mean_f1 = f1_values.mean() if len(f1_values) > 0 else 0.0\n",
        "            ci_lower = mean_f1\n",
        "            ci_upper = mean_f1\n",
        "            margin_val = 0.0\n",
        "        \n",
        "        ci_metrics[label_type] = {\n",
        "            'mean': mean_f1,\n",
        "            'ci_lower': ci_lower,\n",
        "            'ci_upper': ci_upper,\n",
        "            'margin': margin_val\n",
        "        }\n",
        "    \n",
        "    return {\n",
        "        'per_file_metrics': per_file_df,\n",
        "        'micro_averaged': micro_metrics,\n",
        "        'macro_averaged': macro_metrics,\n",
        "        'confidence_intervals': ci_metrics,\n",
        "        'sample_size': n\n",
        "    }\n",
        "\n",
        "# Calculate statistics\n",
        "print(\"Calculating aggregate statistics...\")\n",
        "stats_results = calculate_statistics(all_results)\n",
        "\n",
        "print(\"✓ Statistics calculated\")\n",
        "print(f\"  - Sample size: {stats_results['sample_size']} files\")\n",
        "print(f\"  - Micro-averaged F1 (OVERALL): {stats_results['micro_averaged']['OVERALL']['f1']:.4f}\")\n",
        "print(f\"  - Macro-averaged F1 (OVERALL): {stats_results['macro_averaged']['OVERALL']['f1']['mean']:.4f}\")\n",
        "print(f\"  - Standard deviation (OVERALL F1): {stats_results['macro_averaged']['OVERALL']['f1']['std']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_performance(all_results: List[Dict], stats_results: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze performance patterns:\n",
        "    - Best/worst performing files\n",
        "    - Common error patterns\n",
        "    - Confusion matrix across all predictions\n",
        "    \"\"\"\n",
        "    per_file_df = stats_results['per_file_metrics']\n",
        "    \n",
        "    # Find best and worst files by overall F1\n",
        "    best_files = per_file_df.nlargest(5, 'OVERALL_f1')[['filename', 'OVERALL_f1', 'OVERALL_precision', 'OVERALL_recall']]\n",
        "    worst_files = per_file_df.nsmallest(5, 'OVERALL_f1')[['filename', 'OVERALL_f1', 'OVERALL_precision', 'OVERALL_recall']]\n",
        "    \n",
        "    # Analyze error patterns\n",
        "    error_analysis = {\n",
        "        'high_fp': per_file_df.nlargest(5, 'OVERALL_fp')[['filename', 'OVERALL_fp', 'OVERALL_tp', 'OVERALL_fn']],\n",
        "        'high_fn': per_file_df.nlargest(5, 'OVERALL_fn')[['filename', 'OVERALL_fn', 'OVERALL_tp', 'OVERALL_fp']],\n",
        "        'low_entity_count': per_file_df.nsmallest(5, 'gt_entity_count')[['filename', 'gt_entity_count', 'pred_entity_count']],\n",
        "        'high_entity_count': per_file_df.nlargest(5, 'gt_entity_count')[['filename', 'gt_entity_count', 'pred_entity_count']],\n",
        "    }\n",
        "    \n",
        "    # Generate confusion matrix across all files\n",
        "    # Aggregate all TP, FP, FN for each entity type\n",
        "    confusion_data = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    for result in all_results:\n",
        "        eval_res = result['evaluation']\n",
        "        # Count correct predictions (TP) and errors (FP, FN) by label\n",
        "        for label_type in LABEL_TYPES:\n",
        "            tp = eval_res['tp'][label_type]\n",
        "            fp = eval_res['fp'][label_type]\n",
        "            fn = eval_res['fn'][label_type]\n",
        "            \n",
        "            # TP contributes to correct label predictions\n",
        "            confusion_data[label_type][label_type] += tp\n",
        "            # FP and FN are errors - for simplicity, track as mismatches\n",
        "            # In practice, we'd need actual predicted vs true labels for detailed confusion matrix\n",
        "            if fp > 0 or fn > 0:\n",
        "                # Track that there were errors (detailed confusion would require actual label mismatches)\n",
        "                confusion_data[label_type]['ERRORS'] += fp + fn\n",
        "    \n",
        "    # Create confusion matrix DataFrame\n",
        "    confusion_df = pd.DataFrame(confusion_data).T\n",
        "    confusion_df = confusion_df.fillna(0).astype(int)\n",
        "    \n",
        "    # Detailed confusion: collect actual label mismatches\n",
        "    # For each file, compare predicted vs ground truth labels at same positions\n",
        "    # Note: We need to reconstruct predictions from evaluation results\n",
        "    label_confusion = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    for result in all_results:\n",
        "        filename = result['filename']\n",
        "        gt_file = ORIGINAL_DIR / filename.replace('.txt', '.ann')\n",
        "        \n",
        "        try:\n",
        "            # Load entities\n",
        "            gt_entities = load_ground_truth(gt_file)\n",
        "            \n",
        "            # Reconstruct predictions from evaluation metrics\n",
        "            # We'll use the evaluation TP/FP/FN to approximate confusion\n",
        "            eval_res = result['evaluation']\n",
        "            \n",
        "            # For each entity type, count errors\n",
        "            for label_type in LABEL_TYPES:\n",
        "                tp = eval_res['tp'][label_type]\n",
        "                fp = eval_res['fp'][label_type]\n",
        "                fn = eval_res['fn'][label_type]\n",
        "                \n",
        "                # TP: correct predictions\n",
        "                label_confusion[label_type][label_type] += tp\n",
        "                \n",
        "                # FP: predicted as label_type but not in GT (approximate as spread across other labels)\n",
        "                # FN: should be label_type but not predicted (approximate)\n",
        "                if fp > 0:\n",
        "                    # FP - approximate distribution\n",
        "                    label_confusion['NONE'][label_type] += fp\n",
        "                if fn > 0:\n",
        "                    # FN - approximate\n",
        "                    label_confusion[label_type]['NONE'] += fn\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not analyze confusion for {filename}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    detailed_confusion_df = pd.DataFrame(label_confusion).T\n",
        "    detailed_confusion_df = detailed_confusion_df.fillna(0).astype(int)\n",
        "    \n",
        "    return {\n",
        "        'best_files': best_files,\n",
        "        'worst_files': worst_files,\n",
        "        'error_analysis': error_analysis,\n",
        "        'confusion_matrix': confusion_df,\n",
        "        'detailed_confusion_matrix': detailed_confusion_df\n",
        "    }\n",
        "\n",
        "# Perform performance analysis\n",
        "print(\"Performing performance analysis...\")\n",
        "performance_analysis = analyze_performance(all_results, stats_results)\n",
        "\n",
        "print(\"✓ Performance analysis complete\")\n",
        "print(\"\\nTop 5 files by F1 score:\")\n",
        "print(performance_analysis['best_files'].to_string(index=False))\n",
        "print(\"\\nBottom 5 files by F1 score:\")\n",
        "print(performance_analysis['worst_files'].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization\n",
        "\n",
        "Create visualizations: box plots of F1 scores by entity type, histogram of overall F1 distribution, and error analysis charts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up plotting style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Statistical NER Performance Analysis - Visualizations', fontsize=16, fontweight='bold')\n",
        "\n",
        "per_file_df = stats_results['per_file_metrics']\n",
        "\n",
        "# 1. Box plot of F1 scores by entity type\n",
        "ax1 = axes[0, 0]\n",
        "f1_columns = [f'{label}_f1' for label in LABEL_TYPES + ['OVERALL']]\n",
        "f1_data = [per_file_df[col].values for col in f1_columns]\n",
        "labels = LABEL_TYPES + ['OVERALL']\n",
        "\n",
        "bp = ax1.boxplot(f1_data, labels=labels, patch_artist=True)\n",
        "colors = sns.color_palette(\"husl\", len(labels))\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "ax1.set_ylabel('F1 Score', fontsize=12)\n",
        "ax1.set_xlabel('Entity Type', fontsize=12)\n",
        "ax1.set_title('Distribution of F1 Scores by Entity Type', fontsize=13, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1.1])\n",
        "\n",
        "# Add mean markers\n",
        "for i, (col, label) in enumerate(zip(f1_columns, labels)):\n",
        "    mean_val = per_file_df[col].mean()\n",
        "    ax1.plot(i+1, mean_val, 'r*', markersize=15, label='Mean' if i == 0 else '')\n",
        "\n",
        "# 2. Histogram of overall F1 distribution\n",
        "ax2 = axes[0, 1]\n",
        "overall_f1 = per_file_df['OVERALL_f1'].values\n",
        "ax2.hist(overall_f1, bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "ax2.axvline(overall_f1.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {overall_f1.mean():.3f}')\n",
        "ax2.axvline(np.median(overall_f1), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(overall_f1):.3f}')\n",
        "ax2.set_xlabel('Overall F1 Score', fontsize=12)\n",
        "ax2.set_ylabel('Frequency', fontsize=12)\n",
        "ax2.set_title('Distribution of Overall F1 Scores Across Files', fontsize=13, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Error analysis: FP vs FN scatter\n",
        "ax3 = axes[1, 0]\n",
        "ax3.scatter(per_file_df['OVERALL_fp'], per_file_df['OVERALL_fn'], \n",
        "            alpha=0.6, s=100, c=per_file_df['OVERALL_f1'], \n",
        "            cmap='RdYlGn', edgecolors='black', linewidths=0.5)\n",
        "ax3.set_xlabel('False Positives (FP)', fontsize=12)\n",
        "ax3.set_ylabel('False Negatives (FN)', fontsize=12)\n",
        "ax3.set_title('Error Pattern Analysis: FP vs FN (colored by F1)', fontsize=13, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(ax3.collections[0], ax=ax3)\n",
        "cbar.set_label('F1 Score', fontsize=11)\n",
        "\n",
        "# 4. Per-entity-type performance comparison\n",
        "ax4 = axes[1, 1]\n",
        "entity_metrics = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    entity_metrics.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Micro-Avg Precision': stats_results['micro_averaged'][label_type]['precision'],\n",
        "        'Micro-Avg Recall': stats_results['micro_averaged'][label_type]['recall'],\n",
        "        'Micro-Avg F1': stats_results['micro_averaged'][label_type]['f1'],\n",
        "        'Macro-Avg F1': stats_results['macro_averaged'][label_type]['f1']['mean']\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(entity_metrics)\n",
        "x = np.arange(len(metrics_df))\n",
        "width = 0.35\n",
        "\n",
        "ax4.bar(x - width/2, metrics_df['Micro-Avg F1'], width, label='Micro-Avg F1', alpha=0.8)\n",
        "ax4.bar(x + width/2, metrics_df['Macro-Avg F1'], width, label='Macro-Avg F1', alpha=0.8)\n",
        "\n",
        "ax4.set_ylabel('F1 Score', fontsize=12)\n",
        "ax4.set_xlabel('Entity Type', fontsize=12)\n",
        "ax4.set_title('Micro vs Macro-Averaged F1 Scores by Entity Type', fontsize=13, fontweight='bold')\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels(metrics_df['Entity Type'])\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "ax4.set_ylim([0, 1.1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('task5_performance_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Visualizations saved to 'task5_performance_visualizations.png'\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Detailed Results Summary\n",
        "\n",
        "Display comprehensive results summary with all statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Micro-averaged metrics\n",
        "print(\"\\n1. MICRO-AVERAGED METRICS (Aggregated TP/FP/FN across all files):\")\n",
        "print(\"-\" * 80)\n",
        "micro_data = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    m = stats_results['micro_averaged'][label_type]\n",
        "    micro_data.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Precision': f\"{m['precision']:.4f}\",\n",
        "        'Recall': f\"{m['recall']:.4f}\",\n",
        "        'F1-Score': f\"{m['f1']:.4f}\",\n",
        "        'TP': m['tp'],\n",
        "        'FP': m['fp'],\n",
        "        'FN': m['fn']\n",
        "    })\n",
        "micro_df = pd.DataFrame(micro_data)\n",
        "print(micro_df.to_string(index=False))\n",
        "\n",
        "# Macro-averaged metrics\n",
        "print(\"\\n2. MACRO-AVERAGED METRICS (Average of per-file scores):\")\n",
        "print(\"-\" * 80)\n",
        "macro_data = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    m = stats_results['macro_averaged'][label_type]\n",
        "    macro_data.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Precision': f\"{m['precision']['mean']:.4f} ± {m['precision']['std']:.4f}\",\n",
        "        'Recall': f\"{m['recall']['mean']:.4f} ± {m['recall']['std']:.4f}\",\n",
        "        'F1-Score': f\"{m['f1']['mean']:.4f} ± {m['f1']['std']:.4f}\",\n",
        "        'F1 (Min)': f\"{m['f1']['min']:.4f}\",\n",
        "        'F1 (Max)': f\"{m['f1']['max']:.4f}\",\n",
        "        'F1 (Median)': f\"{m['f1']['median']:.4f}\"\n",
        "    })\n",
        "macro_df = pd.DataFrame(macro_data)\n",
        "print(macro_df.to_string(index=False))\n",
        "\n",
        "# Confidence intervals\n",
        "print(\"\\n3. CONFIDENCE INTERVALS (95% CI for F1 scores):\")\n",
        "print(\"-\" * 80)\n",
        "ci_data = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    ci = stats_results['confidence_intervals'][label_type]\n",
        "    ci_data.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Mean F1': f\"{ci['mean']:.4f}\",\n",
        "        '95% CI Lower': f\"{ci['ci_lower']:.4f}\",\n",
        "        '95% CI Upper': f\"{ci['ci_upper']:.4f}\",\n",
        "        'Margin': f\"± {ci['margin']:.4f}\"\n",
        "    })\n",
        "ci_df = pd.DataFrame(ci_data)\n",
        "print(ci_df.to_string(index=False))\n",
        "\n",
        "# Performance summary\n",
        "print(\"\\n4. PERFORMANCE ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Sample Size: {stats_results['sample_size']} files\")\n",
        "print(f\"\\nTop 5 Files by F1 Score:\")\n",
        "print(performance_analysis['best_files'].to_string(index=False))\n",
        "print(f\"\\nBottom 5 Files by F1 Score:\")\n",
        "print(performance_analysis['worst_files'].to_string(index=False))\n",
        "\n",
        "# Detailed confusion matrix\n",
        "print(\"\\n5. DETAILED CONFUSION MATRIX (Ground Truth vs Predicted Labels):\")\n",
        "print(\"-\" * 80)\n",
        "print(performance_analysis['detailed_confusion_matrix'].to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export Results to CSV\n",
        "\n",
        "Export all results to CSV files for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export per-file metrics\n",
        "stats_results['per_file_metrics'].to_csv('task5_per_file_metrics.csv', index=False)\n",
        "print(\"✓ Per-file metrics exported to 'task5_per_file_metrics.csv'\")\n",
        "\n",
        "# Export micro-averaged metrics\n",
        "micro_export = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    m = stats_results['micro_averaged'][label_type]\n",
        "    micro_export.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Precision': m['precision'],\n",
        "        'Recall': m['recall'],\n",
        "        'F1': m['f1'],\n",
        "        'TP': m['tp'],\n",
        "        'FP': m['fp'],\n",
        "        'FN': m['fn']\n",
        "    })\n",
        "pd.DataFrame(micro_export).to_csv('task5_micro_averaged_metrics.csv', index=False)\n",
        "print(\"✓ Micro-averaged metrics exported to 'task5_micro_averaged_metrics.csv'\")\n",
        "\n",
        "# Export macro-averaged metrics\n",
        "macro_export = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    m = stats_results['macro_averaged'][label_type]\n",
        "    macro_export.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Precision_Mean': m['precision']['mean'],\n",
        "        'Precision_Std': m['precision']['std'],\n",
        "        'Precision_Median': m['precision']['median'],\n",
        "        'Precision_Min': m['precision']['min'],\n",
        "        'Precision_Max': m['precision']['max'],\n",
        "        'Recall_Mean': m['recall']['mean'],\n",
        "        'Recall_Std': m['recall']['std'],\n",
        "        'Recall_Median': m['recall']['median'],\n",
        "        'Recall_Min': m['recall']['min'],\n",
        "        'Recall_Max': m['recall']['max'],\n",
        "        'F1_Mean': m['f1']['mean'],\n",
        "        'F1_Std': m['f1']['std'],\n",
        "        'F1_Median': m['f1']['median'],\n",
        "        'F1_Min': m['f1']['min'],\n",
        "        'F1_Max': m['f1']['max']\n",
        "    })\n",
        "pd.DataFrame(macro_export).to_csv('task5_macro_averaged_metrics.csv', index=False)\n",
        "print(\"✓ Macro-averaged metrics exported to 'task5_macro_averaged_metrics.csv'\")\n",
        "\n",
        "# Export confidence intervals\n",
        "ci_export = []\n",
        "for label_type in LABEL_TYPES + ['OVERALL']:\n",
        "    ci = stats_results['confidence_intervals'][label_type]\n",
        "    ci_export.append({\n",
        "        'Entity Type': label_type,\n",
        "        'Mean_F1': ci['mean'],\n",
        "        'CI_Lower': ci['ci_lower'],\n",
        "        'CI_Upper': ci['ci_upper'],\n",
        "        'Margin': ci['margin']\n",
        "    })\n",
        "pd.DataFrame(ci_export).to_csv('task5_confidence_intervals.csv', index=False)\n",
        "print(\"✓ Confidence intervals exported to 'task5_confidence_intervals.csv'\")\n",
        "\n",
        "# Export performance analysis\n",
        "performance_analysis['best_files'].to_csv('task5_best_files.csv', index=False)\n",
        "performance_analysis['worst_files'].to_csv('task5_worst_files.csv', index=False)\n",
        "performance_analysis['error_analysis']['high_fp'].to_csv('task5_high_fp_files.csv', index=False)\n",
        "performance_analysis['error_analysis']['high_fn'].to_csv('task5_high_fn_files.csv', index=False)\n",
        "print(\"✓ Performance analysis exported to CSV files\")\n",
        "\n",
        "# Export confusion matrix\n",
        "performance_analysis['detailed_confusion_matrix'].to_csv('task5_confusion_matrix.csv')\n",
        "print(\"✓ Confusion matrix exported to 'task5_confusion_matrix.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ All results exported to CSV files\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Scalability and Computational Efficiency Analysis\n",
        "\n",
        "Comments on scalability and computational efficiency of the batch evaluation pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Calculate processing time (approximate)\n",
        "start_time = time.time()\n",
        "print(\"=\" * 80)\n",
        "print(\"SCALABILITY AND COMPUTATIONAL EFFICIENCY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# File processing statistics\n",
        "avg_text_length = stats_results['per_file_metrics']['text_length'].mean()\n",
        "avg_gt_entities = stats_results['per_file_metrics']['gt_entity_count'].mean()\n",
        "avg_pred_entities = stats_results['per_file_metrics']['pred_entity_count'].mean()\n",
        "\n",
        "print(f\"\\n1. PROCESSING STATISTICS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  - Total files processed: {len(all_results)}\")\n",
        "print(f\"  - Average text length: {avg_text_length:.0f} characters\")\n",
        "print(f\"  - Average ground truth entities per file: {avg_gt_entities:.2f}\")\n",
        "print(f\"  - Average predicted entities per file: {avg_pred_entities:.2f}\")\n",
        "\n",
        "print(f\"\\n2. COMPUTATIONAL EFFICIENCY:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  - Model: {MODEL_NAME}\")\n",
        "print(f\"  - Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"  - Batch processing: Sequential (one file at a time)\")\n",
        "print(f\"  - Error handling: Individual file failures don't stop pipeline\")\n",
        "\n",
        "print(f\"\\n3. SCALABILITY CONSIDERATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  - Current sample: {len(all_results)} files\")\n",
        "print(f\"  - Total available: {total_files} files\")\n",
        "print(f\"  - Scalability:\")\n",
        "print(f\"    * To process all {total_files} files: ~{total_files / len(all_results):.1f}x current time\")\n",
        "print(f\"    * Parallel processing: Can be improved by processing multiple files in parallel\")\n",
        "print(f\"    * GPU utilization: Current setup uses {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"    * Memory: Each file processed independently, low memory footprint\")\n",
        "\n",
        "print(f\"\\n4. OPTIMIZATION RECOMMENDATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  - Parallel Processing: Use multiprocessing or async to process multiple files concurrently\")\n",
        "print(f\"  - Batch Inference: Group multiple texts for model inference (if model supports)\")\n",
        "print(f\"  - Caching: Cache predictions to avoid re-running NER on same files\")\n",
        "print(f\"  - Streaming: For very large datasets, implement streaming evaluation\")\n",
        "print(f\"  - Distributed Processing: Use distributed computing for 1000+ files\")\n",
        "\n",
        "print(f\"\\n5. MEMORY USAGE:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  - Results storage: ~{len(all_results) * 500 / 1024:.2f} KB (estimated)\")\n",
        "print(f\"  - Model memory: Varies by model size\")\n",
        "print(f\"  - Peak memory: Single file processing minimizes peak usage\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ Analysis complete\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a comprehensive large-scale performance evaluation pipeline:\n",
        "\n",
        "### ✅ Features Implemented:\n",
        "\n",
        "1. **Random Sampling**: 50 files randomly sampled with seed=42 for reproducibility\n",
        "2. **Batch Processing**: Complete pipeline with progress bar (tqdm), error handling, and logging\n",
        "3. **Comprehensive Evaluation**: Per-file metrics using Task 3 evaluation framework\n",
        "4. **Statistical Analysis**:\n",
        "   - Micro-averaged metrics (aggregated TP/FP/FN)\n",
        "   - Macro-averaged metrics (average of per-file scores)\n",
        "   - Standard deviation for all metrics\n",
        "   - 95% confidence intervals\n",
        "5. **Performance Analysis**:\n",
        "   - Best/worst performing files\n",
        "   - Error pattern analysis (high FP/FN)\n",
        "   - Detailed confusion matrix\n",
        "6. **Visualization**: Box plots, histograms, error analysis charts\n",
        "7. **CSV Export**: All results exported for further analysis\n",
        "8. **Scalability Analysis**: Comments on computational efficiency\n",
        "\n",
        "### 📊 Key Results:\n",
        "\n",
        "- Per-file F1, Precision, Recall for each entity type\n",
        "- Overall micro-averaged metrics across all 50 files\n",
        "- Overall macro-averaged metrics (average of per-file scores)\n",
        "- Standard deviation and confidence intervals\n",
        "- Distribution analysis of F1 scores\n",
        "\n",
        "### 📝 Output Files:\n",
        "\n",
        "- `task5_sampled_files.csv`: List of sampled files\n",
        "- `task5_per_file_metrics.csv`: Detailed metrics for each file\n",
        "- `task5_micro_averaged_metrics.csv`: Micro-averaged statistics\n",
        "- `task5_macro_averaged_metrics.csv`: Macro-averaged statistics\n",
        "- `task5_confidence_intervals.csv`: Confidence intervals\n",
        "- `task5_best_files.csv`: Top performing files\n",
        "- `task5_worst_files.csv`: Worst performing files\n",
        "- `task5_confusion_matrix.csv`: Confusion matrix\n",
        "- `task5_performance_visualizations.png`: All visualizations\n",
        "- `task5_evaluation_*.log`: Processing log file\n",
        "\n",
        "### 🔧 Usage Notes:\n",
        "\n",
        "- Ensure Task 2 model is loaded before running batch processing\n",
        "- Processing time depends on model and device (CPU/GPU)\n",
        "- Individual file failures are logged but don't stop the pipeline\n",
        "- Results are reproducible due to fixed random seed (42)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
