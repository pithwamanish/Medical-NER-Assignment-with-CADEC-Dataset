{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: LLM-Based BIO Tagging and Format Conversion\n",
        "\n",
        "**Title:** \"Medical NER with LLM: BIO Tagging and Annotation Format Conversion\"\n",
        "\n",
        "## Objective\n",
        "Implement a two-step NER pipeline using a Hugging Face LLM:\n",
        "- **STEP A:** BIO Tagging - Label each token with BIO format using a medical NER model\n",
        "- **STEP B:** Format Conversion - Convert BIO-tagged output to the original annotation format\n",
        "\n",
        "## Overview\n",
        "1. Load a medical NER model from Hugging Face\n",
        "2. Create prompt templates with few-shot examples\n",
        "3. Tokenize input text word-by-word\n",
        "4. Generate BIO predictions for each token\n",
        "5. Parse BIO tags to extract entity spans\n",
        "6. Convert to original annotation format with character ranges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = Path(\"cadec\")\n",
        "TEXT_DIR = BASE_DIR / \"text\"\n",
        "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
        "\n",
        "# Verify directories exist\n",
        "if not TEXT_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {TEXT_DIR}\")\n",
        "if not ORIGINAL_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Directory not found: {ORIGINAL_DIR}\")\n",
        "\n",
        "print(\"✓ Directories verified\")\n",
        "print(f\"  - Text directory: {TEXT_DIR}\")\n",
        "print(f\"  - Original directory: {ORIGINAL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP A: BIO Tagging Implementation\n",
        "\n",
        "This section implements the BIO tagging pipeline:\n",
        "1. Model loading with proper tokenizer\n",
        "2. Prompt engineering with medical domain context\n",
        "3. Word-by-word tokenization\n",
        "4. BIO tag generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "# Using a medical NER model fine-tuned for token classification\n",
        "# Options: \n",
        "# - 'HUMADEX/english_medical_ner' (recommended for medical NER)\n",
        "# - 'dslim/bert-base-NER' (general NER, good baseline)\n",
        "# - 'blaze999/Medical-NER' (if available)\n",
        "\n",
        "MODEL_NAME = \"HUMADEX/english_medical_ner\"  # Medical domain model\n",
        "# Fallback to general NER if medical model is unavailable\n",
        "FALLBACK_MODEL = \"dslim/bert-base-NER\"\n",
        "\n",
        "# Label types we're looking for\n",
        "LABEL_TYPES = ['ADR', 'Drug', 'Disease', 'Symptom']\n",
        "\n",
        "# BIO tag mapping\n",
        "# Note: The model may output different labels, we'll map them to our BIO format\n",
        "BIO_TAGS = {\n",
        "    'B-ADR': 'B-ADR',\n",
        "    'I-ADR': 'I-ADR', \n",
        "    'B-Drug': 'B-Drug',\n",
        "    'I-Drug': 'I-Drug',\n",
        "    'B-Disease': 'B-Disease',\n",
        "    'I-Disease': 'I-Disease',\n",
        "    'B-Symptom': 'B-Symptom',\n",
        "    'I-Symptom': 'I-Symptom',\n",
        "    'O': 'O'\n",
        "}\n",
        "\n",
        "print(f\"Model configuration:\")\n",
        "print(f\"  - Primary model: {MODEL_NAME}\")\n",
        "print(f\"  - Fallback model: {FALLBACK_MODEL}\")\n",
        "print(f\"  - Label types: {LABEL_TYPES}\")\n",
        "print(f\"  - Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "# Using pipeline for easier token classification\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "try:\n",
        "    # Try loading the medical NER model\n",
        "    ner_pipeline = pipeline(\n",
        "        \"token-classification\",\n",
        "        model=MODEL_NAME,\n",
        "        aggregation_strategy=\"simple\",  # Simple aggregation for BIO tags\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    print(f\"✓ Successfully loaded model: {MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not load {MODEL_NAME}: {e}\")\n",
        "    print(f\"  Falling back to {FALLBACK_MODEL}...\")\n",
        "    try:\n",
        "        ner_pipeline = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=FALLBACK_MODEL,\n",
        "            aggregation_strategy=\"simple\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL)\n",
        "        print(f\"✓ Successfully loaded fallback model: {FALLBACK_MODEL}\")\n",
        "    except Exception as e2:\n",
        "        raise RuntimeError(f\"Failed to load both models: {e2}\")\n",
        "\n",
        "# Get the model's label mapping to understand output format\n",
        "model_labels = ner_pipeline.model.config.id2label if hasattr(ner_pipeline.model.config, 'id2label') else {}\n",
        "print(f\"  Model label mapping: {list(model_labels.values())[:10] if model_labels else 'N/A'}...\")\n",
        "print(\"✓ Model and tokenizer ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Prompt Template with Few-Shot Examples\n",
        "\n",
        "The prompt template provides context to guide the model in medical NER tasks.\n",
        "This is particularly useful for models that benefit from in-context learning,\n",
        "or when we need to map general NER labels to our specific medical categories.\n",
        "\"\"\"\n",
        "\n",
        "# Few-shot examples for prompt engineering\n",
        "FEW_SHOT_EXAMPLES = \"\"\"\n",
        "Examples of medical entity recognition:\n",
        "\n",
        "Text: \"I feel a bit drowsy and have blurred vision after taking ibuprofen.\"\n",
        "BIO Tags: O O O B-ADR I-ADR O O O B-Drug O B-Symptom I-Symptom\n",
        "Entities: ADR(drowsy), Drug(ibuprofen), Symptom(blurred vision)\n",
        "\n",
        "Text: \"The patient reported severe headache and nausea due to hypertension medication.\"\n",
        "BIO Tags: O O O O B-Symptom O B-Symptom O O B-Disease O B-Drug\n",
        "Entities: Symptom(headache), Symptom(nausea), Disease(hypertension), Drug(medication)\n",
        "\n",
        "Text: \"Aspirin caused stomach pain and gastric bleeding.\"\n",
        "BIO Tags: B-Drug O B-Symptom I-Symptom O B-ADR I-ADR\n",
        "Entities: Drug(Aspirin), Symptom(stomach pain), ADR(gastric bleeding)\n",
        "\"\"\"\n",
        "\n",
        "def create_prompt_template(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a prompt template with few-shot examples for medical NER.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        The input text to tag\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Formatted prompt with examples and input text\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"{FEW_SHOT_EXAMPLES}\n",
        "\n",
        "Now, tag the following text with BIO format labels:\n",
        "Text: \"{text}\"\n",
        "\n",
        "BIO Tags (one label per word):\"\"\"\n",
        "    return prompt\n",
        "\n",
        "print(\"✓ Prompt template defined with few-shot examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_model_labels_to_bio(model_label: str, entity_text: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Map model output labels to our BIO format.\n",
        "    \n",
        "    The HUMADEX model uses labels like PROBLEM and TREATMENT.\n",
        "    This function maps them to our target categories (ADR, Drug, Disease, Symptom).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_label : str\n",
        "        Label from the model (e.g., 'PROBLEM', 'TREATMENT', 'B-PROBLEM', etc.)\n",
        "    entity_text : str, optional\n",
        "        Entity text for context-aware mapping\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        BIO tag in our format (B-ADR, I-ADR, B-Drug, etc. or O)\n",
        "    \"\"\"\n",
        "    if not model_label or model_label == 'O':\n",
        "        return 'O'\n",
        "    \n",
        "    model_label_upper = model_label.upper()\n",
        "    \n",
        "    # Extract BIO prefix if present (B-, I-, E-, L-, S-, U-)\n",
        "    # BILOU tags: B=Beginning, I=Inside, L=Last, O=Outside, U=Unit, E=End, S=Single\n",
        "    bio_prefix = None\n",
        "    if '-' in model_label_upper:\n",
        "        parts = model_label_upper.split('-', 1)\n",
        "        bio_prefix = parts[0]\n",
        "        entity_type = parts[1]\n",
        "    else:\n",
        "        entity_type = model_label_upper\n",
        "    \n",
        "    # Convert BILOU prefix to BIO prefix\n",
        "    if bio_prefix:\n",
        "        if bio_prefix in ['B', 'U', 'S']:  # Beginning, Unit, Single -> B\n",
        "            final_prefix = 'B-'\n",
        "        elif bio_prefix in ['I', 'E', 'L']:  # Inside, End, Last -> I\n",
        "            final_prefix = 'I-'\n",
        "        else:\n",
        "            final_prefix = 'B-'  # Default\n",
        "    else:\n",
        "        final_prefix = 'B-'  # Default if no prefix\n",
        "    \n",
        "    # Direct mapping if already in our format\n",
        "    if entity_type in ['ADR', 'DRUG', 'DISEASE', 'SYMPTOM']:\n",
        "        return f\"{final_prefix}{entity_type.capitalize()}\"\n",
        "    \n",
        "    # Map HUMADEX model labels to our categories\n",
        "    # PROBLEM can be ADR, Disease, or Symptom - we'll use heuristics\n",
        "    if entity_type == 'PROBLEM':\n",
        "        # Try to distinguish based on entity text if available\n",
        "        if entity_text:\n",
        "            entity_lower = entity_text.lower()\n",
        "            # Common ADR terms (symptoms that occur as side effects)\n",
        "            adr_indicators = ['pain', 'drowsy', 'drowsiness', 'nausea', 'dizziness', 'headache', \n",
        "                            'cramping', 'numbness', 'numb', 'blurred', 'vision', 'stomach',\n",
        "                            'gastric', 'bleeding', 'side effect', 'adverse', 'muscle',\n",
        "                            'joint', 'cramp', 'ache', 'aches']\n",
        "            # Disease indicators (chronic conditions)\n",
        "            disease_indicators = ['arthritis', 'hypertension', 'diabetes', 'cancer',\n",
        "                                'disease', 'condition', 'disorder', 'syndrome',\n",
        "                                'infection', 'tumor']\n",
        "            \n",
        "            # Check ADR first (more specific)\n",
        "            if any(indicator in entity_lower for indicator in adr_indicators):\n",
        "                return f\"{final_prefix}ADR\"\n",
        "            elif any(indicator in entity_lower for indicator in disease_indicators):\n",
        "                return f\"{final_prefix}Disease\"\n",
        "            else:\n",
        "                # Default: ADR (in CADEC dataset context, most problems mentioned are ADRs)\n",
        "                return f\"{final_prefix}ADR\"\n",
        "        else:\n",
        "            # Default to ADR (in drug forum context, problems are usually ADRs)\n",
        "            return f\"{final_prefix}ADR\"\n",
        "    \n",
        "    # TREATMENT -> Drug\n",
        "    if entity_type == 'TREATMENT':\n",
        "        return f\"{final_prefix}Drug\"\n",
        "    \n",
        "    # Map common NER labels to our medical categories\n",
        "    # ADR (Adverse Drug Reactions)\n",
        "    if 'ADR' in entity_type or 'ADVERSE' in entity_type:\n",
        "        return f\"{final_prefix}ADR\"\n",
        "    \n",
        "    # Drug/Medication\n",
        "    if any(term in entity_type for term in ['DRUG', 'MEDICATION', 'MEDICINE', 'MED']):\n",
        "        return f\"{final_prefix}Drug\"\n",
        "    \n",
        "    # Disease/Condition\n",
        "    if any(term in entity_type for term in ['DISEASE', 'CONDITION', 'ILLNESS', 'DISORDER']):\n",
        "        return f\"{final_prefix}Disease\"\n",
        "    \n",
        "    # Symptom\n",
        "    if any(term in entity_type for term in ['SYMPTOM', 'SIGN', 'MANIFESTATION']):\n",
        "        return f\"{final_prefix}Symptom\"\n",
        "    \n",
        "    # Other medical entities\n",
        "    if entity_type == 'O' or 'OUTSIDE' in entity_type:\n",
        "        return 'O'\n",
        "    \n",
        "    # Default: return O for unrecognized labels\n",
        "    return 'O'\n",
        "\n",
        "print(\"✓ Label mapping function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_text_word_by_word(text: str) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Tokenize text word-by-word and preserve character positions.\n",
        "    \n",
        "    This function splits text into words (whitespace-separated tokens)\n",
        "    and tracks character positions for each token. This is crucial for\n",
        "    later conversion to character ranges in the original format.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        Raw input text\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Tuple[str, int, int]]\n",
        "        List of (token, start_char, end_char) tuples\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    words = text.split()\n",
        "    \n",
        "    current_pos = 0\n",
        "    for word in words:\n",
        "        # Find the position of this word in the original text\n",
        "        # Account for potential leading/trailing whitespace\n",
        "        word_start = text.find(word, current_pos)\n",
        "        if word_start == -1:\n",
        "            # Fallback: use current position\n",
        "            word_start = current_pos\n",
        "        word_end = word_start + len(word)\n",
        "        \n",
        "        tokens.append((word, word_start, word_end))\n",
        "        \n",
        "        # Move past this word and any trailing whitespace\n",
        "        # Find next non-whitespace character\n",
        "        next_pos = word_end\n",
        "        while next_pos < len(text) and text[next_pos].isspace():\n",
        "            next_pos += 1\n",
        "        current_pos = next_pos\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(\"✓ Word-by-word tokenization function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_bio_tags(text: str, model_pipeline, tokenizer) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Generate BIO tags for each token in the input text.\n",
        "    \n",
        "    This is the core STEP A function. It:\n",
        "    1. Tokenizes text word-by-word\n",
        "    2. Uses the NER model to predict entity labels\n",
        "    3. Maps model labels to BIO format\n",
        "    4. Returns (token, BIO_label) tuples\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        Raw text from a file in the 'text' subdirectory\n",
        "    model_pipeline : transformers pipeline\n",
        "        Token classification pipeline\n",
        "    tokenizer : transformers tokenizer\n",
        "        Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Tuple[str, str]]\n",
        "        List of (token, BIO_label) tuples\n",
        "    \"\"\"\n",
        "    # Step 1: Word-by-word tokenization with character positions\n",
        "    word_tokens = tokenize_text_word_by_word(text)\n",
        "    \n",
        "    # Step 2: Use model to predict entities\n",
        "    # The pipeline returns entities with their character spans\n",
        "    try:\n",
        "        model_predictions = model_pipeline(text)\n",
        "        # Debug: Check what the model is returning\n",
        "        if not model_predictions or len(model_predictions) == 0:\n",
        "            # No entities found, return all O\n",
        "            return [(token, 'O') for token, _, _ in word_tokens]\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error in model prediction: {e}\")\n",
        "        # Fallback: assign O to all tokens\n",
        "        return [(token, 'O') for token, _, _ in word_tokens]\n",
        "    \n",
        "    # Step 3: Map model predictions to word tokens\n",
        "    # Create a list to track which words belong to which entities\n",
        "    word_labels = ['O'] * len(word_tokens)\n",
        "    \n",
        "    # Process model predictions - they come as aggregated entities with spans\n",
        "    if isinstance(model_predictions, list):\n",
        "        for pred in model_predictions:\n",
        "            if isinstance(pred, dict):\n",
        "                # Extract entity information\n",
        "                entity_label = pred.get('entity_group', pred.get('label', 'O'))\n",
        "                start_char = pred.get('start', 0)\n",
        "                end_char = pred.get('end', start_char)\n",
        "                entity_text = pred.get('word', '')\n",
        "                \n",
        "                # Skip if no valid label\n",
        "                if not entity_label or entity_label == 'O':\n",
        "                    continue\n",
        "                \n",
        "                # Map to our BIO format using entity text for context\n",
        "                mapped_label = map_model_labels_to_bio(entity_label, entity_text)\n",
        "                \n",
        "                # Extract entity type (ADR, Drug, Disease, Symptom)\n",
        "                if '-' in mapped_label:\n",
        "                    parts = mapped_label.split('-', 1)\n",
        "                    bio_prefix_part = parts[0]\n",
        "                    entity_type = parts[1]\n",
        "                else:\n",
        "                    entity_type = mapped_label\n",
        "                    bio_prefix_part = 'B'\n",
        "                \n",
        "                # Skip if mapped to O or invalid entity type\n",
        "                if entity_type == 'O' or entity_type not in LABEL_TYPES:\n",
        "                    continue\n",
        "                \n",
        "                # Validate entity type is one of our target types\n",
        "                if entity_type not in ['ADR', 'Drug', 'Disease', 'Symptom']:\n",
        "                    continue\n",
        "                \n",
        "                # Find which words overlap with this entity span\n",
        "                overlapping_indices = []\n",
        "                for i, (word, word_start, word_end) in enumerate(word_tokens):\n",
        "                    # Check if word overlaps with entity span\n",
        "                    # Word overlaps if: word_start < end_char AND word_end > start_char\n",
        "                    if word_start < end_char and word_end > start_char:\n",
        "                        overlapping_indices.append(i)\n",
        "                \n",
        "                # Assign BIO labels: B- for first word, I- for subsequent words\n",
        "                if overlapping_indices:\n",
        "                    for idx, word_idx in enumerate(overlapping_indices):\n",
        "                        if idx == 0:\n",
        "                            # First word gets B- prefix\n",
        "                            word_labels[word_idx] = f\"B-{entity_type}\"\n",
        "                        else:\n",
        "                            # Subsequent words get I- prefix if same entity type\n",
        "                            prev_label = word_labels[word_idx - 1]\n",
        "                            prev_entity_type = prev_label.split('-')[-1] if '-' in prev_label else None\n",
        "                            \n",
        "                            if prev_entity_type == entity_type:\n",
        "                                word_labels[word_idx] = f\"I-{entity_type}\"\n",
        "                            else:\n",
        "                                # Different entity type, start new entity\n",
        "                                word_labels[word_idx] = f\"B-{entity_type}\"\n",
        "    \n",
        "    # Step 4: Create final BIO tagged output\n",
        "    bio_tagged = [(word, label) for (word, _, _), label in zip(word_tokens, word_labels)]\n",
        "    \n",
        "    return bio_tagged\n",
        "\n",
        "print(\"✓ BIO tagging function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP B: Convert BIO to Original Format\n",
        "\n",
        "This section implements the conversion from BIO tags to the original annotation format:\n",
        "1. Parse BIO-tagged output to extract entity spans\n",
        "2. Convert continuous B-I sequences into single entities\n",
        "3. Generate output matching the 'original' subdirectory format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_bio_tags_to_entities(bio_tagged: List[Tuple[str, str]], \n",
        "                                word_tokens: List[Tuple[str, int, int]]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse BIO-tagged output to extract entity spans.\n",
        "    \n",
        "    This function converts BIO tags into continuous entity spans.\n",
        "    It handles:\n",
        "    - B- followed by I- tags (single multi-word entity)\n",
        "    - Standalone B- tags (single-word entity)\n",
        "    - Edge cases (I- without B-, consecutive B- tags, etc.)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    bio_tagged : List[Tuple[str, str]]\n",
        "        List of (token, BIO_label) tuples from STEP A\n",
        "    word_tokens : List[Tuple[str, int, int]]\n",
        "        List of (token, start_char, end_char) tuples for character positions\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of entity dictionaries with:\n",
        "        - 'label': Entity type (ADR, Drug, Disease, Symptom)\n",
        "        - 'text': Entity text\n",
        "        - 'start': Start character position\n",
        "        - 'end': End character position\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    \n",
        "    for i, (token, bio_label) in enumerate(bio_tagged):\n",
        "        word, start_char, end_char = word_tokens[i]\n",
        "        \n",
        "        if bio_label == 'O':\n",
        "            # End current entity if exists\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "            continue\n",
        "        \n",
        "        # Parse BIO label\n",
        "        if '-' in bio_label:\n",
        "            label_type = bio_label.split('-', 1)[1]  # Extract ADR, Drug, Disease, Symptom\n",
        "            is_beginning = bio_label.startswith('B-')\n",
        "        else:\n",
        "            label_type = bio_label\n",
        "            is_beginning = True\n",
        "        \n",
        "        # Validate label type\n",
        "        if label_type not in LABEL_TYPES:\n",
        "            # Invalid label, treat as O\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "            continue\n",
        "        \n",
        "        if is_beginning:\n",
        "            # Begin new entity\n",
        "            # Save previous entity if exists\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            \n",
        "            # Start new entity\n",
        "            current_entity = {\n",
        "                'label': label_type,\n",
        "                'text': word,\n",
        "                'start': start_char,\n",
        "                'end': end_char,\n",
        "                'tokens': [word]\n",
        "            }\n",
        "        else:\n",
        "            # Continue current entity (I- tag)\n",
        "            if current_entity and current_entity['label'] == label_type:\n",
        "                # Valid continuation\n",
        "                current_entity['text'] += ' ' + word\n",
        "                current_entity['end'] = end_char\n",
        "                current_entity['tokens'].append(word)\n",
        "            else:\n",
        "                # I- tag without matching B- tag (edge case)\n",
        "                # Treat as new entity\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                \n",
        "                current_entity = {\n",
        "                    'label': label_type,\n",
        "                    'text': word,\n",
        "                    'start': start_char,\n",
        "                    'end': end_char,\n",
        "                    'tokens': [word]\n",
        "                }\n",
        "    \n",
        "    # Don't forget the last entity\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    return entities\n",
        "\n",
        "print(\"✓ BIO parsing function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_original_format(entities: List[Dict], tag_start: int = 1) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convert entity spans to original annotation format.\n",
        "    \n",
        "    This function generates output matching the 'original' subdirectory format:\n",
        "    - Tag (T1, T2, etc.)\n",
        "    - Label type (ADR, Drug, Disease, Symptom)\n",
        "    - Character ranges (start, end)\n",
        "    - Entity text\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    entities : List[Dict]\n",
        "        List of entity dictionaries from parse_bio_tags_to_entities()\n",
        "    tag_start : int\n",
        "        Starting tag number (default: 1, so tags start at T1)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[str]\n",
        "        List of annotation lines in original format\n",
        "    \"\"\"\n",
        "    annotation_lines = []\n",
        "    tag_num = tag_start\n",
        "    \n",
        "    for entity in entities:\n",
        "        label = entity['label']\n",
        "        text = entity['text']\n",
        "        start = entity['start']\n",
        "        end = entity['end']\n",
        "        \n",
        "        # Format: TAG\\tLABEL START END\\tTEXT\n",
        "        # Example: T1\tADR 9 19\tbit drowsy\n",
        "        annotation_line = f\"T{tag_num}\\t{label} {start} {end}\\t{text}\"\n",
        "        annotation_lines.append(annotation_line)\n",
        "        tag_num += 1\n",
        "    \n",
        "    return annotation_lines\n",
        "\n",
        "print(\"✓ Format conversion function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_text_file(text_file_path: Path, model_pipeline, tokenizer) -> Tuple[List[Tuple[str, str]], List[str]]:\n",
        "    \"\"\"\n",
        "    Complete pipeline: Process a text file through STEP A and STEP B.\n",
        "    \n",
        "    This is the main function that orchestrates the entire process:\n",
        "    1. Read text from file\n",
        "    2. Generate BIO tags (STEP A)\n",
        "    3. Convert to original format (STEP B)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text_file_path : Path\n",
        "        Path to text file in 'text' subdirectory\n",
        "    model_pipeline : transformers pipeline\n",
        "        Token classification pipeline\n",
        "    tokenizer : transformers tokenizer\n",
        "        Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[List[Tuple[str, str]], List[str]]\n",
        "        - BIO tagged tokens: List of (token, BIO_label) tuples\n",
        "        - Annotation lines: List of annotation lines in original format\n",
        "    \"\"\"\n",
        "    # Read text file\n",
        "    try:\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Could not read file {text_file_path}: {e}\")\n",
        "    \n",
        "    # STEP A: Generate BIO tags\n",
        "    bio_tagged = generate_bio_tags(text, model_pipeline, tokenizer)\n",
        "    \n",
        "    # Get word tokens for character position mapping\n",
        "    word_tokens = tokenize_text_word_by_word(text)\n",
        "    \n",
        "    # STEP B: Parse BIO tags to entities\n",
        "    entities = parse_bio_tags_to_entities(bio_tagged, word_tokens)\n",
        "    \n",
        "    # STEP B: Convert to original format\n",
        "    annotation_lines = convert_to_original_format(entities)\n",
        "    \n",
        "    return bio_tagged, annotation_lines\n",
        "\n",
        "print(\"✓ Complete pipeline function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Debug: Test model output directly\n",
        "print(\"Testing model output format...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with a simple example\n",
        "test_text = \"Muscle pain after taking ibuprofen.\"\n",
        "print(f\"\\nTest text: '{test_text}'\")\n",
        "\n",
        "# Get raw model predictions\n",
        "raw_predictions = ner_pipeline(test_text)\n",
        "print(f\"\\nRaw model predictions ({len(raw_predictions)} entities):\")\n",
        "for i, pred in enumerate(raw_predictions[:5], 1):\n",
        "    print(f\"  {i}. {pred}\")\n",
        "\n",
        "# Test label mapping\n",
        "print(\"\\nLabel mapping test:\")\n",
        "test_labels = [\"PROBLEM\", \"TREATMENT\", \"B-PROBLEM\", \"I-PROBLEM\", \"E-PROBLEM\"]\n",
        "for label in test_labels:\n",
        "    mapped = map_model_labels_to_bio(label, \"test entity\")\n",
        "    print(f\"  {label:15s} -> {mapped}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with Sample Files\n",
        "\n",
        "Let's test the pipeline with a sample file to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample file\n",
        "sample_text_files = list(TEXT_DIR.glob(\"*.txt\"))[:3]  # Get first 3 files for testing\n",
        "\n",
        "if sample_text_files:\n",
        "    test_file = sample_text_files[0]\n",
        "    print(f\"Testing with file: {test_file.name}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Process the file\n",
        "    bio_tagged, annotation_lines = process_text_file(test_file, ner_pipeline, tokenizer)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\nSTEP A - BIO Tagged Output (first 20 tokens):\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, (token, label) in enumerate(bio_tagged[:20]):\n",
        "        print(f\"{i+1:3d}. {token:20s} -> {label}\")\n",
        "    if len(bio_tagged) > 20:\n",
        "        print(f\"... ({len(bio_tagged) - 20} more tokens)\")\n",
        "    \n",
        "    print(f\"\\nTotal tokens tagged: {len(bio_tagged)}\")\n",
        "    \n",
        "    # Count tags\n",
        "    tag_counts = {}\n",
        "    for _, label in bio_tagged:\n",
        "        tag_counts[label] = tag_counts.get(label, 0) + 1\n",
        "    print(f\"\\nBIO Tag distribution:\")\n",
        "    for tag, count in sorted(tag_counts.items()):\n",
        "        print(f\"  {tag:15s}: {count:4d}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"\\nSTEP B - Original Format Annotation:\")\n",
        "    print(\"-\" * 80)\n",
        "    for line in annotation_lines:\n",
        "        print(line)\n",
        "    \n",
        "    print(f\"\\nTotal entities extracted: {len(annotation_lines)}\")\n",
        "    \n",
        "    # Compare with ground truth if available\n",
        "    original_file = ORIGINAL_DIR / test_file.name.replace('.txt', '.ann')\n",
        "    if original_file.exists():\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"\\nGround Truth (from original annotation file):\")\n",
        "        print(\"-\" * 80)\n",
        "        with open(original_file, 'r', encoding='utf-8') as f:\n",
        "            gt_lines = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]\n",
        "            for line in gt_lines[:10]:  # Show first 10 lines\n",
        "                print(line)\n",
        "            if len(gt_lines) > 10:\n",
        "                print(f\"... ({len(gt_lines) - 10} more lines)\")\n",
        "        print(f\"\\nTotal ground truth entities: {len(gt_lines)}\")\n",
        "else:\n",
        "    print(\"⚠ No text files found for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Example: Full Pipeline Walkthrough\n",
        "\n",
        "Let's walk through a complete example to understand each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed example with a sample text\n",
        "example_text = \"I feel a bit drowsy and have blurred vision after taking ibuprofen for my arthritis.\"\n",
        "\n",
        "print(\"Example Text:\")\n",
        "print(f'\"{example_text}\"')\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Step 1: Tokenize word-by-word\n",
        "word_tokens = tokenize_text_word_by_word(example_text)\n",
        "print(\"\\nStep 1: Word-by-word Tokenization (with character positions):\")\n",
        "print(\"-\" * 80)\n",
        "for i, (word, start, end) in enumerate(word_tokens):\n",
        "    print(f\"{i+1:2d}. '{word:15s}' -> chars [{start:3d}:{end:3d}]\")\n",
        "\n",
        "# Step 2: Generate BIO tags\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nStep 2: Generate BIO Tags using NER Model\")\n",
        "print(\"-\" * 80)\n",
        "bio_tagged = generate_bio_tags(example_text, ner_pipeline, tokenizer)\n",
        "\n",
        "print(\"\\nBIO Tagged Tokens:\")\n",
        "for i, (token, label) in enumerate(bio_tagged):\n",
        "    print(f\"{i+1:2d}. {token:15s} -> {label}\")\n",
        "\n",
        "# Step 3: Parse BIO tags to entities\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nStep 3: Parse BIO Tags to Entity Spans\")\n",
        "print(\"-\" * 80)\n",
        "entities = parse_bio_tags_to_entities(bio_tagged, word_tokens)\n",
        "\n",
        "print(\"\\nExtracted Entities:\")\n",
        "for i, entity in enumerate(entities, 1):\n",
        "    print(f\"{i}. Label: {entity['label']:10s} | Text: '{entity['text']:30s}' | Range: [{entity['start']:3d}:{entity['end']:3d}]\")\n",
        "\n",
        "# Step 4: Convert to original format\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nStep 4: Convert to Original Annotation Format\")\n",
        "print(\"-\" * 80)\n",
        "annotation_lines = convert_to_original_format(entities)\n",
        "\n",
        "print(\"\\nOriginal Format Annotation Lines:\")\n",
        "for line in annotation_lines:\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Edge Case Handling\n",
        "\n",
        "The implementation includes handling for various edge cases in BIO tagging:\n",
        "- I- tags without preceding B- tags (treated as new entity)\n",
        "- Consecutive B- tags (each starts a new entity)\n",
        "- Multi-word entities spanning punctuation\n",
        "- Overlapping entities (handled by priority rules)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test edge cases\n",
        "print(\"Edge Case Testing:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test case 1: I- tag without B- tag\n",
        "edge_case_bio = [\n",
        "    (\"I\", \"O\"),\n",
        "    (\"feel\", \"I-ADR\"),  # I- without preceding B-\n",
        "    (\"drowsy\", \"I-ADR\"),\n",
        "    (\"after\", \"O\"),\n",
        "    (\"ibuprofen\", \"B-Drug\")\n",
        "]\n",
        "\n",
        "print(\"\\n1. I- tag without B- tag (should be treated as B-):\")\n",
        "word_tokens_edge = [(\"I\", 0, 1), (\"feel\", 2, 6), (\"drowsy\", 7, 13), (\"after\", 14, 19), (\"ibuprofen\", 20, 29)]\n",
        "entities_edge = parse_bio_tags_to_entities(edge_case_bio, word_tokens_edge)\n",
        "print(f\"   Input: {edge_case_bio}\")\n",
        "print(f\"   Output: {len(entities_edge)} entities extracted\")\n",
        "for entity in entities_edge:\n",
        "    print(f\"     - {entity['label']}: '{entity['text']}'\")\n",
        "\n",
        "# Test case 2: Consecutive B- tags\n",
        "edge_case_bio2 = [\n",
        "    (\"Aspirin\", \"B-Drug\"),\n",
        "    (\"caused\", \"O\"),\n",
        "    (\"headache\", \"B-Symptom\"),\n",
        "    (\"and\", \"O\"),\n",
        "    (\"nausea\", \"B-Symptom\")  # Consecutive B- tags (both valid)\n",
        "]\n",
        "\n",
        "print(\"\\n2. Consecutive B- tags (both should be separate entities):\")\n",
        "word_tokens_edge2 = [(\"Aspirin\", 0, 7), (\"caused\", 8, 14), (\"headache\", 15, 23), (\"and\", 24, 27), (\"nausea\", 28, 34)]\n",
        "entities_edge2 = parse_bio_tags_to_entities(edge_case_bio2, word_tokens_edge2)\n",
        "print(f\"   Input: {edge_case_bio2}\")\n",
        "print(f\"   Output: {len(entities_edge2)} entities extracted\")\n",
        "for entity in entities_edge2:\n",
        "    print(f\"     - {entity['label']}: '{entity['text']}'\")\n",
        "\n",
        "# Test case 3: Mixed entity types\n",
        "edge_case_bio3 = [\n",
        "    (\"The\", \"O\"),\n",
        "    (\"patient\", \"O\"),\n",
        "    (\"took\", \"O\"),\n",
        "    (\"aspirin\", \"B-Drug\"),\n",
        "    (\"for\", \"O\"),\n",
        "    (\"arthritis\", \"B-Disease\"),\n",
        "    (\"and\", \"O\"),\n",
        "    (\"experienced\", \"O\"),\n",
        "    (\"stomach\", \"B-ADR\"),\n",
        "    (\"pain\", \"I-ADR\")\n",
        "]\n",
        "\n",
        "print(\"\\n3. Mixed entity types with proper B-I sequences:\")\n",
        "word_tokens_edge3 = [(\"The\", 0, 3), (\"patient\", 4, 11), (\"took\", 12, 16), (\"aspirin\", 17, 24),\n",
        "                     (\"for\", 25, 28), (\"arthritis\", 29, 38), (\"and\", 39, 42), (\"experienced\", 43, 54),\n",
        "                     (\"stomach\", 55, 62), (\"pain\", 63, 67)]\n",
        "entities_edge3 = parse_bio_tags_to_entities(edge_case_bio3, word_tokens_edge3)\n",
        "print(f\"   Output: {len(entities_edge3)} entities extracted\")\n",
        "for entity in entities_edge3:\n",
        "    print(f\"     - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ Edge case handling verified\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a complete two-step pipeline for medical NER:\n",
        "\n",
        "### STEP A: BIO Tagging\n",
        "- **Model Loading**: Uses Hugging Face token classification models (medical NER preferred)\n",
        "- **Word Tokenization**: Splits text word-by-word while preserving character positions\n",
        "- **BIO Generation**: Maps model predictions to BIO format (B-ADR, I-ADR, B-Drug, etc.)\n",
        "- **Label Mapping**: Converts model-specific labels to our medical categories\n",
        "\n",
        "### STEP B: Format Conversion\n",
        "- **Entity Extraction**: Parses BIO tags to extract continuous entity spans\n",
        "- **Character Ranges**: Calculates accurate character positions for each entity\n",
        "- **Format Matching**: Generates output in the original annotation format (TAG\\tLABEL START END\\tTEXT)\n",
        "\n",
        "### Key Features\n",
        "- ✅ Handles multi-word entities correctly\n",
        "- ✅ Edge case handling (I- without B-, consecutive B- tags, etc.)\n",
        "- ✅ Accurate character offset calculation\n",
        "- ✅ Comprehensive documentation and inline comments\n",
        "- ✅ Few-shot prompt engineering examples\n",
        "- ✅ Fallback model support for robustness\n",
        "\n",
        "### Usage Example\n",
        "```python\n",
        "# Process a single file\n",
        "bio_tagged, annotations = process_text_file(text_file_path, ner_pipeline, tokenizer)\n",
        "\n",
        "# bio_tagged: List of (token, BIO_label) tuples\n",
        "# annotations: List of annotation lines in original format\n",
        "```\n",
        "\n",
        "### Notes\n",
        "- Model performance depends on the specific Hugging Face model used\n",
        "- Label mapping may need adjustment for different models\n",
        "- Character offsets are calculated from word positions in original text\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
