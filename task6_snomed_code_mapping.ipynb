{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 6: SNOMED CT Code Mapping with Dual Matching Approaches\n",
        "\n",
        "**Title:** \"Multi-Strategy Entity Normalization: Fuzzy Matching vs Embedding Similarity\"\n",
        "\n",
        "## Objective\n",
        "Implement entity normalization using two different string matching approaches:\n",
        "\n",
        "**SETUP:**\n",
        "1. Parse 'sct' subdirectory to build knowledge base:\n",
        "   - Extract: Identifier, SNOMED CT code(s), standard descriptions, ranges, entity text\n",
        "   - Parse format: \"TT1\\t271782001 | Drowsy | 9 19\\tbit drowsy\"\n",
        "   - Handle multiple code pairs when present\n",
        "2. Parse 'original' subdirectory for entity types (ADR, Drug, Disease, Symptom)\n",
        "3. Combine into unified data structure\n",
        "\n",
        "**APPROACH A - Fuzzy String Matching:**\n",
        "- Use fuzzywuzzy library with fuzz.token_set_ratio()\n",
        "- Compare entity text against all SNOMED descriptions\n",
        "- Set threshold (e.g., 80% similarity)\n",
        "\n",
        "**APPROACH B - Embedding-Based Matching:**\n",
        "- Load sentence embedding model from Hugging Face\n",
        "- Generate embeddings for SNOMED descriptions and entities\n",
        "- Calculate cosine similarity to find nearest neighbor\n",
        "\n",
        "**COMPARISON:**\n",
        "- Agreement rate between methods\n",
        "- Accuracy comparison against ground truth\n",
        "- Runtime performance comparison\n",
        "- Analysis of strengths/weaknesses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Optional, Set\n",
        "from collections import defaultdict, Counter\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For fuzzy matching (Approach A)\n",
        "try:\n",
        "    from fuzzywuzzy import fuzz\n",
        "    from fuzzywuzzy import process as fuzzy_process\n",
        "except ImportError:\n",
        "    print(\"⚠ fuzzywuzzy not found. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fuzzywuzzy\", \"python-Levenshtein\"])\n",
        "    from fuzzywuzzy import fuzz\n",
        "    from fuzzywuzzy import process as fuzzy_process\n",
        "    print(\"✓ fuzzywuzzy installed successfully\")\n",
        "\n",
        "# For embedding-based matching (Approach B)\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import torch\n",
        "except ImportError:\n",
        "    print(\"⚠ sentence-transformers not found. Installing...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import torch\n",
        "    print(\"✓ sentence-transformers installed successfully\")\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Configuration\n",
        "BASE_DIR = Path(\"cadec\")\n",
        "TEXT_DIR = BASE_DIR / \"text\"\n",
        "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
        "SCT_DIR = BASE_DIR / \"sct\"\n",
        "\n",
        "# Label types\n",
        "LABEL_TYPES = ['ADR', 'Drug', 'Disease', 'Symptom']\n",
        "\n",
        "# Matching thresholds\n",
        "FUZZY_THRESHOLD = 80  # Minimum similarity score for fuzzy matching\n",
        "EMBEDDING_THRESHOLD = 0.7  # Minimum cosine similarity for embedding matching\n",
        "\n",
        "# Model for embedding-based matching\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Fast and efficient\n",
        "# Alternative: \"dmis-lab/biobert-base-cased-v1.1\" for medical domain (slower but more accurate)\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"  - Fuzzy matching threshold: {FUZZY_THRESHOLD}%\")\n",
        "print(f\"  - Embedding similarity threshold: {EMBEDDING_THRESHOLD}\")\n",
        "print(f\"  - Embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"  - Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Parse SCT Subdirectory to Build Knowledge Base\n",
        "\n",
        "Parse SNOMED CT annotation files to extract:\n",
        "- Identifier (TT1, TT2, etc.)\n",
        "- SNOMED CT code(s) - can have multiple codes separated by \"or\"\n",
        "- Standard descriptions\n",
        "- Character ranges\n",
        "- Entity text (the actual text in the document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_sct_file(ann_file_path: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse SNOMED CT annotation file from 'sct' subdirectory.\n",
        "    \n",
        "    Format: TAG\\tCODE | DESCRIPTION | START END\\tENTITY_TEXT\n",
        "    Example: TT1\\t271782001 | Drowsy | 9 19\\tbit drowsy\n",
        "    Example with multiple codes: TT6\\t102498003 | Agony | or 76948002|Severe pain| 260 265\\tagony\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ann_file_path : Path\n",
        "        Path to the .ann annotation file in sct directory\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of SNOMED CT entity dictionaries with:\n",
        "        - 'identifier': Tag identifier (TT1, TT2, etc.)\n",
        "        - 'snomed_codes': List of SNOMED CT codes (can have multiple)\n",
        "        - 'snomed_descriptions': List of standard descriptions (can have multiple)\n",
        "        - 'char_start': Start character position\n",
        "        - 'char_end': End character position\n",
        "        - 'entity_text': Actual entity text from document\n",
        "        - 'file_name': Source file name\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    \n",
        "    try:\n",
        "        # Try UTF-8 first, fallback to latin-1 if needed\n",
        "        try:\n",
        "            with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "        except UnicodeDecodeError:\n",
        "            # Fallback to latin-1 for files with encoding issues\n",
        "            with open(ann_file_path, 'r', encoding='latin-1') as f:\n",
        "                lines = f.readlines()\n",
        "        \n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            \n",
        "            # Skip empty lines\n",
        "            if not line:\n",
        "                continue\n",
        "            \n",
        "            # Parse format: TAG\\tCODE_INFO | START END\\tENTITY_TEXT\n",
        "            # CODE_INFO can be: \"CODE | DESC\" or \"CODE | DESC | or CODE2|DESC2|\"\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "            \n",
        "            identifier = parts[0]\n",
        "            code_and_desc_str = parts[1]  # \"CODE | DESC | START END\" or \"CODE | DESC | or CODE2|DESC2| START END\"\n",
        "            entity_text = parts[2]\n",
        "            \n",
        "            # Extract ranges and code/description pairs\n",
        "            # The format can be:\n",
        "            # \"271782001 | Drowsy | 9 19\" -> single code\n",
        "            # \"102498003 | Agony | or 76948002|Severe pain| 260 265\" -> multiple codes\n",
        "            \n",
        "            # Find where the character ranges start (last two numbers)\n",
        "            # Match pattern: spaces, then number, space, number at the end\n",
        "            range_match = re.search(r'\\s+(\\d+)\\s+(\\d+)\\s*$', code_and_desc_str)\n",
        "            if not range_match:\n",
        "                continue\n",
        "            \n",
        "            char_start = int(range_match.group(1))\n",
        "            char_end = int(range_match.group(2))\n",
        "            \n",
        "            # Extract code/description part (everything before the ranges)\n",
        "            code_desc_part = code_and_desc_str[:range_match.start()].strip()\n",
        "            \n",
        "            # Parse code/description pairs\n",
        "            # Handle multiple codes separated by \"or\"\n",
        "            snomed_codes = []\n",
        "            snomed_descriptions = []\n",
        "            \n",
        "            if ' or ' in code_desc_part.lower():\n",
        "                # Multiple codes format: \"CODE | DESC | or CODE2|DESC2|\"\n",
        "                # Split by \"or\" (case insensitive)\n",
        "                parts_code = re.split(r'\\s+or\\s+', code_desc_part, flags=re.IGNORECASE)\n",
        "                for part in parts_code:\n",
        "                    part = part.strip()\n",
        "                    # Remove trailing pipes\n",
        "                    part = part.rstrip('|').strip()\n",
        "                    # Split by \"|\"\n",
        "                    code_desc = part.split('|')\n",
        "                    if len(code_desc) >= 2:\n",
        "                        code = code_desc[0].strip()\n",
        "                        desc = code_desc[1].strip()\n",
        "                        snomed_codes.append(code)\n",
        "                        snomed_descriptions.append(desc)\n",
        "                    elif len(code_desc) == 1:\n",
        "                        # Try to parse as \"CODE | DESC\" format\n",
        "                        if '|' in code_desc[0]:\n",
        "                            split_parts = code_desc[0].split('|')\n",
        "                            if len(split_parts) >= 2:\n",
        "                                snomed_codes.append(split_parts[0].strip())\n",
        "                                snomed_descriptions.append(split_parts[1].strip())\n",
        "            else:\n",
        "                # Single code format: \"CODE | DESC\"\n",
        "                code_desc = code_desc_part.split('|')\n",
        "                if len(code_desc) >= 2:\n",
        "                    code = code_desc[0].strip()\n",
        "                    desc = code_desc[1].strip()\n",
        "                    snomed_codes.append(code)\n",
        "                    snomed_descriptions.append(desc)\n",
        "            \n",
        "            # Only add if we have at least one code/description pair\n",
        "            if snomed_codes and snomed_descriptions:\n",
        "                entities.append({\n",
        "                    'identifier': identifier,\n",
        "                    'snomed_codes': snomed_codes,\n",
        "                    'snomed_descriptions': snomed_descriptions,\n",
        "                    'char_start': char_start,\n",
        "                    'char_end': char_end,\n",
        "                    'entity_text': entity_text,\n",
        "                    'file_name': ann_file_path.name\n",
        "                })\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing SCT file {ann_file_path}: {e}\")\n",
        "        return []\n",
        "    \n",
        "    return entities\n",
        "\n",
        "print(\"✓ SCT parsing function defined\")\n",
        "\n",
        "# Test with a sample file\n",
        "sample_sct_file = list(SCT_DIR.glob(\"*.ann\"))[0] if list(SCT_DIR.glob(\"*.ann\")) else None\n",
        "if sample_sct_file:\n",
        "    print(f\"\\nTesting SCT parsing with: {sample_sct_file.name}\")\n",
        "    test_entities = parse_sct_file(sample_sct_file)\n",
        "    print(f\"  Parsed {len(test_entities)} entities\")\n",
        "    if test_entities:\n",
        "        print(f\"\\n  Sample entity:\")\n",
        "        for key, value in test_entities[0].items():\n",
        "            print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Parse Original Subdirectory for Entity Types\n",
        "\n",
        "Parse original annotation files to get entity types (ADR, Drug, Disease, Symptom) that correspond to the SNOMED CT annotations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_original_file(ann_file_path: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Parse original annotation file from 'original' subdirectory.\n",
        "    \n",
        "    Format: TAG\\tLABEL START END\\tTEXT\n",
        "    Example: T1\tADR 9 19\tbit drowsy\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    ann_file_path : Path\n",
        "        Path to the .ann annotation file in original directory\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of entity dictionaries with:\n",
        "        - 'identifier': Tag identifier (T1, T2, etc.)\n",
        "        - 'label_type': Entity type (ADR, Drug, Disease, Symptom)\n",
        "        - 'char_start': Start character position\n",
        "        - 'char_end': End character position\n",
        "        - 'entity_text': Entity text\n",
        "        - 'file_name': Source file name\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    \n",
        "    try:\n",
        "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                \n",
        "                # Skip empty lines and comment lines\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "                \n",
        "                # Parse format: TAG\\tLABEL RANGES\\tTEXT\n",
        "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
        "                if match:\n",
        "                    identifier = match.group(1)\n",
        "                    label_and_ranges = match.group(2)\n",
        "                    entity_text = match.group(3)\n",
        "                    \n",
        "                    # Extract label type and ranges\n",
        "                    parts = label_and_ranges.split(None, 1)\n",
        "                    if len(parts) < 2:\n",
        "                        continue\n",
        "                    \n",
        "                    label_type = parts[0]\n",
        "                    ranges_str = parts[1]\n",
        "                    \n",
        "                    # Only process relevant label types\n",
        "                    if label_type not in LABEL_TYPES:\n",
        "                        continue\n",
        "                    \n",
        "                    # Extract ranges (handle multiple ranges separated by semicolons)\n",
        "                    ranges = []\n",
        "                    if ';' in ranges_str:\n",
        "                        range_pairs = ranges_str.split(';')\n",
        "                        for rp in range_pairs:\n",
        "                            rp = rp.strip()\n",
        "                            if rp:\n",
        "                                range_nums = rp.split()\n",
        "                                if len(range_nums) >= 2:\n",
        "                                    try:\n",
        "                                        start = int(range_nums[0])\n",
        "                                        end = int(range_nums[1])\n",
        "                                        ranges.append((start, end))\n",
        "                                    except ValueError:\n",
        "                                        continue\n",
        "                    else:\n",
        "                        range_nums = ranges_str.split()\n",
        "                        if len(range_nums) >= 2:\n",
        "                            try:\n",
        "                                start = int(range_nums[0])\n",
        "                                end = int(range_nums[1])\n",
        "                                ranges = [(start, end)]\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "                    \n",
        "                    # Create entity entries for each range\n",
        "                    for start, end in ranges:\n",
        "                        entities.append({\n",
        "                            'identifier': identifier,\n",
        "                            'label_type': label_type,\n",
        "                            'char_start': start,\n",
        "                            'char_end': end,\n",
        "                            'entity_text': entity_text.strip(),\n",
        "                            'file_name': ann_file_path.name\n",
        "                        })\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing original file {ann_file_path}: {e}\")\n",
        "        return []\n",
        "    \n",
        "    return entities\n",
        "\n",
        "print(\"✓ Original parsing function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Build Unified Knowledge Base\n",
        "\n",
        "Combine SCT and original annotations into a unified data structure matching entities by file name and character ranges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_unified_knowledge_base(sct_dir: Path, original_dir: Path, \n",
        "                                 tolerance: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Build unified knowledge base by combining SCT and original annotations.\n",
        "    \n",
        "    Matches entities by:\n",
        "    1. File name (same base filename)\n",
        "    2. Character ranges (within tolerance for flexibility)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sct_dir : Path\n",
        "        Directory containing SCT annotation files\n",
        "    original_dir : Path\n",
        "        Directory containing original annotation files\n",
        "    tolerance : int\n",
        "        Character position tolerance for matching (default: 5)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        Unified knowledge base entries with structure:\n",
        "        {\n",
        "            'entity_text': str,\n",
        "            'snomed_code': str,  # Primary SNOMED code (first one if multiple)\n",
        "            'snomed_codes': List[str],  # All SNOMED codes\n",
        "            'snomed_description': str,  # Primary description (first one if multiple)\n",
        "            'snomed_descriptions': List[str],  # All descriptions\n",
        "            'label_type': str,  # ADR, Drug, Disease, Symptom\n",
        "            'char_ranges': tuple,  # (start, end)\n",
        "            'file_name': str\n",
        "        }\n",
        "    \"\"\"\n",
        "    unified_kb = []\n",
        "    \n",
        "    # Get all SCT files\n",
        "    sct_files = list(sct_dir.glob(\"*.ann\"))\n",
        "    print(f\"Found {len(sct_files)} SCT files\")\n",
        "    \n",
        "    # Process each SCT file\n",
        "    matched_count = 0\n",
        "    unmatched_sct_count = 0\n",
        "    \n",
        "    for sct_file in tqdm(sct_files, desc=\"Building knowledge base\"):\n",
        "        # Parse SCT file\n",
        "        sct_entities = parse_sct_file(sct_file)\n",
        "        \n",
        "        # Find corresponding original file\n",
        "        original_file = original_dir / sct_file.name\n",
        "        if not original_file.exists():\n",
        "            # Try to match without extension issues\n",
        "            continue\n",
        "        \n",
        "        # Parse original file\n",
        "        original_entities = parse_original_file(original_file)\n",
        "        \n",
        "        # Match entities by character ranges\n",
        "        # Create a mapping from ranges to original entities\n",
        "        original_by_range = {}\n",
        "        for orig_ent in original_entities:\n",
        "            range_key = (orig_ent['char_start'], orig_ent['char_end'])\n",
        "            # Allow multiple entities at same range (shouldn't happen, but handle it)\n",
        "            if range_key not in original_by_range:\n",
        "                original_by_range[range_key] = []\n",
        "            original_by_range[range_key].append(orig_ent)\n",
        "        \n",
        "        # Match SCT entities with original entities\n",
        "        for sct_ent in sct_entities:\n",
        "            sct_start = sct_ent['char_start']\n",
        "            sct_end = sct_ent['char_end']\n",
        "            \n",
        "            # Find matching original entity (within tolerance)\n",
        "            matched = False\n",
        "            for (orig_start, orig_end), orig_ents in original_by_range.items():\n",
        "                # Check if ranges overlap or are within tolerance\n",
        "                if (abs(sct_start - orig_start) <= tolerance and \n",
        "                    abs(sct_end - orig_end) <= tolerance):\n",
        "                    # Match found\n",
        "                    orig_ent = orig_ents[0]  # Take first if multiple\n",
        "                    \n",
        "                    # Create unified entry\n",
        "                    unified_entry = {\n",
        "                        'entity_text': sct_ent['entity_text'],  # Use entity text from SCT\n",
        "                        'snomed_code': sct_ent['snomed_codes'][0],  # Primary code\n",
        "                        'snomed_codes': sct_ent['snomed_codes'],  # All codes\n",
        "                        'snomed_description': sct_ent['snomed_descriptions'][0],  # Primary description\n",
        "                        'snomed_descriptions': sct_ent['snomed_descriptions'],  # All descriptions\n",
        "                        'label_type': orig_ent['label_type'],\n",
        "                        'char_ranges': (sct_start, sct_end),\n",
        "                        'file_name': sct_file.name\n",
        "                    }\n",
        "                    unified_kb.append(unified_entry)\n",
        "                    matched = True\n",
        "                    matched_count += 1\n",
        "                    break\n",
        "            \n",
        "            if not matched:\n",
        "                unmatched_sct_count += 1\n",
        "                # Still add to KB even without label_type match (can infer from context)\n",
        "                unified_entry = {\n",
        "                    'entity_text': sct_ent['entity_text'],\n",
        "                    'snomed_code': sct_ent['snomed_codes'][0],\n",
        "                    'snomed_codes': sct_ent['snomed_codes'],\n",
        "                    'snomed_description': sct_ent['snomed_descriptions'][0],\n",
        "                    'snomed_descriptions': sct_ent['snomed_descriptions'],\n",
        "                    'label_type': 'Unknown',  # Not matched\n",
        "                    'char_ranges': (sct_start, sct_end),\n",
        "                    'file_name': sct_file.name\n",
        "                }\n",
        "                unified_kb.append(unified_entry)\n",
        "    \n",
        "    print(f\"\\n✓ Knowledge base built:\")\n",
        "    print(f\"  - Total entries: {len(unified_kb)}\")\n",
        "    print(f\"  - Matched entries: {matched_count}\")\n",
        "    print(f\"  - Unmatched SCT entries: {unmatched_sct_count}\")\n",
        "    \n",
        "    # Statistics by label type\n",
        "    label_counts = Counter(entry['label_type'] for entry in unified_kb)\n",
        "    print(f\"\\n  Label type distribution:\")\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        print(f\"    {label}: {count}\")\n",
        "    \n",
        "    return unified_kb\n",
        "\n",
        "# Build the knowledge base\n",
        "print(\"Building unified knowledge base...\")\n",
        "knowledge_base = build_unified_knowledge_base(SCT_DIR, ORIGINAL_DIR)\n",
        "\n",
        "# Display sample entries\n",
        "print(f\"\\n✓ Sample knowledge base entries (first 5):\")\n",
        "for i, entry in enumerate(knowledge_base[:5]):\n",
        "    print(f\"\\n  Entry {i+1}:\")\n",
        "    for key, value in entry.items():\n",
        "        if isinstance(value, list):\n",
        "            print(f\"    {key}: {value[:3]}...\" if len(value) > 3 else f\"    {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Load Predicted ADR Entities from Task 2\n",
        "\n",
        "We need to load ADR entity predictions from Task 2 NER pipeline. For this task, we'll either:\n",
        "1. Run Task 2 pipeline on a sample of files, or\n",
        "2. Use ground truth ADR entities from original annotations (as proxy for predictions)\n",
        "\n",
        "We'll use ground truth ADR entities as a proxy to demonstrate the normalization approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_predicted_adr_entities(original_dir: Path, max_files: Optional[int] = None) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load predicted ADR entities. For this demonstration, we use ground truth ADR entities\n",
        "    from original annotations as a proxy for Task 2 predictions.\n",
        "    \n",
        "    In a real scenario, this would load from Task 2 NER pipeline output.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    original_dir : Path\n",
        "        Directory containing original annotation files\n",
        "    max_files : Optional[int]\n",
        "        Maximum number of files to process (None for all)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of predicted ADR entity dictionaries with:\n",
        "        - 'entity_text': Entity text\n",
        "        - 'char_start': Start character position\n",
        "        - 'char_end': End character position\n",
        "        - 'file_name': Source file name\n",
        "    \"\"\"\n",
        "    predicted_adr = []\n",
        "    \n",
        "    # Get original annotation files\n",
        "    original_files = list(original_dir.glob(\"*.ann\"))\n",
        "    if max_files:\n",
        "        original_files = original_files[:max_files]\n",
        "    \n",
        "    print(f\"Loading ADR entities from {len(original_files)} files...\")\n",
        "    \n",
        "    for ann_file in tqdm(original_files, desc=\"Loading ADR entities\"):\n",
        "        original_entities = parse_original_file(ann_file)\n",
        "        \n",
        "        # Filter to ADR only\n",
        "        adr_entities = [ent for ent in original_entities if ent['label_type'] == 'ADR']\n",
        "        \n",
        "        # Convert to predicted format\n",
        "        for ent in adr_entities:\n",
        "            predicted_adr.append({\n",
        "                'entity_text': ent['entity_text'],\n",
        "                'char_start': ent['char_start'],\n",
        "                'char_end': ent['char_end'],\n",
        "                'file_name': ent['file_name']\n",
        "            })\n",
        "    \n",
        "    print(f\"✓ Loaded {len(predicted_adr)} predicted ADR entities\")\n",
        "    return predicted_adr\n",
        "\n",
        "# Load predicted ADR entities (using ground truth as proxy)\n",
        "# In production, these would come from Task 2 NER pipeline\n",
        "predicted_adr_entities = load_predicted_adr_entities(ORIGINAL_DIR, max_files=100)  # Use 100 files for faster processing\n",
        "\n",
        "print(f\"\\nSample predicted ADR entities (first 5):\")\n",
        "for i, ent in enumerate(predicted_adr_entities[:5]):\n",
        "    print(f\"  {i+1}. '{ent['entity_text']}' [{ent['char_start']}-{ent['char_end']}] from {ent['file_name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach A: Fuzzy String Matching\n",
        "\n",
        "Implement fuzzy matching using fuzzywuzzy library with `fuzz.token_set_ratio()` for word-order independence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fuzzy_match_entity(entity_text: str, kb_descriptions: List[str], \n",
        "                       kb_entries: List[Dict], threshold: int = 80) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Match an entity text against SNOMED descriptions using fuzzy string matching.\n",
        "    \n",
        "    Uses fuzz.token_set_ratio() for word-order independence, which is important\n",
        "    for medical terminology where word order can vary.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    entity_text : str\n",
        "        Entity text to match\n",
        "    kb_descriptions : List[str]\n",
        "        List of SNOMED descriptions from knowledge base\n",
        "    kb_entries : List[Dict]\n",
        "        Full knowledge base entries (corresponding to descriptions)\n",
        "    threshold : int\n",
        "        Minimum similarity score (0-100) to consider a match\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Optional[Dict]\n",
        "        Matched entry with additional fields:\n",
        "        - 'match_score': Similarity score (0-100)\n",
        "        - 'matched_description': The description that matched\n",
        "        - Or None if no match above threshold\n",
        "    \"\"\"\n",
        "    if not kb_descriptions or not kb_entries:\n",
        "        return None\n",
        "    \n",
        "    # Use fuzzywuzzy's process.extractOne for efficient matching\n",
        "    # This finds the best match automatically\n",
        "    # extractOne returns (matched_string, score) tuple\n",
        "    result = fuzzy_process.extractOne(\n",
        "        entity_text,\n",
        "        kb_descriptions,\n",
        "        scorer=fuzz.token_set_ratio  # Word-order independent\n",
        "    )\n",
        "    \n",
        "    if result:\n",
        "        matched_description, score = result\n",
        "        \n",
        "        if score >= threshold:\n",
        "            # Find the index of the matched description\n",
        "            try:\n",
        "                idx = kb_descriptions.index(matched_description)\n",
        "            except ValueError:\n",
        "                # If exact match not found, search for the best match manually\n",
        "                # This shouldn't happen, but handle it gracefully\n",
        "                best_score = 0\n",
        "                best_idx = 0\n",
        "                for i, desc in enumerate(kb_descriptions):\n",
        "                    current_score = fuzz.token_set_ratio(entity_text, desc)\n",
        "                    if current_score > best_score:\n",
        "                        best_score = current_score\n",
        "                        best_idx = i\n",
        "                idx = best_idx\n",
        "            \n",
        "            # Get the corresponding knowledge base entry\n",
        "            matched_entry = kb_entries[idx].copy()\n",
        "            matched_entry['match_score'] = score\n",
        "            matched_entry['matched_description'] = matched_description\n",
        "            matched_entry['match_method'] = 'fuzzy'\n",
        "            return matched_entry\n",
        "    \n",
        "    return None\n",
        "\n",
        "def batch_fuzzy_match(predicted_entities: List[Dict], knowledge_base: List[Dict],\n",
        "                     threshold: int = 80) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Batch process fuzzy matching for multiple predicted entities.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    predicted_entities : List[Dict]\n",
        "        List of predicted ADR entities to match\n",
        "    knowledge_base : List[Dict]\n",
        "        Unified knowledge base\n",
        "    threshold : int\n",
        "        Minimum similarity score threshold\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of match results, each containing:\n",
        "        - Original entity info\n",
        "        - Matched SNOMED code and description\n",
        "        - Match score\n",
        "        - Match method ('fuzzy')\n",
        "    \"\"\"\n",
        "    # Prepare descriptions and entries for efficient matching\n",
        "    kb_descriptions = [entry['snomed_description'] for entry in knowledge_base]\n",
        "    kb_entries = knowledge_base\n",
        "    \n",
        "    print(f\"\\nPerforming fuzzy matching on {len(predicted_entities)} entities...\")\n",
        "    print(f\"  Knowledge base size: {len(knowledge_base)} SNOMED descriptions\")\n",
        "    print(f\"  Threshold: {threshold}%\")\n",
        "    \n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for pred_ent in tqdm(predicted_entities, desc=\"Fuzzy matching\"):\n",
        "        entity_text = pred_ent['entity_text']\n",
        "        \n",
        "        match_result = fuzzy_match_entity(\n",
        "            entity_text, \n",
        "            kb_descriptions, \n",
        "            kb_entries, \n",
        "            threshold\n",
        "        )\n",
        "        \n",
        "        result = {\n",
        "            'entity_text': entity_text,\n",
        "            'char_start': pred_ent['char_start'],\n",
        "            'char_end': pred_ent['char_end'],\n",
        "            'file_name': pred_ent['file_name']\n",
        "        }\n",
        "        \n",
        "        if match_result:\n",
        "            result.update({\n",
        "                'matched': True,\n",
        "                'snomed_code': match_result['snomed_code'],\n",
        "                'snomed_description': match_result['snomed_description'],\n",
        "                'match_score': match_result['match_score'],\n",
        "                'matched_description': match_result['matched_description'],\n",
        "                'label_type': match_result.get('label_type', 'Unknown')\n",
        "            })\n",
        "        else:\n",
        "            result.update({\n",
        "                'matched': False,\n",
        "                'snomed_code': None,\n",
        "                'snomed_description': None,\n",
        "                'match_score': 0,\n",
        "                'matched_description': None,\n",
        "                'label_type': None\n",
        "            })\n",
        "        \n",
        "        results.append(result)\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    matched_count = sum(1 for r in results if r['matched'])\n",
        "    print(f\"\\n✓ Fuzzy matching completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"  - Matched: {matched_count}/{len(predicted_entities)} ({100*matched_count/len(predicted_entities):.1f}%)\")\n",
        "    print(f\"  - Average time per entity: {elapsed_time/len(predicted_entities)*1000:.2f} ms\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Perform fuzzy matching\n",
        "print(\"=\" * 80)\n",
        "print(\"APPROACH A: Fuzzy String Matching\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fuzzy_results = batch_fuzzy_match(predicted_adr_entities, knowledge_base, threshold=FUZZY_THRESHOLD)\n",
        "\n",
        "# Display sample matches\n",
        "print(f\"\\nSample fuzzy matches (first 10):\")\n",
        "for i, result in enumerate(fuzzy_results[:10]):\n",
        "    status = \"✓ MATCHED\" if result['matched'] else \"✗ NO MATCH\"\n",
        "    score_str = f\" (score: {result['match_score']})\" if result['matched'] else \"\"\n",
        "    print(f\"  {i+1}. {status}{score_str}\")\n",
        "    print(f\"     Entity: '{result['entity_text']}'\")\n",
        "    if result['matched']:\n",
        "        print(f\"     → SNOMED: {result['snomed_code']} | {result['snomed_description']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach B: Embedding-Based Matching\n",
        "\n",
        "Implement embedding-based matching using sentence transformers. Pre-compute embeddings for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_embedding_model(model_name: str = EMBEDDING_MODEL_NAME):\n",
        "    \"\"\"\n",
        "    Load sentence embedding model from Hugging Face.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_name : str\n",
        "        Hugging Face model name\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    SentenceTransformer\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    print(f\"Loading embedding model: {model_name}...\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(f\"✓ Model loaded (device: {model.device})\")\n",
        "    return model\n",
        "\n",
        "def precompute_kb_embeddings(knowledge_base: List[Dict], model: SentenceTransformer) -> Tuple[np.ndarray, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Pre-compute embeddings for all SNOMED descriptions in knowledge base.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    knowledge_base : List[Dict]\n",
        "        Unified knowledge base\n",
        "    model : SentenceTransformer\n",
        "        Embedding model\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[np.ndarray, List[Dict]]\n",
        "        - Embeddings matrix (n_samples, embedding_dim)\n",
        "        - List of corresponding KB entries\n",
        "    \"\"\"\n",
        "    print(f\"\\nPre-computing embeddings for {len(knowledge_base)} SNOMED descriptions...\")\n",
        "    \n",
        "    descriptions = [entry['snomed_description'] for entry in knowledge_base]\n",
        "    \n",
        "    # Batch encode for efficiency\n",
        "    start_time = time.time()\n",
        "    embeddings = model.encode(descriptions, show_progress_bar=True, batch_size=32)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"✓ Embeddings computed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"  - Embedding dimension: {embeddings.shape[1]}\")\n",
        "    \n",
        "    return embeddings, knowledge_base\n",
        "\n",
        "def embedding_match_entity(entity_text: str, entity_embedding: np.ndarray,\n",
        "                          kb_embeddings: np.ndarray, kb_entries: List[Dict],\n",
        "                          threshold: float = 0.7) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Match an entity using embedding cosine similarity.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    entity_text : str\n",
        "        Entity text (for reference)\n",
        "    entity_embedding : np.ndarray\n",
        "        Embedding vector for the entity (1, dim)\n",
        "    kb_embeddings : np.ndarray\n",
        "        Embedding matrix for knowledge base (n_samples, dim)\n",
        "    kb_entries : List[Dict]\n",
        "        Knowledge base entries corresponding to embeddings\n",
        "    threshold : float\n",
        "        Minimum cosine similarity threshold (0-1)\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Optional[Dict]\n",
        "        Matched entry with additional fields:\n",
        "        - 'match_score': Cosine similarity score (0-1)\n",
        "        - 'matched_description': The description that matched\n",
        "        - Or None if no match above threshold\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarities\n",
        "    # entity_embedding is (1, dim), kb_embeddings is (n_samples, dim)\n",
        "    similarities = cosine_similarity(\n",
        "        entity_embedding.reshape(1, -1),\n",
        "        kb_embeddings\n",
        "    )[0]  # Get 1D array of similarities\n",
        "    \n",
        "    # Find best match\n",
        "    best_idx = np.argmax(similarities)\n",
        "    best_score = float(similarities[best_idx])\n",
        "    \n",
        "    if best_score >= threshold:\n",
        "        matched_entry = kb_entries[best_idx].copy()\n",
        "        matched_entry['match_score'] = best_score\n",
        "        matched_entry['matched_description'] = matched_entry['snomed_description']\n",
        "        matched_entry['match_method'] = 'embedding'\n",
        "        return matched_entry\n",
        "    \n",
        "    return None\n",
        "\n",
        "def batch_embedding_match(predicted_entities: List[Dict], knowledge_base: List[Dict],\n",
        "                         kb_embeddings: np.ndarray, model: SentenceTransformer,\n",
        "                         threshold: float = 0.7) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Batch process embedding-based matching for multiple predicted entities.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    predicted_entities : List[Dict]\n",
        "        List of predicted ADR entities to match\n",
        "    knowledge_base : List[Dict]\n",
        "        Unified knowledge base\n",
        "    kb_embeddings : np.ndarray\n",
        "        Pre-computed embeddings for knowledge base\n",
        "    model : SentenceTransformer\n",
        "        Embedding model\n",
        "    threshold : float\n",
        "        Minimum cosine similarity threshold\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict]\n",
        "        List of match results, each containing:\n",
        "        - Original entity info\n",
        "        - Matched SNOMED code and description\n",
        "        - Match score (cosine similarity)\n",
        "        - Match method ('embedding')\n",
        "    \"\"\"\n",
        "    print(f\"\\nPerforming embedding-based matching on {len(predicted_entities)} entities...\")\n",
        "    print(f\"  Knowledge base size: {len(knowledge_base)} SNOMED descriptions\")\n",
        "    print(f\"  Threshold: {threshold}\")\n",
        "    \n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Encode all entity texts in batch for efficiency\n",
        "    entity_texts = [ent['entity_text'] for ent in predicted_entities]\n",
        "    print(\"  Encoding entity texts...\")\n",
        "    entity_embeddings = model.encode(entity_texts, show_progress_bar=True, batch_size=32)\n",
        "    \n",
        "    # Match each entity\n",
        "    print(\"  Matching entities...\")\n",
        "    for i, pred_ent in enumerate(tqdm(predicted_entities, desc=\"Embedding matching\")):\n",
        "        entity_embedding = entity_embeddings[i]\n",
        "        \n",
        "        match_result = embedding_match_entity(\n",
        "            pred_ent['entity_text'],\n",
        "            entity_embedding,\n",
        "            kb_embeddings,\n",
        "            knowledge_base,\n",
        "            threshold\n",
        "        )\n",
        "        \n",
        "        result = {\n",
        "            'entity_text': pred_ent['entity_text'],\n",
        "            'char_start': pred_ent['char_start'],\n",
        "            'char_end': pred_ent['char_end'],\n",
        "            'file_name': pred_ent['file_name']\n",
        "        }\n",
        "        \n",
        "        if match_result:\n",
        "            result.update({\n",
        "                'matched': True,\n",
        "                'snomed_code': match_result['snomed_code'],\n",
        "                'snomed_description': match_result['snomed_description'],\n",
        "                'match_score': match_result['match_score'],\n",
        "                'matched_description': match_result['matched_description'],\n",
        "                'label_type': match_result.get('label_type', 'Unknown')\n",
        "            })\n",
        "        else:\n",
        "            result.update({\n",
        "                'matched': False,\n",
        "                'snomed_code': None,\n",
        "                'snomed_description': None,\n",
        "                'match_score': 0.0,\n",
        "                'matched_description': None,\n",
        "                'label_type': None\n",
        "            })\n",
        "        \n",
        "        results.append(result)\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    matched_count = sum(1 for r in results if r['matched'])\n",
        "    print(f\"\\n✓ Embedding matching completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"  - Matched: {matched_count}/{len(predicted_entities)} ({100*matched_count/len(predicted_entities):.1f}%)\")\n",
        "    print(f\"  - Average time per entity: {elapsed_time/len(predicted_entities)*1000:.2f} ms\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Load embedding model and perform matching\n",
        "print(\"=\" * 80)\n",
        "print(\"APPROACH B: Embedding-Based Matching\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "embedding_model = load_embedding_model()\n",
        "kb_embeddings, kb_entries_for_embedding = precompute_kb_embeddings(knowledge_base, embedding_model)\n",
        "\n",
        "embedding_results = batch_embedding_match(\n",
        "    predicted_adr_entities, \n",
        "    knowledge_base,\n",
        "    kb_embeddings,\n",
        "    embedding_model,\n",
        "    threshold=EMBEDDING_THRESHOLD\n",
        ")\n",
        "\n",
        "# Display sample matches\n",
        "print(f\"\\nSample embedding matches (first 10):\")\n",
        "for i, result in enumerate(embedding_results[:10]):\n",
        "    status = \"✓ MATCHED\" if result['matched'] else \"✗ NO MATCH\"\n",
        "    score_str = f\" (score: {result['match_score']:.3f})\" if result['matched'] else \"\"\n",
        "    print(f\"  {i+1}. {status}{score_str}\")\n",
        "    print(f\"     Entity: '{result['entity_text']}'\")\n",
        "    if result['matched']:\n",
        "        print(f\"     → SNOMED: {result['snomed_code']} | {result['snomed_description']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Comparison and Evaluation\n",
        "\n",
        "Compare both approaches:\n",
        "1. Agreement rate between methods\n",
        "2. Accuracy comparison against ground truth\n",
        "3. Runtime performance comparison\n",
        "4. Case analysis where methods agree/disagree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_matching_approaches(fuzzy_results: List[Dict], embedding_results: List[Dict],\n",
        "                                 knowledge_base: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation and comparison of both matching approaches.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    fuzzy_results : List[Dict]\n",
        "        Results from fuzzy matching approach\n",
        "    embedding_results : List[Dict]\n",
        "        Results from embedding matching approach\n",
        "    knowledge_base : List[Dict]\n",
        "        Knowledge base for ground truth lookup\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    Dict\n",
        "        Comprehensive evaluation metrics and analysis\n",
        "    \"\"\"\n",
        "    # Create ground truth mapping by entity text and file\n",
        "    # Map: (file_name, char_start, char_end) -> SNOMED code\n",
        "    gt_map = {}\n",
        "    for kb_entry in knowledge_base:\n",
        "        key = (kb_entry['file_name'], kb_entry['char_ranges'][0], kb_entry['char_ranges'][1])\n",
        "        gt_map[key] = kb_entry['snomed_code']\n",
        "    \n",
        "    # Initialize metrics\n",
        "    metrics = {\n",
        "        'fuzzy_matched': 0,\n",
        "        'embedding_matched': 0,\n",
        "        'both_matched': 0,\n",
        "        'both_unmatched': 0,\n",
        "        'fuzzy_only': 0,\n",
        "        'embedding_only': 0,\n",
        "        'agreement_on_match': 0,\n",
        "        'agreement_on_code': 0,\n",
        "        'fuzzy_correct': 0,\n",
        "        'embedding_correct': 0,\n",
        "        'disagreement_cases': [],\n",
        "        'fuzzy_scores': [],\n",
        "        'embedding_scores': []\n",
        "    }\n",
        "    \n",
        "    # Compare results\n",
        "    for i, (fuzzy_res, emb_res) in enumerate(zip(fuzzy_results, embedding_results)):\n",
        "        # Check match status\n",
        "        fuzzy_matched = fuzzy_res['matched']\n",
        "        embedding_matched = emb_res['matched']\n",
        "        \n",
        "        if fuzzy_matched:\n",
        "            metrics['fuzzy_matched'] += 1\n",
        "            metrics['fuzzy_scores'].append(fuzzy_res['match_score'])\n",
        "        if embedding_matched:\n",
        "            metrics['embedding_matched'] += 1\n",
        "            metrics['embedding_scores'].append(emb_res['match_score'])\n",
        "        \n",
        "        if fuzzy_matched and embedding_matched:\n",
        "            metrics['both_matched'] += 1\n",
        "            \n",
        "            # Check if they agree on the code\n",
        "            if fuzzy_res['snomed_code'] == emb_res['snomed_code']:\n",
        "                metrics['agreement_on_code'] += 1\n",
        "            else:\n",
        "                # Disagreement case\n",
        "                metrics['disagreement_cases'].append({\n",
        "                    'entity_text': fuzzy_res['entity_text'],\n",
        "                    'fuzzy_code': fuzzy_res['snomed_code'],\n",
        "                    'fuzzy_desc': fuzzy_res['snomed_description'],\n",
        "                    'fuzzy_score': fuzzy_res['match_score'],\n",
        "                    'embedding_code': emb_res['snomed_code'],\n",
        "                    'embedding_desc': emb_res['snomed_description'],\n",
        "                    'embedding_score': emb_res['match_score'],\n",
        "                    'file_name': fuzzy_res['file_name']\n",
        "                })\n",
        "            \n",
        "            # Check agreement on match status\n",
        "            metrics['agreement_on_match'] += 1\n",
        "        elif not fuzzy_matched and not embedding_matched:\n",
        "            metrics['both_unmatched'] += 1\n",
        "            metrics['agreement_on_match'] += 1\n",
        "        elif fuzzy_matched:\n",
        "            metrics['fuzzy_only'] += 1\n",
        "        elif embedding_matched:\n",
        "            metrics['embedding_only'] += 1\n",
        "        \n",
        "        # Check accuracy against ground truth (if available)\n",
        "        key = (fuzzy_res['file_name'], fuzzy_res['char_start'], fuzzy_res['char_end'])\n",
        "        if key in gt_map:\n",
        "            gt_code = gt_map[key]\n",
        "            if fuzzy_matched and fuzzy_res['snomed_code'] == gt_code:\n",
        "                metrics['fuzzy_correct'] += 1\n",
        "            if embedding_matched and emb_res['snomed_code'] == gt_code:\n",
        "                metrics['embedding_correct'] += 1\n",
        "    \n",
        "    total = len(fuzzy_results)\n",
        "    \n",
        "    # Calculate percentages\n",
        "    metrics['total_entities'] = total\n",
        "    metrics['fuzzy_match_rate'] = metrics['fuzzy_matched'] / total * 100\n",
        "    metrics['embedding_match_rate'] = metrics['embedding_matched'] / total * 100\n",
        "    metrics['agreement_rate'] = metrics['agreement_on_match'] / total * 100\n",
        "    metrics['code_agreement_rate'] = (metrics['agreement_on_code'] / metrics['both_matched'] * 100) if metrics['both_matched'] > 0 else 0\n",
        "    \n",
        "    # Accuracy rates (if ground truth available)\n",
        "    if metrics['fuzzy_matched'] > 0:\n",
        "        metrics['fuzzy_accuracy'] = metrics['fuzzy_correct'] / metrics['fuzzy_matched'] * 100\n",
        "    else:\n",
        "        metrics['fuzzy_accuracy'] = 0\n",
        "    \n",
        "    if metrics['embedding_matched'] > 0:\n",
        "        metrics['embedding_accuracy'] = metrics['embedding_correct'] / metrics['embedding_matched'] * 100\n",
        "    else:\n",
        "        metrics['embedding_accuracy'] = 0\n",
        "    \n",
        "    # Average scores\n",
        "    metrics['avg_fuzzy_score'] = np.mean(metrics['fuzzy_scores']) if metrics['fuzzy_scores'] else 0\n",
        "    metrics['avg_embedding_score'] = np.mean(metrics['embedding_scores']) if metrics['embedding_scores'] else 0\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Perform comprehensive evaluation\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "evaluation_metrics = evaluate_matching_approaches(fuzzy_results, embedding_results, knowledge_base)\n",
        "\n",
        "# Display metrics\n",
        "print(f\"\\n📊 Matching Statistics:\")\n",
        "print(f\"  Total entities evaluated: {evaluation_metrics['total_entities']}\")\n",
        "print(f\"\\n  Fuzzy Matching:\")\n",
        "print(f\"    - Matched: {evaluation_metrics['fuzzy_matched']} ({evaluation_metrics['fuzzy_match_rate']:.1f}%)\")\n",
        "print(f\"    - Average score: {evaluation_metrics['avg_fuzzy_score']:.1f}\")\n",
        "if evaluation_metrics['fuzzy_matched'] > 0:\n",
        "    print(f\"    - Accuracy (vs ground truth): {evaluation_metrics['fuzzy_correct']}/{evaluation_metrics['fuzzy_matched']} ({evaluation_metrics['fuzzy_accuracy']:.1f}%)\")\n",
        "\n",
        "print(f\"\\n  Embedding Matching:\")\n",
        "print(f\"    - Matched: {evaluation_metrics['embedding_matched']} ({evaluation_metrics['embedding_match_rate']:.1f}%)\")\n",
        "print(f\"    - Average score: {evaluation_metrics['avg_embedding_score']:.3f}\")\n",
        "if evaluation_metrics['embedding_matched'] > 0:\n",
        "    print(f\"    - Accuracy (vs ground truth): {evaluation_metrics['embedding_correct']}/{evaluation_metrics['embedding_matched']} ({evaluation_metrics['embedding_accuracy']:.1f}%)\")\n",
        "\n",
        "print(f\"\\n  Agreement:\")\n",
        "print(f\"    - Both matched: {evaluation_metrics['both_matched']}\")\n",
        "print(f\"    - Both unmatched: {evaluation_metrics['both_unmatched']}\")\n",
        "print(f\"    - Fuzzy only: {evaluation_metrics['fuzzy_only']}\")\n",
        "print(f\"    - Embedding only: {evaluation_metrics['embedding_only']}\")\n",
        "print(f\"    - Agreement rate (on match status): {evaluation_metrics['agreement_rate']:.1f}%\")\n",
        "if evaluation_metrics['both_matched'] > 0:\n",
        "    print(f\"    - Code agreement rate: {evaluation_metrics['code_agreement_rate']:.1f}%\")\n",
        "    print(f\"    - Code disagreements: {len(evaluation_metrics['disagreement_cases'])}\")\n",
        "\n",
        "# Display disagreement cases\n",
        "if evaluation_metrics['disagreement_cases']:\n",
        "    print(f\"\\n🔍 Sample Disagreement Cases (first 5):\")\n",
        "    for i, case in enumerate(evaluation_metrics['disagreement_cases'][:5]):\n",
        "        print(f\"\\n  Case {i+1}: '{case['entity_text']}'\")\n",
        "        print(f\"    Fuzzy: {case['fuzzy_code']} | {case['fuzzy_desc']} (score: {case['fuzzy_score']:.1f})\")\n",
        "        print(f\"    Embedding: {case['embedding_code']} | {case['embedding_desc']} (score: {case['embedding_score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Matching Approaches Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Match rate comparison\n",
        "ax1 = axes[0, 0]\n",
        "methods = ['Fuzzy\\nMatching', 'Embedding\\nMatching']\n",
        "match_rates = [evaluation_metrics['fuzzy_match_rate'], evaluation_metrics['embedding_match_rate']]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "bars = ax1.bar(methods, match_rates, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_ylabel('Match Rate (%)', fontsize=12)\n",
        "ax1.set_title('Match Rate Comparison', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylim(0, 100)\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.1f}%',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# 2. Score distribution comparison\n",
        "ax2 = axes[0, 1]\n",
        "if evaluation_metrics['fuzzy_scores'] and evaluation_metrics['embedding_scores']:\n",
        "    ax2.hist(evaluation_metrics['fuzzy_scores'], bins=20, alpha=0.6, label='Fuzzy Scores', \n",
        "             color='#3498db', edgecolor='black')\n",
        "    ax2.hist([s * 100 for s in evaluation_metrics['embedding_scores']], bins=20, alpha=0.6, \n",
        "             label='Embedding Scores (×100)', color='#e74c3c', edgecolor='black')\n",
        "    ax2.set_xlabel('Score', fontsize=12)\n",
        "    ax2.set_ylabel('Frequency', fontsize=12)\n",
        "    ax2.set_title('Score Distribution', fontsize=13, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# 3. Agreement analysis\n",
        "ax3 = axes[1, 0]\n",
        "agreement_categories = ['Both\\nMatched', 'Both\\nUnmatched', 'Fuzzy\\nOnly', 'Embedding\\nOnly']\n",
        "agreement_counts = [\n",
        "    evaluation_metrics['both_matched'],\n",
        "    evaluation_metrics['both_unmatched'],\n",
        "    evaluation_metrics['fuzzy_only'],\n",
        "    evaluation_metrics['embedding_only']\n",
        "]\n",
        "colors_agreement = ['#27ae60', '#95a5a6', '#3498db', '#e74c3c']\n",
        "bars3 = ax3.bar(agreement_categories, agreement_counts, color=colors_agreement, \n",
        "                alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax3.set_ylabel('Number of Entities', fontsize=12)\n",
        "ax3.set_title('Agreement Analysis', fontsize=13, fontweight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "# Add value labels\n",
        "for bar in bars3:\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}',\n",
        "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# 4. Accuracy comparison (if available)\n",
        "ax4 = axes[1, 1]\n",
        "if evaluation_metrics['fuzzy_correct'] > 0 or evaluation_metrics['embedding_correct'] > 0:\n",
        "    accuracy_methods = ['Fuzzy\\nAccuracy', 'Embedding\\nAccuracy']\n",
        "    accuracy_rates = [evaluation_metrics['fuzzy_accuracy'], evaluation_metrics['embedding_accuracy']]\n",
        "    bars4 = ax4.bar(accuracy_methods, accuracy_rates, color=['#3498db', '#e74c3c'], \n",
        "                    alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax4.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax4.set_title('Accuracy vs Ground Truth', fontsize=13, fontweight='bold')\n",
        "    ax4.set_ylim(0, 100)\n",
        "    ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    # Add value labels\n",
        "    for bar in bars4:\n",
        "        height = bar.get_height()\n",
        "        if height > 0:\n",
        "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                     f'{height:.1f}%',\n",
        "                     ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'Ground truth\\ndata not available\\nfor accuracy calculation',\n",
        "             ha='center', va='center', fontsize=12, transform=ax4.transAxes)\n",
        "    ax4.set_title('Accuracy vs Ground Truth', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualizations created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Detailed Analysis and Discussion\n",
        "\n",
        "Provide comprehensive analysis of the results, strengths/weaknesses of each approach, and recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create detailed comparison DataFrame\n",
        "comparison_data = []\n",
        "for i, (fuzzy_res, emb_res) in enumerate(zip(fuzzy_results, embedding_results)):\n",
        "    comparison_data.append({\n",
        "        'entity_text': fuzzy_res['entity_text'],\n",
        "        'fuzzy_matched': fuzzy_res['matched'],\n",
        "        'fuzzy_code': fuzzy_res.get('snomed_code', None),\n",
        "        'fuzzy_desc': fuzzy_res.get('snomed_description', None),\n",
        "        'fuzzy_score': fuzzy_res.get('match_score', 0),\n",
        "        'embedding_matched': emb_res['matched'],\n",
        "        'embedding_code': emb_res.get('snomed_code', None),\n",
        "        'embedding_desc': emb_res.get('snomed_description', None),\n",
        "        'embedding_score': emb_res.get('match_score', 0),\n",
        "        'codes_agree': (fuzzy_res.get('snomed_code') == emb_res.get('snomed_code')) \n",
        "                       if (fuzzy_res['matched'] and emb_res['matched']) else None\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DETAILED ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. STRENGTHS AND WEAKNESSES OF EACH APPROACH\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n📌 Fuzzy String Matching (Approach A):\")\n",
        "print(\"  ✅ Strengths:\")\n",
        "print(\"     - Fast and lightweight (no model loading required)\")\n",
        "print(\"     - Good for exact or near-exact text matches\")\n",
        "print(\"     - Word-order independent (token_set_ratio handles variations)\")\n",
        "print(\"     - Interpretable similarity scores (0-100)\")\n",
        "print(\"     - Works well with typographical variations\")\n",
        "print(\"  ❌ Weaknesses:\")\n",
        "print(\"     - Struggles with semantic similarity (e.g., 'tired' vs 'fatigue')\")\n",
        "print(\"     - Cannot handle synonyms or paraphrasing\")\n",
        "print(\"     - May fail with abbreviations vs full terms\")\n",
        "print(\"     - No understanding of medical domain context\")\n",
        "\n",
        "print(\"\\n📌 Embedding-Based Matching (Approach B):\")\n",
        "print(\"  ✅ Strengths:\")\n",
        "print(\"     - Captures semantic similarity (understands synonyms)\")\n",
        "print(\"     - Better with paraphrasing and contextual variations\")\n",
        "print(\"     - Can match conceptually related terms\")\n",
        "print(\"     - More robust to wording variations\")\n",
        "print(\"  ❌ Weaknesses:\")\n",
        "print(\"     - Slower (requires model loading and encoding)\")\n",
        "print(\"     - Higher computational and memory requirements\")\n",
        "print(\"     - May match semantically related but incorrect concepts\")\n",
        "print(\"     - Less interpretable similarity scores\")\n",
        "print(\"     - Dependent on model quality and domain relevance\")\n",
        "\n",
        "print(\"\\n2. WHEN EACH APPROACH OUTPERFORMS\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\n  Fuzzy Matching excels when:\")\n",
        "print(\"    - Entity text closely matches SNOMED description wording\")\n",
        "print(\"    - Speed is critical (real-time processing)\")\n",
        "print(\"    - Computational resources are limited\")\n",
        "print(\"    - Handling typos and spelling variations is important\")\n",
        "\n",
        "print(\"\\n  Embedding Matching excels when:\")\n",
        "print(\"    - Entity text uses different wording than SNOMED descriptions\")\n",
        "print(\"    - Semantic understanding is crucial (synonyms, paraphrases)\")\n",
        "print(\"    - Processing can be batched for efficiency\")\n",
        "print(\"    - Computational resources allow for model inference\")\n",
        "\n",
        "print(\"\\n3. RECOMMENDATIONS FOR PRODUCTION USE\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\n  🔄 Hybrid Approach:\")\n",
        "print(\"    - Use fuzzy matching as first pass (fast filtering)\")\n",
        "print(\"    - Apply embedding matching to fuzzy-matched candidates (refinement)\")\n",
        "print(\"    - Use embedding matching for fuzzy-unmatched entities (second chance)\")\n",
        "\n",
        "print(\"\\n  ⚙️ Optimization Strategies:\")\n",
        "print(\"    - Pre-compute SNOMED embeddings (done once, reused many times)\")\n",
        "print(\"    - Batch entity encoding for embedding approach\")\n",
        "print(\"    - Cache frequently matched entities\")\n",
        "print(\"    - Adjust thresholds based on precision/recall trade-offs\")\n",
        "\n",
        "print(\"\\n  📊 Quality Assurance:\")\n",
        "print(\"    - Monitor disagreement cases between approaches\")\n",
        "print(\"    - Human review of low-confidence matches\")\n",
        "print(\"    - Track accuracy metrics over time\")\n",
        "print(\"    - Update knowledge base regularly\")\n",
        "\n",
        "print(\"\\n4. MEDICAL TERMINOLOGY NORMALIZATION CHALLENGES\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\n  Medical entity normalization faces unique challenges:\")\n",
        "print(\"    • Synonym variations: 'drowsy' vs 'sleepy' vs 'somnolent'\")\n",
        "print(\"    • Abbreviation ambiguity: 'MI' can mean 'myocardial infarction' or 'mitral insufficiency'\")\n",
        "print(\"    • Lay vs medical terminology: 'heart attack' vs 'myocardial infarction'\")\n",
        "print(\"    • Severity variations: 'mild pain' vs 'severe pain' vs 'agony'\")\n",
        "print(\"    • Multi-word phrases with different word orders\")\n",
        "print(\"    • Misspellings and typos in clinical notes\")\n",
        "\n",
        "print(\"\\n5. IMPORTANCE OF STANDARDIZED CODING IN HEALTHCARE\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\n  Standardized coding systems like SNOMED CT are critical because:\")\n",
        "print(\"    ✓ Enable interoperability between healthcare systems\")\n",
        "print(\"    ✓ Support clinical decision support systems\")\n",
        "print(\"    ✓ Facilitate research and data aggregation\")\n",
        "print(\"    ✓ Ensure consistent documentation across providers\")\n",
        "print(\"    ✓ Enable automated analysis and reporting\")\n",
        "print(\"    ✓ Improve patient safety through standardized communication\")\n",
        "print(\"    ✓ Support regulatory compliance and quality measures\")\n",
        "\n",
        "print(\"\\n6. SAMPLE COMPARISON DATA\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\nShowing first 10 rows of comparison data:\")\n",
        "print(comparison_df.head(10).to_string(index=False))\n",
        "\n",
        "# Save results to CSV for further analysis\n",
        "output_file = \"task6_comparison_results.csv\"\n",
        "comparison_df.to_csv(output_file, index=False)\n",
        "print(f\"\\n✓ Comparison results saved to: {output_file}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n7. SUMMARY STATISTICS\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  Total entities: {len(comparison_df)}\")\n",
        "print(f\"  Entities matched by both methods: {sum(comparison_df['fuzzy_matched'] & comparison_df['embedding_matched'])}\")\n",
        "print(f\"  Entities matched only by fuzzy: {sum(comparison_df['fuzzy_matched'] & ~comparison_df['embedding_matched'])}\")\n",
        "print(f\"  Entities matched only by embedding: {sum(~comparison_df['fuzzy_matched'] & comparison_df['embedding_matched'])}\")\n",
        "print(f\"  Entities unmatched by both: {sum(~comparison_df['fuzzy_matched'] & ~comparison_df['embedding_matched'])}\")\n",
        "\n",
        "matched_by_both = comparison_df[comparison_df['fuzzy_matched'] & comparison_df['embedding_matched']]\n",
        "if len(matched_by_both) > 0:\n",
        "    codes_agree_count = matched_by_both['codes_agree'].sum()\n",
        "    print(f\"  When both matched, codes agree: {codes_agree_count}/{len(matched_by_both)} ({codes_agree_count/len(matched_by_both)*100:.1f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
