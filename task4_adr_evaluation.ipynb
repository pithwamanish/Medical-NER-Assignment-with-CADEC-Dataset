{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ADR-Specific Evaluation with MedDRA Annotations\n",
    "\n",
    "**Title:** \"ADR-Focused Performance Analysis with MedDRA Ground Truth\"\n",
    "\n",
    "## Objective\n",
    "Extend Task 3 evaluation to focus specifically on ADR entities using MedDRA annotations:\n",
    "- Load ground truth from 'meddra' subdirectory (contains only ADR entities with MedDRA codes)\n",
    "- Parse MedDRA format (TT prefix + MedDRA code + character ranges + entity text)\n",
    "- Filter predicted entities to only ADR labels\n",
    "- Match predicted ADR entities against MedDRA ground truth with exact span matching\n",
    "- Calculate ADR-specific Precision, Recall, F1\n",
    "- Compare with ADR performance from Task 3 (original annotations)\n",
    "- Analyze differences between original and MedDRA ground truth\n",
    "\n",
    "## Overview\n",
    "This notebook extends Task 3 evaluation framework to focus specifically on Adverse Drug Reaction (ADR) entities using MedDRA-standardized annotations as ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding MedDRA Annotation Format\n",
    "\n",
    "### MedDRA Format Explanation\n",
    "\n",
    "The MedDRA (Medical Dictionary for Regulatory Activities) annotation format contains **only ADR entities** with standardized medical codes:\n",
    "\n",
    "**Format Structure:**\n",
    "```\n",
    "TT<original_tag>\\t<MedDRA_code> <start> <end>\\t<entity_text>\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "TT1\\t10028836 9 18\\tneck pain\n",
    "TT2\\t10001949 20 31\\tmemory loss\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Identifier**: `TT1`, `TT2`, etc. (TT prefix + original tag from 'original' directory)\n",
    "2. **MedDRA Code**: Numeric code (e.g., `10028836`) - standardized medical term identifier\n",
    "3. **Character Ranges**: Start and end positions (e.g., `9 18`)\n",
    "4. **Entity Text**: The actual ADR mention in the text (e.g., `neck pain`)\n",
    "\n",
    "### Why ADR Detection is Critical in Pharmacovigilance\n",
    "\n",
    "**Adverse Drug Reaction (ADR) detection is particularly important in pharmacovigilance** for several reasons:\n",
    "\n",
    "1. **Patient Safety**: ADRs can range from mild discomfort to life-threatening conditions. Accurate detection enables timely medical intervention.\n",
    "\n",
    "2. **Regulatory Compliance**: Pharmaceutical companies must report ADRs to regulatory bodies (FDA, EMA). Standardized MedDRA coding ensures consistent reporting.\n",
    "\n",
    "3. **Signal Detection**: Automated ADR detection from patient forums, social media, and clinical notes helps identify potential safety signals early.\n",
    "\n",
    "4. **Drug Monitoring**: Post-marketing surveillance relies on accurate ADR extraction to monitor drug safety in real-world populations.\n",
    "\n",
    "5. **Knowledge Discovery**: ADR patterns can reveal drug-drug interactions, contraindications, and population-specific risks.\n",
    "\n",
    "6. **Clinical Decision Support**: Healthcare systems use ADR information to alert clinicians about potential adverse events.\n",
    "\n",
    "7. **Standardization**: MedDRA provides a hierarchical taxonomy (SOC → HLGT → HLT → PT → LLT) enabling structured analysis across different data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Install seqeval if not already installed\n",
    "try:\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"⚠ seqeval not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seqeval\"])\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score\n",
    "    )\n",
    "    print(\"✓ seqeval installed successfully\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(\"cadec\")\n",
    "TEXT_DIR = BASE_DIR / \"text\"\n",
    "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
    "MEDDRA_DIR = BASE_DIR / \"meddra\"\n",
    "\n",
    "# Verify directories exist\n",
    "if not TEXT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {TEXT_DIR}\")\n",
    "if not ORIGINAL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {ORIGINAL_DIR}\")\n",
    "if not MEDDRA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {MEDDRA_DIR}\")\n",
    "\n",
    "print(\"✓ Directories verified\")\n",
    "print(f\"  - Text directory: {TEXT_DIR}\")\n",
    "print(f\"  - Original directory: {ORIGINAL_DIR}\")\n",
    "print(f\"  - MedDRA directory: {MEDDRA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse MedDRA Annotation Format\n",
    "\n",
    "MedDRA annotations use a specific format with TT prefix, MedDRA code, and character ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meddra_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse MedDRA ground truth annotation file.\n",
    "    \n",
    "    MedDRA Format: TT<tag>\\t<MedDRA_code1> [ + <MedDRA_code2> ...] <start1> <end1>[;<start2> <end2>...]\\t<text>\n",
    "    Examples:\n",
    "        TT1\\t10028836 9 18\\tneck pain\n",
    "        TT3\\t10033371 + 10023477 13 37;52 57\\tSevere joint pain in the knees\n",
    "        TT4\\t10033430 59 63;77 82;83 88\\tPain in my hands\n",
    "    \n",
    "    Note: Multiple MedDRA codes are joined with '+', multiple ranges are separated by ';'\n",
    "    For entities with multiple ranges, we create separate entity entries for each range.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the .ann annotation file in meddra directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entity dictionaries with:\n",
    "        - 'label': Always 'ADR' (MedDRA contains only ADR entities)\n",
    "        - 'text': Entity text\n",
    "        - 'start': Start character position\n",
    "        - 'end': End character position\n",
    "        - 'tag': Original tag identifier (TT1, TT2, etc.)\n",
    "        - 'meddra_code': First MedDRA standardized code (primary code)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines and comment lines (starting with '#')\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse MedDRA format: TT<tag>\\t<MedDRA_codes> <ranges>\\t<text>\n",
    "                # Split by tab first\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                identifier = parts[0]  # TT1, TT2, etc.\n",
    "                \n",
    "                # Get the metadata part (MedDRA codes + ranges) and text\n",
    "                if len(parts) >= 3:\n",
    "                    metadata_part = parts[1]\n",
    "                    text = parts[2]\n",
    "                else:\n",
    "                    # Sometimes text might be in the same part as metadata\n",
    "                    metadata_part = parts[1]\n",
    "                    # Try to extract text from metadata_part (everything after the last range)\n",
    "                    # This is a fallback - normally text should be in parts[2]\n",
    "                    text = \"\"\n",
    "                \n",
    "                # Parse metadata part: <MedDRA_code1> [ + <MedDRA_code2> ...] <ranges>\n",
    "                # Example: \"10033371 + 10023477 13 37;52 57\"\n",
    "                # Strategy: Use regex to find all MedDRA codes, then everything after is ranges\n",
    "                \n",
    "                # Find all MedDRA codes (8+ digit numbers, or CONCEPT_LESS)\n",
    "                # MedDRA codes are typically 8 digits starting with 100\n",
    "                meddra_code_pattern = r'\\b(100\\d{5}|\\d{8,}|CONCEPT_LESS)\\b'\n",
    "                code_matches = list(re.finditer(meddra_code_pattern, metadata_part))\n",
    "                \n",
    "                if not code_matches:\n",
    "                    # No codes found, skip this line\n",
    "                    continue\n",
    "                \n",
    "                # Get the primary MedDRA code (first one found)\n",
    "                primary_meddra_code = code_matches[0].group(1)\n",
    "                \n",
    "                # Find where ranges start (after the last code)\n",
    "                last_code_end = code_matches[-1].end()\n",
    "                ranges_str = metadata_part[last_code_end:].strip()\n",
    "                \n",
    "                # Remove any '+' or extra whitespace\n",
    "                ranges_str = re.sub(r'\\s*\\+\\s*', '', ranges_str)\n",
    "                ranges_str = ranges_str.strip()\n",
    "                \n",
    "                # Parse ranges: can be multiple pairs separated by semicolons\n",
    "                # Format: \"START1 END1;START2 END2;START3 END3\" or \"START END\"\n",
    "                ranges = []\n",
    "                \n",
    "                if ';' in ranges_str:\n",
    "                    # Multiple ranges format: \"START1 END1;START2 END2;...\"\n",
    "                    # Split by semicolon first, then parse each pair\n",
    "                    range_pairs = ranges_str.split(';')\n",
    "                    for rp in range_pairs:\n",
    "                        rp = rp.strip()\n",
    "                        if rp:\n",
    "                            # Split by whitespace and get first two numbers\n",
    "                            range_nums = rp.split()\n",
    "                            if len(range_nums) >= 2:\n",
    "                                try:\n",
    "                                    start = int(range_nums[0])\n",
    "                                    end = int(range_nums[1])\n",
    "                                    ranges.append((start, end))\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                else:\n",
    "                    # Single range format: \"START END\"\n",
    "                    range_nums = ranges_str.split()\n",
    "                    if len(range_nums) >= 2:\n",
    "                        try:\n",
    "                            start = int(range_nums[0])\n",
    "                            end = int(range_nums[1])\n",
    "                            ranges = [(start, end)]\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                \n",
    "                # If no ranges found, skip this entity\n",
    "                if not ranges:\n",
    "                    continue\n",
    "                \n",
    "                # Create entity entries for each range\n",
    "                # For multiple ranges, we create separate entities (standard practice in NER)\n",
    "                for start, end in ranges:\n",
    "                    entities.append({\n",
    "                        'label': 'ADR',  # MedDRA only contains ADR entities\n",
    "                        'text': text.strip(),\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'tag': identifier,\n",
    "                        'meddra_code': primary_meddra_code\n",
    "                    })\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MedDRA ground truth from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"✓ MedDRA ground truth loading function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Original Annotations (for Comparison with Task 3)\n",
    "\n",
    "We'll also load original annotations to compare ADR performance between original and MedDRA ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse ground truth annotation file from 'original' subdirectory.\n",
    "    This is the same function from Task 3, used for comparison.\n",
    "    \n",
    "    Format: TAG\\tLABEL START END\\tTEXT\n",
    "    Example: T1\\tADR 9 19\\tbit drowsy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the .ann annotation file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entity dictionaries with:\n",
    "        - 'label': Entity type (ADR)\n",
    "        - 'text': Entity text\n",
    "        - 'start': Start character position\n",
    "        - 'end': End character position\n",
    "        - 'tag': Original tag identifier (T1, T2, etc.)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines and comment lines (starting with '#')\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse entity annotation lines (starting with 'T' followed by a number)\n",
    "                # Format: TAG\\tLABEL RANGES\\tTEXT\n",
    "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
    "                if match:\n",
    "                    tag = match.group(1)\n",
    "                    label_and_ranges = match.group(2)\n",
    "                    text = match.group(3)\n",
    "                    \n",
    "                    # Extract label type (first word) and ranges (remaining part)\n",
    "                    parts = label_and_ranges.split(None, 1)\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    label_type = parts[0]\n",
    "                    ranges_str = parts[1]\n",
    "                    \n",
    "                    # Only process ADR labels for this task\n",
    "                    if label_type != 'ADR':\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract ranges (can be multiple pairs separated by semicolons)\n",
    "                    ranges = []\n",
    "                    if ';' in ranges_str:\n",
    "                        # Multiple ranges format: \"START1 END1;START2 END2;...\"\n",
    "                        range_pairs = ranges_str.split(';')\n",
    "                        for rp in range_pairs:\n",
    "                            rp = rp.strip()\n",
    "                            if rp:\n",
    "                                range_nums = rp.split()\n",
    "                                if len(range_nums) >= 2:\n",
    "                                    try:\n",
    "                                        start = int(range_nums[0])\n",
    "                                        end = int(range_nums[1])\n",
    "                                        ranges.append((start, end))\n",
    "                                    except ValueError:\n",
    "                                        continue\n",
    "                    else:\n",
    "                        # Single range format: \"START END\"\n",
    "                        range_nums = ranges_str.split()\n",
    "                        if len(range_nums) >= 2:\n",
    "                            try:\n",
    "                                start = int(range_nums[0])\n",
    "                                end = int(range_nums[1])\n",
    "                                ranges = [(start, end)]\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    # Create entity entries for each range\n",
    "                    for start, end in ranges:\n",
    "                        entities.append({\n",
    "                            'label': label_type,\n",
    "                            'text': text.strip(),\n",
    "                            'start': start,\n",
    "                            'end': end,\n",
    "                            'tag': tag\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original ground truth from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"✓ Original ground truth loading function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse predicted annotation file (same format as original annotations).\n",
    "    \n",
    "    Format: TAG\\tLABEL START END\\tTEXT\n",
    "    This should match the output format from Task 2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the predicted .ann file (or can be a list of annotation lines)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of entity dictionaries (all labels)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse: TAG\\tLABEL START END\\tTEXT\n",
    "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
    "                if match:\n",
    "                    tag = match.group(1)\n",
    "                    label_and_ranges = match.group(2)\n",
    "                    text = match.group(3)\n",
    "                    \n",
    "                    parts = label_and_ranges.split(None, 1)\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    label_type = parts[0]\n",
    "                    ranges_str = parts[1]\n",
    "                    \n",
    "                    # Parse ranges\n",
    "                    ranges = []\n",
    "                    if ';' in ranges_str:\n",
    "                        range_pairs = ranges_str.split(';')\n",
    "                        for rp in range_pairs:\n",
    "                            rp = rp.strip()\n",
    "                            if rp:\n",
    "                                range_nums = rp.split()\n",
    "                                if len(range_nums) >= 2:\n",
    "                                    try:\n",
    "                                        start = int(range_nums[0])\n",
    "                                        end = int(range_nums[1])\n",
    "                                        ranges.append((start, end))\n",
    "                                    except ValueError:\n",
    "                                        continue\n",
    "                    else:\n",
    "                        range_nums = ranges_str.split()\n",
    "                        if len(range_nums) >= 2:\n",
    "                            try:\n",
    "                                start = int(range_nums[0])\n",
    "                                end = int(range_nums[1])\n",
    "                                ranges = [(start, end)]\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    for start, end in ranges:\n",
    "                        entities.append({\n",
    "                            'label': label_type,\n",
    "                            'text': text.strip(),\n",
    "                            'start': start,\n",
    "                            'end': end,\n",
    "                            'tag': tag\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading predictions from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def filter_adr_entities(entities: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter entities to only include ADR labels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    entities : List[Dict]\n",
    "        List of all predicted entities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entities only\n",
    "    \"\"\"\n",
    "    return [entity for entity in entities if entity['label'] == 'ADR']\n",
    "\n",
    "print(\"✓ Prediction loading and filtering functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ADR-Specific Evaluation with Exact Span Matching\n",
    "\n",
    "Evaluate ADR entities using exact span matching (same approach as Task 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adr_entities(ground_truth: List[Dict], predictions: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform entity-level evaluation for ADR entities (exact boundary + label matching).\n",
    "    \n",
    "    Entity-level evaluation requires:\n",
    "    - Exact match of entity boundaries (start AND end positions)\n",
    "    - Exact match of label type (ADR)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ground_truth : List[Dict]\n",
    "        List of ground truth ADR entities\n",
    "    predictions : List[Dict]\n",
    "        List of predicted ADR entities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing:\n",
    "        - 'tp', 'fp', 'fn' counts\n",
    "        - 'precision', 'recall', 'f1' scores\n",
    "    \"\"\"\n",
    "    # Convert entities to sets of tuples for exact matching\n",
    "    # Format: (label, start, end) - exact boundary matching required\n",
    "    gt_set = set()\n",
    "    for entity in ground_truth:\n",
    "        gt_set.add((entity['label'], entity['start'], entity['end']))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in predictions:\n",
    "        pred_set.add((entity['label'], entity['start'], entity['end']))\n",
    "    \n",
    "    # Calculate True Positives: entities that appear in both sets\n",
    "    tp = len(gt_set.intersection(pred_set))\n",
    "    \n",
    "    # Calculate False Positives: predicted entities not in ground truth\n",
    "    fp = len(pred_set - gt_set)\n",
    "    \n",
    "    # Calculate False Negatives: ground truth entities not predicted\n",
    "    fn = len(gt_set - pred_set)\n",
    "    \n",
    "    # Calculate Precision, Recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"✓ ADR evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Evaluation Pipeline\n",
    "\n",
    "Evaluate all files and aggregate results for both MedDRA and original ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_files(text_files: List[Path], \n",
    "                       get_predictions_func,\n",
    "                       max_files: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate all text files in the dataset against both MedDRA and original ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text_files : List[Path]\n",
    "        List of text file paths to evaluate\n",
    "    get_predictions_func : callable\n",
    "        Function that takes (text_file_path, text) and returns predicted entities\n",
    "        Format: List[Dict] with 'label', 'start', 'end', 'text'\n",
    "    max_files : int, optional\n",
    "        Maximum number of files to evaluate (for testing on subset)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing:\n",
    "        - 'meddra_results': Aggregated metrics against MedDRA ground truth\n",
    "        - 'original_results': Aggregated metrics against original ground truth\n",
    "        - 'per_file_results': Per-file evaluation results\n",
    "        - 'files_evaluated': Number of files processed\n",
    "    \"\"\"\n",
    "    all_meddra_results = []\n",
    "    all_original_results = []\n",
    "    per_file_results = []\n",
    "    files_evaluated = 0\n",
    "    \n",
    "    if max_files:\n",
    "        text_files = text_files[:max_files]\n",
    "    \n",
    "    total_files = len(text_files)\n",
    "    print(f\"Evaluating {total_files} files...\")\n",
    "    \n",
    "    for idx, text_file in enumerate(text_files, 1):\n",
    "        try:\n",
    "            # Load text\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            \n",
    "            # Load MedDRA ground truth\n",
    "            meddra_file = MEDDRA_DIR / text_file.name.replace('.txt', '.ann')\n",
    "            meddra_gt = []\n",
    "            if meddra_file.exists():\n",
    "                meddra_gt = load_meddra_ground_truth(meddra_file)\n",
    "            \n",
    "            # Load original ground truth (ADR only)\n",
    "            original_file = ORIGINAL_DIR / text_file.name.replace('.txt', '.ann')\n",
    "            original_gt = []\n",
    "            if original_file.exists():\n",
    "                original_gt = load_original_ground_truth(original_file)\n",
    "            \n",
    "            # Get predictions and filter to ADR only\n",
    "            pred_entities = get_predictions_func(text_file, text)\n",
    "            pred_adr = filter_adr_entities(pred_entities)\n",
    "            \n",
    "            # Evaluate against MedDRA ground truth\n",
    "            meddra_result = evaluate_adr_entities(meddra_gt, pred_adr)\n",
    "            all_meddra_results.append(meddra_result)\n",
    "            \n",
    "            # Evaluate against original ground truth\n",
    "            original_result = evaluate_adr_entities(original_gt, pred_adr)\n",
    "            all_original_results.append(original_result)\n",
    "            \n",
    "            # Store per-file results\n",
    "            per_file_results.append({\n",
    "                'file': text_file.name,\n",
    "                'meddra': meddra_result,\n",
    "                'original': original_result,\n",
    "                'meddra_gt_count': len(meddra_gt),\n",
    "                'original_gt_count': len(original_gt),\n",
    "                'pred_count': len(pred_adr)\n",
    "            })\n",
    "            \n",
    "            files_evaluated += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"  Processed {idx}/{total_files} files...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error evaluating {text_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation complete: {files_evaluated} files evaluated\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    def aggregate(results_list):\n",
    "        total_tp = sum(r['tp'] for r in results_list)\n",
    "        total_fp = sum(r['fp'] for r in results_list)\n",
    "        total_fn = sum(r['fn'] for r in results_list)\n",
    "        \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'tp': total_tp,\n",
    "            'fp': total_fp,\n",
    "            'fn': total_fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'meddra_results': aggregate(all_meddra_results),\n",
    "        'original_results': aggregate(all_original_results),\n",
    "        'per_file_results': per_file_results,\n",
    "        'files_evaluated': files_evaluated\n",
    "    }\n",
    "\n",
    "print(\"✓ Complete evaluation pipeline defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing with Sample File\n",
    "\n",
    "Test the evaluation pipeline with a sample file to verify everything works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample file\n",
    "sample_text_files = list(TEXT_DIR.glob(\"*.txt\"))[:1]  # Get first file for testing\n",
    "\n",
    "if sample_text_files:\n",
    "    test_file = sample_text_files[0]\n",
    "    print(f\"Testing evaluation with file: {test_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load text\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    \n",
    "    # Load MedDRA ground truth\n",
    "    meddra_file = MEDDRA_DIR / test_file.name.replace('.txt', '.ann')\n",
    "    meddra_gt = load_meddra_ground_truth(meddra_file) if meddra_file.exists() else []\n",
    "    \n",
    "    # Load original ground truth (ADR only)\n",
    "    original_file = ORIGINAL_DIR / test_file.name.replace('.txt', '.ann')\n",
    "    original_gt = load_original_ground_truth(original_file) if original_file.exists() else []\n",
    "    \n",
    "    print(f\"\\nMedDRA Ground Truth: {len(meddra_gt)} ADR entities\")\n",
    "    for entity in meddra_gt[:5]:  # Show first 5\n",
    "        print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}] (MedDRA: {entity.get('meddra_code', 'N/A')})\")\n",
    "    \n",
    "    print(f\"\\nOriginal Ground Truth (ADR): {len(original_gt)} ADR entities\")\n",
    "    for entity in original_gt[:5]:  # Show first 5\n",
    "        print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
    "    \n",
    "    # For testing, create dummy predictions (in real scenario, these come from Task 2)\n",
    "    print(\"\\n⚠ Note: Using dummy predictions for demonstration.\")\n",
    "    print(\"   In actual evaluation, use predictions from Task 2 pipeline.\")\n",
    "    \n",
    "    # Create dummy predictions (subset of ground truth to simulate predictions)\n",
    "    pred_adr = meddra_gt[:len(meddra_gt)//2] if len(meddra_gt) > 1 else []\n",
    "    \n",
    "    if pred_adr:\n",
    "        print(f\"\\nDummy Predictions (ADR): {len(pred_adr)} entities\")\n",
    "        for entity in pred_adr[:5]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
    "    \n",
    "    # Evaluate against MedDRA\n",
    "    meddra_result = evaluate_adr_entities(meddra_gt, pred_adr)\n",
    "    \n",
    "    # Evaluate against original\n",
    "    original_result = evaluate_adr_entities(original_gt, pred_adr)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nAgainst MedDRA Ground Truth:\")\n",
    "    print(f\"  Precision: {meddra_result['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {meddra_result['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {meddra_result['f1']:.4f}\")\n",
    "    print(f\"  TP: {meddra_result['tp']}, FP: {meddra_result['fp']}, FN: {meddra_result['fn']}\")\n",
    "    \n",
    "    print(\"\\nAgainst Original Ground Truth:\")\n",
    "    print(f\"  Precision: {original_result['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {original_result['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {original_result['f1']:.4f}\")\n",
    "    print(f\"  TP: {original_result['tp']}, FP: {original_result['fp']}, FN: {original_result['fn']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No text files found for testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Helper Function\n",
    "\n",
    "Helper function to integrate with Task 2 predictions. This should be customized based on how Task 2 generates predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TASK 2 INTEGRATION: How to Use Task 2 Pipeline in Task 4 Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "# There are three ways to integrate Task 2 with Task 4:\n",
    "\n",
    "# METHOD 1: Run Task 2 in the same notebook session (RECOMMENDED)\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Run all cells from Task 2 notebook first (or copy necessary cells)\n",
    "# Step 2: Then use the functions and model pipeline from Task 2 here\n",
    "#\n",
    "# This requires:\n",
    "# - Running Task 2 cells to load the model (ner_pipeline, tokenizer)\n",
    "# - Having Task 2 functions available (process_text_file, etc.)\n",
    "\n",
    "def get_predictions_with_task2(text_file_path: Path, text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get predictions using Task 2 pipeline (METHOD 1: Same session).\n",
    "    \n",
    "    Prerequisites:\n",
    "    1. Run Task 2 notebook cells first to load model and functions\n",
    "    2. Ensure these variables are available:\n",
    "       - ner_pipeline (transformers pipeline)\n",
    "       - tokenizer (transformers tokenizer)\n",
    "       - process_text_file function from Task 2\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of entity dictionaries (all labels)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if Task 2 components are available\n",
    "        if 'ner_pipeline' not in globals() or 'tokenizer' not in globals():\n",
    "            raise NameError(\"Task 2 model not loaded. Please run Task 2 notebook first.\")\n",
    "        \n",
    "        if 'process_text_file' not in globals():\n",
    "            raise NameError(\"Task 2 functions not available. Please run Task 2 notebook first.\")\n",
    "        \n",
    "        # Use Task 2 pipeline to generate predictions\n",
    "        bio_tagged, annotation_lines = process_text_file(text_file_path, ner_pipeline, tokenizer)\n",
    "        \n",
    "        # Convert annotation lines to entity dictionaries\n",
    "        # annotation_lines are in format: \"T1\\tADR 9 19\\tbit drowsy\"\n",
    "        pred_entities = []\n",
    "        \n",
    "        for line in annotation_lines:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # Parse: TAG\\tLABEL START END\\tTEXT\n",
    "            match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
    "            if match:\n",
    "                tag = match.group(1)\n",
    "                label_and_ranges = match.group(2)\n",
    "                text_entity = match.group(3)\n",
    "                \n",
    "                parts = label_and_ranges.split(None, 1)\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                label_type = parts[0]\n",
    "                ranges_str = parts[1]\n",
    "                \n",
    "                # Parse ranges\n",
    "                ranges = []\n",
    "                if ';' in ranges_str:\n",
    "                    range_pairs = ranges_str.split(';')\n",
    "                    for rp in range_pairs:\n",
    "                        rp = rp.strip()\n",
    "                        if rp:\n",
    "                            range_nums = rp.split()\n",
    "                            if len(range_nums) >= 2:\n",
    "                                try:\n",
    "                                    start = int(range_nums[0])\n",
    "                                    end = int(range_nums[1])\n",
    "                                    ranges.append((start, end))\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                else:\n",
    "                    range_nums = ranges_str.split()\n",
    "                    if len(range_nums) >= 2:\n",
    "                        try:\n",
    "                            start = int(range_nums[0])\n",
    "                            end = int(range_nums[1])\n",
    "                            ranges = [(start, end)]\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                for start, end in ranges:\n",
    "                    pred_entities.append({\n",
    "                        'label': label_type,\n",
    "                        'text': text_entity.strip(),\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'tag': tag\n",
    "                    })\n",
    "        \n",
    "        return pred_entities\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"⚠ {e}\")\n",
    "        print(\"   Please run Task 2 notebook cells first, or use METHOD 2/3 below.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error generating predictions for {text_file_path.name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# METHOD 2: Load predictions from saved files\n",
    "# ----------------------------------------------------------------------------\n",
    "# If you saved Task 2 predictions to files, load them here\n",
    "\n",
    "def get_predictions_from_file(text_file_path: Path, text: str, predictions_dir: Path = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get predictions from saved annotation files (METHOD 2: Load from files).\n",
    "    \n",
    "    Prerequisites:\n",
    "    1. Task 2 should have saved predictions to annotation files\n",
    "    2. Files should be in format: predictions/<filename>.ann\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text_file_path : Path\n",
    "        Path to the text file\n",
    "    text : str\n",
    "        Text content (not used, but kept for compatibility)\n",
    "    predictions_dir : Path, optional\n",
    "        Directory where predictions are saved (default: Path(\"predictions\"))\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of entity dictionaries (all labels)\n",
    "    \"\"\"\n",
    "    if predictions_dir is None:\n",
    "        predictions_dir = Path(\"predictions\")\n",
    "    \n",
    "    # Find corresponding prediction file\n",
    "    pred_file = predictions_dir / text_file_path.name.replace('.txt', '.ann')\n",
    "    \n",
    "    if not pred_file.exists():\n",
    "        print(f\"⚠ Prediction file not found: {pred_file}\")\n",
    "        return []\n",
    "    \n",
    "    # Load predictions using the load_predictions function\n",
    "    return load_predictions(pred_file)\n",
    "\n",
    "\n",
    "# METHOD 3: Load Task 2 model and functions dynamically\n",
    "# ----------------------------------------------------------------------------\n",
    "# If Task 2 code is in a separate module or can be imported\n",
    "\n",
    "def get_predictions_dynamic_import(text_file_path: Path, text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get predictions by dynamically loading Task 2 components (METHOD 3: Import).\n",
    "    \n",
    "    This method attempts to load Task 2 model and functions from a saved state\n",
    "    or by re-executing key cells. This is more complex and may require:\n",
    "    - Saving model state\n",
    "    - Creating a module from Task 2 functions\n",
    "    - Or using importlib to execute Task 2 cells\n",
    "    \n",
    "    Note: This is advanced and may not work in all environments.\n",
    "    For most cases, use METHOD 1 (same session) or METHOD 2 (saved files).\n",
    "    \"\"\"\n",
    "    # This is a placeholder - implementation depends on your setup\n",
    "    # You might need to:\n",
    "    # 1. Save model after Task 2: torch.save(model.state_dict(), 'model.pt')\n",
    "    # 2. Load model in Task 4: model.load_state_dict(torch.load('model.pt'))\n",
    "    # 3. Recreate pipeline and functions\n",
    "    \n",
    "    print(\"⚠ Dynamic import not implemented. Use METHOD 1 or METHOD 2.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE INSTRUCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 2 INTEGRATION GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nChoose one of the following methods:\\n\")\n",
    "print(\"METHOD 1 (Recommended): Run Task 2 in same notebook session\")\n",
    "print(\"  → Run all Task 2 cells first to load model and functions\")\n",
    "print(\"  → Then use: get_predictions_func = get_predictions_with_task2\")\n",
    "print(\"\\nMETHOD 2: Load predictions from saved files\")\n",
    "print(\"  → Save Task 2 predictions to files first\")\n",
    "print(\"  → Then use: get_predictions_func = get_predictions_from_file\")\n",
    "print(\"\\nMETHOD 3: Dynamic import (advanced)\")\n",
    "print(\"  → Requires custom setup to import Task 2 components\")\n",
    "print(\"  → Use only if you have a specific setup for this\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nExample usage after choosing a method:\")\n",
    "print(\"  results = evaluate_all_files(\")\n",
    "print(\"      text_files=text_files,\")\n",
    "print(\"      get_predictions_func=get_predictions_with_task2,  # or method 2/3\")\n",
    "print(\"      max_files=100\")\n",
    "print(\"  )\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE EXAMPLE: Full Evaluation with Task 2 Integration\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment and modify this section when ready to run full evaluation\n",
    "\n",
    "\"\"\"\n",
    "# STEP 1: Choose your integration method\n",
    "# Option A: Same session (requires Task 2 to be run first)\n",
    "get_predictions_func = get_predictions_with_task2\n",
    "\n",
    "# Option B: Load from files (requires predictions to be saved first)\n",
    "# get_predictions_func = lambda path, text: get_predictions_from_file(path, text, Path(\"predictions\"))\n",
    "\n",
    "# STEP 2: Get all text files\n",
    "text_files = list(TEXT_DIR.glob(\"*.txt\"))\n",
    "print(f\"Found {len(text_files)} text files\")\n",
    "\n",
    "# STEP 3: Run evaluation\n",
    "# For testing, start with a small subset\n",
    "print(\"\\nRunning evaluation on first 10 files (for testing)...\")\n",
    "print(\"Remove max_files parameter to evaluate all files\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = evaluate_all_files(\n",
    "    text_files=text_files,\n",
    "    get_predictions_func=get_predictions_func,\n",
    "    max_files=10  # Remove this line to evaluate all files\n",
    ")\n",
    "\n",
    "# STEP 4: Display results\n",
    "print(\"\\n\")\n",
    "display_comprehensive_results(results)\n",
    "\n",
    "# STEP 5: Optional: Save results to file\n",
    "import json\n",
    "results_file = Path(\"task4_evaluation_results.json\")\n",
    "with open(results_file, 'w') as f:\n",
    "    # Convert to JSON-serializable format\n",
    "    results_json = {\n",
    "        'meddra_results': results['meddra_results'],\n",
    "        'original_results': results['original_results'],\n",
    "        'files_evaluated': results['files_evaluated']\n",
    "    }\n",
    "    json.dump(results_json, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to {results_file}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Complete example code provided above\")\n",
    "print(\"  → Uncomment the code block to run full evaluation\")\n",
    "print(\"  → Make sure Task 2 is integrated first (see METHOD 1 or METHOD 2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Comprehensive Results\n",
    "\n",
    "Display and compare results between MedDRA and original ground truth evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comprehensive_results(results: Dict):\n",
    "    \"\"\"\n",
    "    Display comprehensive evaluation results comparing MedDRA vs Original ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : Dict\n",
    "        Results from evaluate_all_files()\n",
    "    \"\"\"\n",
    "    meddra = results['meddra_results']\n",
    "    original = results['original_results']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADR-SPECIFIC EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFiles Evaluated: {results['files_evaluated']}\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = {\n",
    "        'Ground Truth': ['MedDRA', 'Original (Task 3)'],\n",
    "        'Precision': [meddra['precision'], original['precision']],\n",
    "        'Recall': [meddra['recall'], original['recall']],\n",
    "        'F1-Score': [meddra['f1'], original['f1']],\n",
    "        'True Positives': [meddra['tp'], original['tp']],\n",
    "        'False Positives': [meddra['fp'], original['fp']],\n",
    "        'False Negatives': [meddra['fn'], original['fn']]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON: MedDRA vs Original Ground Truth\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Calculate differences\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PERFORMANCE DIFFERENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    precision_diff = meddra['precision'] - original['precision']\n",
    "    recall_diff = meddra['recall'] - original['recall']\n",
    "    f1_diff = meddra['f1'] - original['f1']\n",
    "    \n",
    "    print(f\"\\nPrecision Difference (MedDRA - Original): {precision_diff:+.4f}\")\n",
    "    print(f\"Recall Difference (MedDRA - Original):     {recall_diff:+.4f}\")\n",
    "    print(f\"F1-Score Difference (MedDRA - Original):     {f1_diff:+.4f}\")\n",
    "    \n",
    "    # Ground truth count analysis\n",
    "    meddra_gt_total = sum(r['meddra_gt_count'] for r in results['per_file_results'])\n",
    "    original_gt_total = sum(r['original_gt_count'] for r in results['per_file_results'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GROUND TRUTH ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal MedDRA ADR Entities:    {meddra_gt_total}\")\n",
    "    print(f\"Total Original ADR Entities:    {original_gt_total}\")\n",
    "    print(f\"Difference:                     {meddra_gt_total - original_gt_total:+.0f}\")\n",
    "    \n",
    "    if meddra_gt_total != original_gt_total:\n",
    "        print(f\"\\n⚠ Note: MedDRA and Original ground truth have different entity counts.\")\n",
    "        print(f\"   This may indicate annotation differences or standardization effects.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"✓ Results display function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis of Performance Differences\n",
    "\n",
    "### Understanding Differences Between Original and MedDRA Ground Truth\n",
    "\n",
    "Several factors can contribute to performance differences between evaluations using original vs MedDRA annotations:\n",
    "\n",
    "#### 1. **Annotation Standardization**\n",
    "- **MedDRA Standardization**: MedDRA annotations use standardized medical terminology with numeric codes, which may:\n",
    "  - Consolidate synonymous terms into single codes\n",
    "  - Normalize variations (e.g., \"drowsiness\" vs \"drowsy\" may map to same code)\n",
    "  - Use more specific clinical terminology\n",
    "- **Original Annotations**: May contain more natural language variations and informal expressions\n",
    "\n",
    "#### 2. **Entity Boundary Differences**\n",
    "- MedDRA annotations may have slightly different character boundaries due to standardization\n",
    "- Original annotations might include/exclude surrounding words differently\n",
    "\n",
    "#### 3. **Entity Count Differences**\n",
    "- MedDRA may consolidate multiple mentions into single standardized entities\n",
    "- Original annotations may preserve all individual mentions\n",
    "\n",
    "#### 4. **Label Consistency**\n",
    "- MedDRA ensures all ADR entities follow standardized coding\n",
    "- Original annotations may have more variability in labeling consistency\n",
    "\n",
    "### Expected Scenarios:\n",
    "\n",
    "1. **MedDRA Performance Higher**: If the model better matches standardized terminology\n",
    "2. **Original Performance Higher**: If the model captures natural language variations better\n",
    "3. **Similar Performance**: If both ground truth sets are well-aligned\n",
    "\n",
    "### Clinical Significance of ADR Detection Accuracy\n",
    "\n",
    "**High ADR detection accuracy is clinically critical** for several reasons:\n",
    "\n",
    "1. **Patient Safety**:\n",
    "   - **False Negatives (Missed ADRs)**: Can lead to:\n",
    "     - Continued use of harmful medications\n",
    "     - Delayed medical intervention\n",
    "     - Severe adverse events going unreported\n",
    "   - **False Positives (Incorrect ADRs)**: Can lead to:\n",
    "     - Unnecessary medication changes\n",
    "     - Patient anxiety\n",
    "     - Over-reporting that dilutes signal detection\n",
    "\n",
    "2. **Regulatory Reporting**:\n",
    "   - Inaccurate ADR detection affects post-marketing surveillance data\n",
    "   - Regulatory bodies (FDA, EMA) require accurate ADR reporting\n",
    "   - MedDRA coding ensures standardized reporting across systems\n",
    "\n",
    "3. **Clinical Decision Support**:\n",
    "   - Electronic health records use ADR information for alerts\n",
    "   - Incorrect ADR detection can generate false alerts (alert fatigue)\n",
    "   - Missing ADRs can miss critical drug-safety information\n",
    "\n",
    "4. **Pharmacovigilance Signal Detection**:\n",
    "   - Automated ADR extraction enables early detection of safety signals\n",
    "   - Low recall can miss emerging safety concerns\n",
    "   - Low precision can create noise that obscures real signals\n",
    "\n",
    "5. **Research and Knowledge Discovery**:\n",
    "   - ADR patterns help identify drug-drug interactions\n",
    "   - Population-specific risks (age, gender, comorbidities)\n",
    "   - Dose-response relationships\n",
    "\n",
    "### Recommended Performance Targets:\n",
    "\n",
    "- **Precision**: > 0.80 (minimize false positives for regulatory reporting)\n",
    "- **Recall**: > 0.85 (minimize false negatives for patient safety)\n",
    "- **F1-Score**: > 0.82 (balanced performance)\n",
    "\n",
    "### MedDRA vs Original Evaluation Benefits:\n",
    "\n",
    "1. **MedDRA Evaluation**:\n",
    "   - Tests model performance on standardized medical terminology\n",
    "   - Aligns with real-world pharmacovigilance workflows\n",
    "   - Enables direct integration with regulatory databases\n",
    "\n",
    "2. **Original Evaluation**:\n",
    "   - Tests model performance on natural language variations\n",
    "   - Reflects patient forum and social media contexts\n",
    "   - Captures informal and colloquial expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "This notebook provides a comprehensive ADR-specific evaluation framework with:\n",
    "\n",
    "### ✅ Features Implemented:\n",
    "\n",
    "1. **MedDRA Format Parsing**: Correctly parses TT prefix + MedDRA code + character ranges format\n",
    "2. **ADR Entity Filtering**: Filters predictions to only ADR labels\n",
    "3. **Exact Span Matching**: Entity-level evaluation with exact boundary matching\n",
    "4. **Dual Ground Truth Evaluation**: Compares performance against both MedDRA and original annotations\n",
    "5. **Comprehensive Metrics**: Precision, Recall, F1-Score with TP/FP/FN breakdowns\n",
    "6. **Performance Comparison**: Analyzes differences between MedDRA and original evaluations\n",
    "7. **Clinical Context**: Explains importance of ADR detection in pharmacovigilance\n",
    "\n",
    "### 📊 Key Insights:\n",
    "\n",
    "- **MedDRA Ground Truth**: Standardized medical terminology with codes\n",
    "- **Original Ground Truth**: Natural language variations\n",
    "- **Performance Differences**: Reflect annotation standardization and terminology alignment\n",
    "- **Clinical Significance**: ADR detection accuracy directly impacts patient safety\n",
    "\n",
    "### 🔧 Usage:\n",
    "\n",
    "1. Integrate with Task 2 to generate predictions\n",
    "2. Run evaluation on single file or full dataset\n",
    "3. Compare MedDRA vs Original performance\n",
    "4. Analyze differences and clinical implications\n",
    "\n",
    "### 📝 Notes:\n",
    "\n",
    "- Predictions must be generated using Task 2 pipeline or loaded from files\n",
    "- Evaluation uses exact boundary matching (consistent with Task 3 methodology)\n",
    "- MedDRA annotations contain only ADR entities (by design)\n",
    "- Original annotations may have slight boundary/text differences from MedDRA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
