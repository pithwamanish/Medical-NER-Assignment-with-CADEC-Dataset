{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ADR-Specific Evaluation with MedDRA Annotations\n",
    "\n",
    "**Title:** \"ADR-Focused Performance Analysis with MedDRA Ground Truth\"\n",
    "\n",
    "## Objective\n",
    "Extend Task 3 evaluation to focus specifically on ADR entities using MedDRA annotations:\n",
    "- Load ground truth from 'meddra' subdirectory (contains only ADR entities with MedDRA codes)\n",
    "- Parse MedDRA format (TT prefix + MedDRA code + character ranges + entity text)\n",
    "- Filter predicted entities to only ADR labels\n",
    "- Match predicted ADR entities against MedDRA ground truth with exact span matching\n",
    "- Calculate ADR-specific Precision, Recall, F1\n",
    "- Compare with ADR performance from Task 3 (original annotations)\n",
    "- Analyze differences between original and MedDRA ground truth\n",
    "\n",
    "## Overview\n",
    "This notebook extends Task 3 evaluation framework to focus specifically on Adverse Drug Reaction (ADR) entities using MedDRA-standardized annotations as ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding MedDRA Annotation Format\n",
    "\n",
    "### MedDRA Format Explanation\n",
    "\n",
    "The MedDRA (Medical Dictionary for Regulatory Activities) annotation format contains **only ADR entities** with standardized medical codes:\n",
    "\n",
    "**Format Structure:**\n",
    "```\n",
    "TT<original_tag>\\t<MedDRA_code> <start> <end>\\t<entity_text>\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "TT1\\t10028836 9 18\\tneck pain\n",
    "TT2\\t10001949 20 31\\tmemory loss\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Identifier**: `TT1`, `TT2`, etc. (TT prefix + original tag from 'original' directory)\n",
    "2. **MedDRA Code**: Numeric code (e.g., `10028836`) - standardized medical term identifier\n",
    "3. **Character Ranges**: Start and end positions (e.g., `9 18`)\n",
    "4. **Entity Text**: The actual ADR mention in the text (e.g., `neck pain`)\n",
    "\n",
    "### Why ADR Detection is Critical in Pharmacovigilance\n",
    "\n",
    "**Adverse Drug Reaction (ADR) detection is particularly important in pharmacovigilance** for several reasons:\n",
    "\n",
    "1. **Patient Safety**: ADRs can range from mild discomfort to life-threatening conditions. Accurate detection enables timely medical intervention.\n",
    "\n",
    "2. **Regulatory Compliance**: Pharmaceutical companies must report ADRs to regulatory bodies (FDA, EMA). Standardized MedDRA coding ensures consistent reporting.\n",
    "\n",
    "3. **Signal Detection**: Automated ADR detection from patient forums, social media, and clinical notes helps identify potential safety signals early.\n",
    "\n",
    "4. **Drug Monitoring**: Post-marketing surveillance relies on accurate ADR extraction to monitor drug safety in real-world populations.\n",
    "\n",
    "5. **Knowledge Discovery**: ADR patterns can reveal drug-drug interactions, contraindications, and population-specific risks.\n",
    "\n",
    "6. **Clinical Decision Support**: Healthcare systems use ADR information to alert clinicians about potential adverse events.\n",
    "\n",
    "7. **Standardization**: MedDRA provides a hierarchical taxonomy (SOC ‚Üí HLGT ‚Üí HLT ‚Üí PT ‚Üí LLT) enabling structured analysis across different data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Install seqeval if not already installed\n",
    "try:\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"‚ö† seqeval not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seqeval\"])\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score\n",
    "    )\n",
    "    print(\"‚úì seqeval installed successfully\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(\"cadec\")\n",
    "TEXT_DIR = BASE_DIR / \"text\"\n",
    "ORIGINAL_DIR = BASE_DIR / \"original\"\n",
    "MEDDRA_DIR = BASE_DIR / \"meddra\"\n",
    "\n",
    "# Verify directories exist\n",
    "if not TEXT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {TEXT_DIR}\")\n",
    "if not ORIGINAL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {ORIGINAL_DIR}\")\n",
    "if not MEDDRA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {MEDDRA_DIR}\")\n",
    "\n",
    "print(\"‚úì Directories verified\")\n",
    "print(f\"  - Text directory: {TEXT_DIR}\")\n",
    "print(f\"  - Original directory: {ORIGINAL_DIR}\")\n",
    "print(f\"  - MedDRA directory: {MEDDRA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse MedDRA Annotation Format\n",
    "\n",
    "MedDRA annotations use a specific format with TT prefix, MedDRA code, and character ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meddra_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse MedDRA ground truth annotation file.\n",
    "    \n",
    "    MedDRA Format: TT<tag>\\t<MedDRA_code> <start> <end>\\t<text>\n",
    "    Example: TT1\\t10028836 9 18\\tneck pain\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the .ann annotation file in meddra directory\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entity dictionaries with:\n",
    "        - 'label': Always 'ADR' (MedDRA contains only ADR entities)\n",
    "        - 'text': Entity text\n",
    "        - 'start': Start character position\n",
    "        - 'end': End character position\n",
    "        - 'tag': Original tag identifier (TT1, TT2, etc.)\n",
    "        - 'meddra_code': MedDRA standardized code (numeric string)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines and comment lines (starting with '#')\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse MedDRA format: TT<tag>\\t<MedDRA_code> <start> <end>\\t<text>\n",
    "                # Split by tab first\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                identifier = parts[0]  # TT1, TT2, etc.\n",
    "                \n",
    "                # Parse the second part: <MedDRA_code> <start> <end> [optional additional text]\n",
    "                metadata_and_text = parts[1].split(None, 3)  # Split into max 4 parts\n",
    "                \n",
    "                if len(metadata_and_text) < 3:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    meddra_code = metadata_and_text[0]\n",
    "                    start = int(metadata_and_text[1])\n",
    "                    end = int(metadata_and_text[2])\n",
    "                    \n",
    "                    # Entity text is the remaining part (if any), or from third tab-separated part\n",
    "                    if len(parts) >= 3:\n",
    "                        text = parts[2]\n",
    "                    elif len(metadata_and_text) >= 4:\n",
    "                        text = metadata_and_text[3]\n",
    "                    else:\n",
    "                        # If no text provided, we'll need to extract from original text\n",
    "                        text = \"\"\n",
    "                    \n",
    "                    entities.append({\n",
    "                        'label': 'ADR',  # MedDRA only contains ADR entities\n",
    "                        'text': text.strip(),\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'tag': identifier,\n",
    "                        'meddra_code': meddra_code\n",
    "                    })\n",
    "                    \n",
    "                except (ValueError, IndexError) as e:\n",
    "                    print(f\"‚ö† Error parsing line in {ann_file_path.name}: {line}\")\n",
    "                    print(f\"   Error: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MedDRA ground truth from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"‚úì MedDRA ground truth loading function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Original Annotations (for Comparison with Task 3)\n",
    "\n",
    "We'll also load original annotations to compare ADR performance between original and MedDRA ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_ground_truth(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse ground truth annotation file from 'original' subdirectory.\n",
    "    This is the same function from Task 3, used for comparison.\n",
    "    \n",
    "    Format: TAG\\tLABEL START END\\tTEXT\n",
    "    Example: T1\\tADR 9 19\\tbit drowsy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the .ann annotation file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entity dictionaries with:\n",
    "        - 'label': Entity type (ADR)\n",
    "        - 'text': Entity text\n",
    "        - 'start': Start character position\n",
    "        - 'end': End character position\n",
    "        - 'tag': Original tag identifier (T1, T2, etc.)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines and comment lines (starting with '#')\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse entity annotation lines (starting with 'T' followed by a number)\n",
    "                # Format: TAG\\tLABEL RANGES\\tTEXT\n",
    "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
    "                if match:\n",
    "                    tag = match.group(1)\n",
    "                    label_and_ranges = match.group(2)\n",
    "                    text = match.group(3)\n",
    "                    \n",
    "                    # Extract label type (first word) and ranges (remaining part)\n",
    "                    parts = label_and_ranges.split(None, 1)\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    label_type = parts[0]\n",
    "                    ranges_str = parts[1]\n",
    "                    \n",
    "                    # Only process ADR labels for this task\n",
    "                    if label_type != 'ADR':\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract ranges (can be multiple pairs separated by semicolons)\n",
    "                    ranges = []\n",
    "                    if ';' in ranges_str:\n",
    "                        # Multiple ranges format: \"START1 END1;START2 END2;...\"\n",
    "                        range_pairs = ranges_str.split(';')\n",
    "                        for rp in range_pairs:\n",
    "                            rp = rp.strip()\n",
    "                            if rp:\n",
    "                                range_nums = rp.split()\n",
    "                                if len(range_nums) >= 2:\n",
    "                                    try:\n",
    "                                        start = int(range_nums[0])\n",
    "                                        end = int(range_nums[1])\n",
    "                                        ranges.append((start, end))\n",
    "                                    except ValueError:\n",
    "                                        continue\n",
    "                    else:\n",
    "                        # Single range format: \"START END\"\n",
    "                        range_nums = ranges_str.split()\n",
    "                        if len(range_nums) >= 2:\n",
    "                            try:\n",
    "                                start = int(range_nums[0])\n",
    "                                end = int(range_nums[1])\n",
    "                                ranges = [(start, end)]\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    # Create entity entries for each range\n",
    "                    for start, end in ranges:\n",
    "                        entities.append({\n",
    "                            'label': label_type,\n",
    "                            'text': text.strip(),\n",
    "                            'start': start,\n",
    "                            'end': end,\n",
    "                            'tag': tag\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original ground truth from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"‚úì Original ground truth loading function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(ann_file_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and parse predicted annotation file (same format as original annotations).\n",
    "    \n",
    "    Format: TAG\\tLABEL START END\\tTEXT\n",
    "    This should match the output format from Task 2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ann_file_path : Path\n",
    "        Path to the predicted .ann file (or can be a list of annotation lines)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of entity dictionaries (all labels)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    \n",
    "    try:\n",
    "        with open(ann_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse: TAG\\tLABEL START END\\tTEXT\n",
    "                match = re.match(r'^(T\\d+)\\t([^\\t]+)\\t(.+)$', line)\n",
    "                if match:\n",
    "                    tag = match.group(1)\n",
    "                    label_and_ranges = match.group(2)\n",
    "                    text = match.group(3)\n",
    "                    \n",
    "                    parts = label_and_ranges.split(None, 1)\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    label_type = parts[0]\n",
    "                    ranges_str = parts[1]\n",
    "                    \n",
    "                    # Parse ranges\n",
    "                    ranges = []\n",
    "                    if ';' in ranges_str:\n",
    "                        range_pairs = ranges_str.split(';')\n",
    "                        for rp in range_pairs:\n",
    "                            rp = rp.strip()\n",
    "                            if rp:\n",
    "                                range_nums = rp.split()\n",
    "                                if len(range_nums) >= 2:\n",
    "                                    try:\n",
    "                                        start = int(range_nums[0])\n",
    "                                        end = int(range_nums[1])\n",
    "                                        ranges.append((start, end))\n",
    "                                    except ValueError:\n",
    "                                        continue\n",
    "                    else:\n",
    "                        range_nums = ranges_str.split()\n",
    "                        if len(range_nums) >= 2:\n",
    "                            try:\n",
    "                                start = int(range_nums[0])\n",
    "                                end = int(range_nums[1])\n",
    "                                ranges = [(start, end)]\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    \n",
    "                    for start, end in ranges:\n",
    "                        entities.append({\n",
    "                            'label': label_type,\n",
    "                            'text': text.strip(),\n",
    "                            'start': start,\n",
    "                            'end': end,\n",
    "                            'tag': tag\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading predictions from {ann_file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def filter_adr_entities(entities: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter entities to only include ADR labels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    entities : List[Dict]\n",
    "        List of all predicted entities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]\n",
    "        List of ADR entities only\n",
    "    \"\"\"\n",
    "    return [entity for entity in entities if entity['label'] == 'ADR']\n",
    "\n",
    "print(\"‚úì Prediction loading and filtering functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ADR-Specific Evaluation with Exact Span Matching\n",
    "\n",
    "Evaluate ADR entities using exact span matching (same approach as Task 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adr_entities(ground_truth: List[Dict], predictions: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform entity-level evaluation for ADR entities (exact boundary + label matching).\n",
    "    \n",
    "    Entity-level evaluation requires:\n",
    "    - Exact match of entity boundaries (start AND end positions)\n",
    "    - Exact match of label type (ADR)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ground_truth : List[Dict]\n",
    "        List of ground truth ADR entities\n",
    "    predictions : List[Dict]\n",
    "        List of predicted ADR entities\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing:\n",
    "        - 'tp', 'fp', 'fn' counts\n",
    "        - 'precision', 'recall', 'f1' scores\n",
    "    \"\"\"\n",
    "    # Convert entities to sets of tuples for exact matching\n",
    "    # Format: (label, start, end) - exact boundary matching required\n",
    "    gt_set = set()\n",
    "    for entity in ground_truth:\n",
    "        gt_set.add((entity['label'], entity['start'], entity['end']))\n",
    "    \n",
    "    pred_set = set()\n",
    "    for entity in predictions:\n",
    "        pred_set.add((entity['label'], entity['start'], entity['end']))\n",
    "    \n",
    "    # Calculate True Positives: entities that appear in both sets\n",
    "    tp = len(gt_set.intersection(pred_set))\n",
    "    \n",
    "    # Calculate False Positives: predicted entities not in ground truth\n",
    "    fp = len(pred_set - gt_set)\n",
    "    \n",
    "    # Calculate False Negatives: ground truth entities not predicted\n",
    "    fn = len(gt_set - pred_set)\n",
    "    \n",
    "    # Calculate Precision, Recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"‚úì ADR evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Evaluation Pipeline\n",
    "\n",
    "Evaluate all files and aggregate results for both MedDRA and original ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_files(text_files: List[Path], \n",
    "                       get_predictions_func,\n",
    "                       max_files: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate all text files in the dataset against both MedDRA and original ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text_files : List[Path]\n",
    "        List of text file paths to evaluate\n",
    "    get_predictions_func : callable\n",
    "        Function that takes (text_file_path, text) and returns predicted entities\n",
    "        Format: List[Dict] with 'label', 'start', 'end', 'text'\n",
    "    max_files : int, optional\n",
    "        Maximum number of files to evaluate (for testing on subset)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary containing:\n",
    "        - 'meddra_results': Aggregated metrics against MedDRA ground truth\n",
    "        - 'original_results': Aggregated metrics against original ground truth\n",
    "        - 'per_file_results': Per-file evaluation results\n",
    "        - 'files_evaluated': Number of files processed\n",
    "    \"\"\"\n",
    "    all_meddra_results = []\n",
    "    all_original_results = []\n",
    "    per_file_results = []\n",
    "    files_evaluated = 0\n",
    "    \n",
    "    if max_files:\n",
    "        text_files = text_files[:max_files]\n",
    "    \n",
    "    total_files = len(text_files)\n",
    "    print(f\"Evaluating {total_files} files...\")\n",
    "    \n",
    "    for idx, text_file in enumerate(text_files, 1):\n",
    "        try:\n",
    "            # Load text\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "            \n",
    "            # Load MedDRA ground truth\n",
    "            meddra_file = MEDDRA_DIR / text_file.name.replace('.txt', '.ann')\n",
    "            meddra_gt = []\n",
    "            if meddra_file.exists():\n",
    "                meddra_gt = load_meddra_ground_truth(meddra_file)\n",
    "            \n",
    "            # Load original ground truth (ADR only)\n",
    "            original_file = ORIGINAL_DIR / text_file.name.replace('.txt', '.ann')\n",
    "            original_gt = []\n",
    "            if original_file.exists():\n",
    "                original_gt = load_original_ground_truth(original_file)\n",
    "            \n",
    "            # Get predictions and filter to ADR only\n",
    "            pred_entities = get_predictions_func(text_file, text)\n",
    "            pred_adr = filter_adr_entities(pred_entities)\n",
    "            \n",
    "            # Evaluate against MedDRA ground truth\n",
    "            meddra_result = evaluate_adr_entities(meddra_gt, pred_adr)\n",
    "            all_meddra_results.append(meddra_result)\n",
    "            \n",
    "            # Evaluate against original ground truth\n",
    "            original_result = evaluate_adr_entities(original_gt, pred_adr)\n",
    "            all_original_results.append(original_result)\n",
    "            \n",
    "            # Store per-file results\n",
    "            per_file_results.append({\n",
    "                'file': text_file.name,\n",
    "                'meddra': meddra_result,\n",
    "                'original': original_result,\n",
    "                'meddra_gt_count': len(meddra_gt),\n",
    "                'original_gt_count': len(original_gt),\n",
    "                'pred_count': len(pred_adr)\n",
    "            })\n",
    "            \n",
    "            files_evaluated += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"  Processed {idx}/{total_files} files...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error evaluating {text_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úì Evaluation complete: {files_evaluated} files evaluated\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    def aggregate(results_list):\n",
    "        total_tp = sum(r['tp'] for r in results_list)\n",
    "        total_fp = sum(r['fp'] for r in results_list)\n",
    "        total_fn = sum(r['fn'] for r in results_list)\n",
    "        \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'tp': total_tp,\n",
    "            'fp': total_fp,\n",
    "            'fn': total_fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'meddra_results': aggregate(all_meddra_results),\n",
    "        'original_results': aggregate(all_original_results),\n",
    "        'per_file_results': per_file_results,\n",
    "        'files_evaluated': files_evaluated\n",
    "    }\n",
    "\n",
    "print(\"‚úì Complete evaluation pipeline defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing with Sample File\n",
    "\n",
    "Test the evaluation pipeline with a sample file to verify everything works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample file\n",
    "sample_text_files = list(TEXT_DIR.glob(\"*.txt\"))[:1]  # Get first file for testing\n",
    "\n",
    "if sample_text_files:\n",
    "    test_file = sample_text_files[0]\n",
    "    print(f\"Testing evaluation with file: {test_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load text\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().strip()\n",
    "    \n",
    "    # Load MedDRA ground truth\n",
    "    meddra_file = MEDDRA_DIR / test_file.name.replace('.txt', '.ann')\n",
    "    meddra_gt = load_meddra_ground_truth(meddra_file) if meddra_file.exists() else []\n",
    "    \n",
    "    # Load original ground truth (ADR only)\n",
    "    original_file = ORIGINAL_DIR / test_file.name.replace('.txt', '.ann')\n",
    "    original_gt = load_original_ground_truth(original_file) if original_file.exists() else []\n",
    "    \n",
    "    print(f\"\\nMedDRA Ground Truth: {len(meddra_gt)} ADR entities\")\n",
    "    for entity in meddra_gt[:5]:  # Show first 5\n",
    "        print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}] (MedDRA: {entity.get('meddra_code', 'N/A')})\")\n",
    "    \n",
    "    print(f\"\\nOriginal Ground Truth (ADR): {len(original_gt)} ADR entities\")\n",
    "    for entity in original_gt[:5]:  # Show first 5\n",
    "        print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
    "    \n",
    "    # For testing, create dummy predictions (in real scenario, these come from Task 2)\n",
    "    print(\"\\n‚ö† Note: Using dummy predictions for demonstration.\")\n",
    "    print(\"   In actual evaluation, use predictions from Task 2 pipeline.\")\n",
    "    \n",
    "    # Create dummy predictions (subset of ground truth to simulate predictions)\n",
    "    pred_adr = meddra_gt[:len(meddra_gt)//2] if len(meddra_gt) > 1 else []\n",
    "    \n",
    "    if pred_adr:\n",
    "        print(f\"\\nDummy Predictions (ADR): {len(pred_adr)} entities\")\n",
    "        for entity in pred_adr[:5]:\n",
    "            print(f\"  - {entity['label']}: '{entity['text']}' [{entity['start']}:{entity['end']}]\")\n",
    "    \n",
    "    # Evaluate against MedDRA\n",
    "    meddra_result = evaluate_adr_entities(meddra_gt, pred_adr)\n",
    "    \n",
    "    # Evaluate against original\n",
    "    original_result = evaluate_adr_entities(original_gt, pred_adr)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nAgainst MedDRA Ground Truth:\")\n",
    "    print(f\"  Precision: {meddra_result['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {meddra_result['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {meddra_result['f1']:.4f}\")\n",
    "    print(f\"  TP: {meddra_result['tp']}, FP: {meddra_result['fp']}, FN: {meddra_result['fn']}\")\n",
    "    \n",
    "    print(\"\\nAgainst Original Ground Truth:\")\n",
    "    print(f\"  Precision: {original_result['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {original_result['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {original_result['f1']:.4f}\")\n",
    "    print(f\"  TP: {original_result['tp']}, FP: {original_result['fp']}, FN: {original_result['fn']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† No text files found for testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Helper Function\n",
    "\n",
    "Helper function to integrate with Task 2 predictions. This should be customized based on how Task 2 generates predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_get_predictions(text_file_path: Path, text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Example function to get predictions for a text file.\n",
    "    \n",
    "    In practice, this would:\n",
    "    1. Call Task 2's process_text_file() function\n",
    "    2. Convert annotation lines to entity dictionaries\n",
    "    3. Return list of entities (all labels)\n",
    "    \n",
    "    For now, this is a placeholder that returns empty list.\n",
    "    Replace this with actual Task 2 integration.\n",
    "    \"\"\"\n",
    "    # Placeholder: return empty predictions\n",
    "    # In real implementation, integrate with Task 2:\n",
    "    #\n",
    "    # from task2_functions import process_text_file  # or however Task 2 is structured\n",
    "    # bio_tagged, annotation_lines = process_text_file(text_file_path, ner_pipeline, tokenizer)\n",
    "    # pred_entities = load_predictions(annotation_lines)  # Parse annotation lines\n",
    "    # return pred_entities\n",
    "    \n",
    "    return []\n",
    "\n",
    "print(\"‚úì Integration helper function defined\")\n",
    "print(\"  ‚Üí Customize example_get_predictions() to integrate with Task 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Comprehensive Results\n",
    "\n",
    "Display and compare results between MedDRA and original ground truth evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comprehensive_results(results: Dict):\n",
    "    \"\"\"\n",
    "    Display comprehensive evaluation results comparing MedDRA vs Original ground truth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : Dict\n",
    "        Results from evaluate_all_files()\n",
    "    \"\"\"\n",
    "    meddra = results['meddra_results']\n",
    "    original = results['original_results']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADR-SPECIFIC EVALUATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFiles Evaluated: {results['files_evaluated']}\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = {\n",
    "        'Ground Truth': ['MedDRA', 'Original (Task 3)'],\n",
    "        'Precision': [meddra['precision'], original['precision']],\n",
    "        'Recall': [meddra['recall'], original['recall']],\n",
    "        'F1-Score': [meddra['f1'], original['f1']],\n",
    "        'True Positives': [meddra['tp'], original['tp']],\n",
    "        'False Positives': [meddra['fp'], original['fp']],\n",
    "        'False Negatives': [meddra['fn'], original['fn']]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON: MedDRA vs Original Ground Truth\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Calculate differences\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PERFORMANCE DIFFERENCES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    precision_diff = meddra['precision'] - original['precision']\n",
    "    recall_diff = meddra['recall'] - original['recall']\n",
    "    f1_diff = meddra['f1'] - original['f1']\n",
    "    \n",
    "    print(f\"\\nPrecision Difference (MedDRA - Original): {precision_diff:+.4f}\")\n",
    "    print(f\"Recall Difference (MedDRA - Original):     {recall_diff:+.4f}\")\n",
    "    print(f\"F1-Score Difference (MedDRA - Original):     {f1_diff:+.4f}\")\n",
    "    \n",
    "    # Ground truth count analysis\n",
    "    meddra_gt_total = sum(r['meddra_gt_count'] for r in results['per_file_results'])\n",
    "    original_gt_total = sum(r['original_gt_count'] for r in results['per_file_results'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GROUND TRUTH ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal MedDRA ADR Entities:    {meddra_gt_total}\")\n",
    "    print(f\"Total Original ADR Entities:    {original_gt_total}\")\n",
    "    print(f\"Difference:                     {meddra_gt_total - original_gt_total:+.0f}\")\n",
    "    \n",
    "    if meddra_gt_total != original_gt_total:\n",
    "        print(f\"\\n‚ö† Note: MedDRA and Original ground truth have different entity counts.\")\n",
    "        print(f\"   This may indicate annotation differences or standardization effects.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"‚úì Results display function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis of Performance Differences\n",
    "\n",
    "### Understanding Differences Between Original and MedDRA Ground Truth\n",
    "\n",
    "Several factors can contribute to performance differences between evaluations using original vs MedDRA annotations:\n",
    "\n",
    "#### 1. **Annotation Standardization**\n",
    "- **MedDRA Standardization**: MedDRA annotations use standardized medical terminology with numeric codes, which may:\n",
    "  - Consolidate synonymous terms into single codes\n",
    "  - Normalize variations (e.g., \"drowsiness\" vs \"drowsy\" may map to same code)\n",
    "  - Use more specific clinical terminology\n",
    "- **Original Annotations**: May contain more natural language variations and informal expressions\n",
    "\n",
    "#### 2. **Entity Boundary Differences**\n",
    "- MedDRA annotations may have slightly different character boundaries due to standardization\n",
    "- Original annotations might include/exclude surrounding words differently\n",
    "\n",
    "#### 3. **Entity Count Differences**\n",
    "- MedDRA may consolidate multiple mentions into single standardized entities\n",
    "- Original annotations may preserve all individual mentions\n",
    "\n",
    "#### 4. **Label Consistency**\n",
    "- MedDRA ensures all ADR entities follow standardized coding\n",
    "- Original annotations may have more variability in labeling consistency\n",
    "\n",
    "### Expected Scenarios:\n",
    "\n",
    "1. **MedDRA Performance Higher**: If the model better matches standardized terminology\n",
    "2. **Original Performance Higher**: If the model captures natural language variations better\n",
    "3. **Similar Performance**: If both ground truth sets are well-aligned\n",
    "\n",
    "### Clinical Significance of ADR Detection Accuracy\n",
    "\n",
    "**High ADR detection accuracy is clinically critical** for several reasons:\n",
    "\n",
    "1. **Patient Safety**:\n",
    "   - **False Negatives (Missed ADRs)**: Can lead to:\n",
    "     - Continued use of harmful medications\n",
    "     - Delayed medical intervention\n",
    "     - Severe adverse events going unreported\n",
    "   - **False Positives (Incorrect ADRs)**: Can lead to:\n",
    "     - Unnecessary medication changes\n",
    "     - Patient anxiety\n",
    "     - Over-reporting that dilutes signal detection\n",
    "\n",
    "2. **Regulatory Reporting**:\n",
    "   - Inaccurate ADR detection affects post-marketing surveillance data\n",
    "   - Regulatory bodies (FDA, EMA) require accurate ADR reporting\n",
    "   - MedDRA coding ensures standardized reporting across systems\n",
    "\n",
    "3. **Clinical Decision Support**:\n",
    "   - Electronic health records use ADR information for alerts\n",
    "   - Incorrect ADR detection can generate false alerts (alert fatigue)\n",
    "   - Missing ADRs can miss critical drug-safety information\n",
    "\n",
    "4. **Pharmacovigilance Signal Detection**:\n",
    "   - Automated ADR extraction enables early detection of safety signals\n",
    "   - Low recall can miss emerging safety concerns\n",
    "   - Low precision can create noise that obscures real signals\n",
    "\n",
    "5. **Research and Knowledge Discovery**:\n",
    "   - ADR patterns help identify drug-drug interactions\n",
    "   - Population-specific risks (age, gender, comorbidities)\n",
    "   - Dose-response relationships\n",
    "\n",
    "### Recommended Performance Targets:\n",
    "\n",
    "- **Precision**: > 0.80 (minimize false positives for regulatory reporting)\n",
    "- **Recall**: > 0.85 (minimize false negatives for patient safety)\n",
    "- **F1-Score**: > 0.82 (balanced performance)\n",
    "\n",
    "### MedDRA vs Original Evaluation Benefits:\n",
    "\n",
    "1. **MedDRA Evaluation**:\n",
    "   - Tests model performance on standardized medical terminology\n",
    "   - Aligns with real-world pharmacovigilance workflows\n",
    "   - Enables direct integration with regulatory databases\n",
    "\n",
    "2. **Original Evaluation**:\n",
    "   - Tests model performance on natural language variations\n",
    "   - Reflects patient forum and social media contexts\n",
    "   - Captures informal and colloquial expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "This notebook provides a comprehensive ADR-specific evaluation framework with:\n",
    "\n",
    "### ‚úÖ Features Implemented:\n",
    "\n",
    "1. **MedDRA Format Parsing**: Correctly parses TT prefix + MedDRA code + character ranges format\n",
    "2. **ADR Entity Filtering**: Filters predictions to only ADR labels\n",
    "3. **Exact Span Matching**: Entity-level evaluation with exact boundary matching\n",
    "4. **Dual Ground Truth Evaluation**: Compares performance against both MedDRA and original annotations\n",
    "5. **Comprehensive Metrics**: Precision, Recall, F1-Score with TP/FP/FN breakdowns\n",
    "6. **Performance Comparison**: Analyzes differences between MedDRA and original evaluations\n",
    "7. **Clinical Context**: Explains importance of ADR detection in pharmacovigilance\n",
    "\n",
    "### üìä Key Insights:\n",
    "\n",
    "- **MedDRA Ground Truth**: Standardized medical terminology with codes\n",
    "- **Original Ground Truth**: Natural language variations\n",
    "- **Performance Differences**: Reflect annotation standardization and terminology alignment\n",
    "- **Clinical Significance**: ADR detection accuracy directly impacts patient safety\n",
    "\n",
    "### üîß Usage:\n",
    "\n",
    "1. Integrate with Task 2 to generate predictions\n",
    "2. Run evaluation on single file or full dataset\n",
    "3. Compare MedDRA vs Original performance\n",
    "4. Analyze differences and clinical implications\n",
    "\n",
    "### üìù Notes:\n",
    "\n",
    "- Predictions must be generated using Task 2 pipeline or loaded from files\n",
    "- Evaluation uses exact boundary matching (consistent with Task 3 methodology)\n",
    "- MedDRA annotations contain only ADR entities (by design)\n",
    "- Original annotations may have slight boundary/text differences from MedDRA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
